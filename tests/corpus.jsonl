{"id": "8803MachineLearning", "title": "8803 Machine Learning Theory, Fall 2011", "abstract": "", "venue": "", "keywords": ["complexity theory", "generalization certification"]}
{"id": "abbeLearningReasonNeural2022", "title": "Learning to Reason with Neural Networks: Generalization, Unseen Data and Boolean Measures", "abstract": "", "venue": "Advances in Neural Information Processing Systems", "keywords": []}
{"id": "abdalLabels4FreeUnsupervisedSegmentation2021", "title": "Labels4Free: Unsupervised Segmentation Using StyleGAN", "abstract": "", "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "keywords": []}
{"id": "abdalLabels4FreeUnsupervisedSegmentation2021a", "title": "Labels4Free: Unsupervised Segmentation Using StyleGAN", "abstract": "We propose an unsupervised segmentation framework for StyleGAN generated objects. We build on two main observations. First, the features generated by StyleGAN hold valuable information that can be utilized towards training segmentation networks. Second, the foreground and background can often be treated to be largely independent and be composited in different ways. For our solution, we propose to augment the StyleGAN2 generator architecture with a segmentation branch and to split the generator into a foreground and background network. This enables us to generate soft segmentation masks for the foreground object in an unsupervised fashion. On multiple object classes, we report comparable results against state-of-the-art supervised segmentation networks, while against the best unsupervised segmentation approach we demonstrate a clear improvement, both in qualitative and quantitative metrics.", "venue": "arXiv", "keywords": ["gans", "semantic segmentation", "unsupervised learning"]}
{"id": "abdalStyleFlowAttributeconditionedExploration2021", "title": "StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images Using Conditional Continuous Normalizing Flows", "abstract": "High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this article, in the context of conditional exploration of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating conditional exploration as an instance of conditional continuous normalizing flows in the GAN latent space conditioned by attribute features. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes on both real photographs and StyleGAN generated images. For example, for faces, we vary camera pose, illumination variation, expression, facial hair, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow over prior and several concurrent works. Project Page and Video: https://rameenabdal.github.io/StyleFlow.", "venue": "ACM Transactions on Graphics", "keywords": ["adversarial learning", "gans", "image synthesis", "normalizing flows"]}
{"id": "abnarQuantifyingAttentionFlow2020", "title": "Quantifying Attention Flow in Transformers", "abstract": "In the Transformer model, \"self-attention\" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.", "venue": "arXiv", "keywords": []}
{"id": "abuClassificationMultipleVisual2021", "title": "Classification of Multiple Visual Field Defects Using Deep Learning", "abstract": "In this work, a custom deep learning method is proposed to develop a detection of visual fields defects which are the markers for serious optic pathway disease. Convolutional Neural Networks (CNN) is a deep learning method that is mostly used in images processing. Therefore, a custom 10 layers of CNN algorithm is built to detect the visual field defect. In this work, 1200 visual field defect images acquired from the Humphrey Field Analyzer 24--2 collected from Google Image have been used to classify 6 types of visual field defect. The defect patterns are including defects at central scotoma, right/left/upper/lower quadratopia, right/left hemianopia, vision tunnel, superior/inferior field defect and normal as baseline. The custom designed CNN is trained to discriminate between defect patterns in visual field images. In the proposed method, a mechanism of pre-processing is included to improve the classification of visual field defects. Then, the 6 visual field defect patterns are detected using a convolutional neural network. The dataset is evaluated using 5-fold cross-validation. The results of this work have shown that the proposed algorithm achieved a high classification rate with 96%. As comparison, traditional machine learning Support Vector Machine (SVM) and Classical Neural Network (NN) is chose and obtained classification rate at 74.54% and 90.72%.", "venue": "Journal of Physics: Conference Series", "keywords": []}
{"id": "achddouMultitaskOnlineLearning2023", "title": "Multitask Online Learning: Listen to the Neighborhood Buzz", "abstract": "We study multitask online learning in a setting where agents can only exchange information with their neighbors on an arbitrary communication network. We introduce \\ , a decentralized algorithm for this setting whose regret depends on the interplay between the task similarities and the network structure. Our analysis shows that the regret of \\ is never worse (up to constants) than the bound obtained when agents do not share information. On the other hand, our bounds significantly improve when neighboring agents operate on similar tasks. In addition, we prove that our algorithm can be made differentially private with a negligible impact on the regret when the losses are linear. Finally, we provide experimental support for our theory.", "venue": "arXiv", "keywords": ["multi-task learning"]}
{"id": "AdaptiveHistogramEqualization", "title": "Adaptive Histogram Equalization and Its Variations - ScienceDirect", "abstract": "", "venue": "", "keywords": ["foundational", "histogram equalization"]}
{"id": "adebisiCalibrationFishEyesCamera2021", "title": "Calibration of Fish-Eyes Camera", "abstract": "Fisheye cameras compared to conventional cameras have much larger angle or field of view. The large angle of view is associated with many issues and problem like non-linear distortions introduced near the boundaries of the image captured by fisheye cameras. Despite the problems, their use is not only in the field of photogrammetry but they have also been use in applications of computer vision, automotive application, robotics and so on. The distortion of the images captured by the cameras can be corrected through the calibration of such cameras. There are many different methods to calibrate fisheye camera based on the type of camera model. This write up present few out of the many approaches to calibration of fisheye camera, distortion correction, 3D reconstruction and highlighted the issues and problem associated with the use of fisheye cameras.", "venue": "", "keywords": []}
{"id": "ADUULMDatasetUlm", "title": "ADUULM Dataset - Ulm University", "abstract": "", "venue": "", "keywords": []}
{"id": "AdvancedVideoCoding2024", "title": "Advanced Video Coding", "abstract": "Advanced Video Coding (AVC), also referred to as H.264 or MPEG-4 Part 10, is a video compression standard based on block-oriented, motion-compensated coding. It is by far the most commonly used format for the recording, compression, and distribution of video content, used by 91% of video industry developers as of September 2019. It supports a maximum resolution of 8K UHD.The intent of the H.264/AVC project was to create a standard capable of providing good video quality at substantially lower bit rates than previous standards (i.e., half or less the bit rate of MPEG-2, H.263, or MPEG-4 Part 2), without increasing the complexity of design so much that it would be impractical or excessively expensive to implement. This was achieved with features such as a reduced-complexity integer discrete cosine transform (integer DCT), variable block-size segmentation, and multi-picture inter-picture prediction. An additional goal was to provide enough flexibility to allow the standard to be applied to a wide variety of applications on a wide variety of networks and systems, including low and high bit rates, low and high resolution video, broadcast, DVD storage, RTP/IP packet networks, and ITU-T multimedia telephony systems. The H.264 standard can be viewed as a \"family of standards\" composed of a number of different profiles, although its \"High profile\" is by far the most commonly used format. A specific decoder decodes at least one, but not necessarily all profiles. The standard describes the format of the encoded data and how the data is decoded, but it does not specify algorithms for encoding video -- that is left open as a matter for encoder designers to select for themselves, and a wide variety of encoding schemes have been developed. H.264 is typically used for lossy compression, although it is also possible to create truly lossless-coded regions within lossy-coded pictures or to support rare use cases for which the entire encoding is lossless. H.264 was standardized by the ITU-T Video Coding Experts Group (VCEG) of Study Group 16 together with the ISO/IEC JTC 1 Moving Picture Experts Group (MPEG). The project partnership effort is known as the Joint Video Team (JVT). The ITU-T H.264 standard and the ISO/IEC MPEG-4 AVC standard (formally, ISO/IEC 14496-10 -- MPEG-4 Part 10, Advanced Video Coding) are jointly maintained so that they have identical technical content. The final drafting work on the first version of the standard was completed in May 2003, and various extensions of its capabilities have been added in subsequent editions. High Efficiency Video Coding (HEVC), a.k.a. H.265 and MPEG-H Part 2 is a successor to H.264/MPEG-4 AVC developed by the same organizations, while earlier standards are still in common use. H.264 is perhaps best known as being the most commonly used video encoding format on Blu-ray Discs. It is also widely used by streaming Internet sources, such as videos from Netflix, Hulu, Amazon Prime Video, Vimeo, YouTube, and the iTunes Store, Web software such as the Adobe Flash Player and Microsoft Silverlight, and also various HDTV broadcasts over terrestrial (ATSC, ISDB-T, DVB-T or DVB-T2), cable (DVB-C), and satellite (DVB-S and DVB-S2) systems. H.264 is restricted by patents owned by various parties. A license covering most (but not all) patents essential to H.264 is administered by a patent pool administered by MPEG LA. The commercial use of patented H.264 technologies requires the payment of royalties to MPEG LA and other patent owners. MPEG LA has allowed the free use of H.264 technologies for streaming Internet video that is free to end users, and Cisco Systems pays royalties to MPEG LA on behalf of the users of binaries for its open source H.264 encoder openH264.", "venue": "Wikipedia", "keywords": []}
{"id": "afifiHistoGANControllingColors2021", "title": "HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms", "abstract": "While generative adversarial networks (GANs) can successfully produce high-quality images, they can be challenging to control. Simplifying GAN-based image generation is critical for their adoption in graphic design and artistic work. This goal has led to significant interest in methods that can intuitively control the appearance of images generated by GANs. In this paper, we present HistoGAN, a color histogram-based method for controlling GAN-generated images' colors. We focus on color histograms as they provide an intuitive way to describe image color while remaining decoupled from domain-specific semantics. Specifically, we introduce an effective modification of the recent StyleGAN architecture to control the colors of GAN-generated images specified by a target color histogram feature. We then describe how to expand HistoGAN to recolor real images. For image recoloring, we jointly train an encoder network along with HistoGAN. The recoloring model, ReHistoGAN, is an unsupervised approach trained to encourage the network to keep the original image's content while changing the colors based on the given target histogram. We show that this histogram-based approach offers a better way to control GAN-generated and real images' colors while producing more compelling results compared to existing alternative strategies.", "venue": "arXiv", "keywords": ["adversarial learning", "gans", "histogram equalization"]}
{"id": "AIChatScientific", "title": "AI Chat for Scientific PDFs SciSpace", "abstract": "Chat with PDF and conduct your literature review faster using SciSpace. Discover 200M+ papers or upload your own PDF, highlight text or ask questions, and extract explanations and summaries.", "venue": "", "keywords": []}
{"id": "al-nasrawiModifiedIterativeGuided2019", "title": "Modified Iterative Guided Texture Filtering Algorithm", "abstract": "Structure-texture decomposition smoothing has been extensively studied due to its wide range of applications in computational photography and image processing. In this paper, we propose a new structure-texture decomposition algorithm which is based on two fundamental ideas: (1) guidance image and (2) iterative smoothing. The guidance image is generated by mitigating high-frequency oscillatory components in the original image. The result is then incorporated in a new generic iterative framework which makes use of well-known guided edge-reserving filters such as bilateral filter (BF), guided filter (GF), domain transform filter (DTF), and the extended Bayesian model averaging filter (BMA) called guided Bayesian model averaging filter (GBMA) to achieve texture smoothing. We have presented a detailed study of the proposed algorithm including: guidance image generation, an evaluation of the guided edge-preserving filters which are incorporated in the proposed iterative framework, the number of iterations for the proposed iterative structure, and the selection of guided edge-preserving filter. We demonstrate that the proposed method is a flexible and effective tool for a wide range of image editing applications including: image abstraction, color pencil drawing, content-aware image resizing, and texture editing. In particular, the proposed approach has the best performance in structure-texture decomposition for an image with low-contrast features.", "venue": "Computers & Graphics", "keywords": ["filter augmentations", "guided filters"]}
{"id": "alaizVisualizationFeatureSpace2020", "title": "Visualization of the Feature Space of Neural Networks", "abstract": "Visualization of a learning machine can be crucial to understand its behaviour, specially in the case of (deep) neural networks, since they are quite difficult to interpret. An approach for visualizing the feature space of a neural network is presented, trying to answer to the question ``what representation of the data is the network using to make its decision?'' The proposed method gives a representation of the space where the network is tackling the problem, reducing it while respecting the linearity of the model. As shown experimentally, this technique allows to study the evolution of the model with respect to the training epochs, to have a representation of the data similar to the one used by the neural network, and even to detect groups of patterns that behave differently.", "venue": "Computational Intelligence", "keywords": ["feature visualization", "visualizations"]}
{"id": "alapattTemporallyConstrainedNeural2021", "title": "Temporally Constrained Neural Networks (TCNN): A Framework for Semi-Supervised Video Semantic Segmentation", "abstract": "A major obstacle to building models for effective semantic segmentation, and particularly video semantic segmentation, is a lack of large and well annotated datasets. This bottleneck is particularly prohibitive in highly specialized and regulated fields such as medicine and surgery, where video semantic segmentation could have important applications but data and expert annotations are scarce. In these settings, temporal clues and anatomical constraints could be leveraged during training to improve performance. Here, we present Temporally Constrained Neural Networks (TCNN), a semi-supervised framework used for video semantic segmentation of surgical videos. In this work, we show that autoencoder networks can be used to efficiently provide both spatial and temporal supervisory signals to train deep learning models. We test our method on a newly introduced video dataset of laparoscopic cholecystectomy procedures, Endoscapes, and an adaptation of a public dataset of cataract surgeries, CaDIS. We demonstrate that lower-dimensional representations of predicted masks can be leveraged to provide a consistent improvement on both sparsely labeled datasets with no additional computational cost at inference time. Further, the TCNN framework is model-agnostic and can be used in conjunction with other model design choices with minimal additional complexity.", "venue": "arXiv", "keywords": ["semantic segmentation", "semi-supervised learning", "temporal consistency"]}
{"id": "albuquerqueGeneralizingUnseenDomains2021", "title": "Generalizing to Unseen Domains via Distribution Matching", "abstract": "Supervised learning results typically rely on assumptions of i.i.d. data. Unfortunately, those assumptions are commonly violated in practice. In this work, we tackle such problem by focusing on domain generalization: a formalization where the data generating process at test time may yield samples from never-before-seen domains (distributions). Our work relies on the following lemma: by minimizing a notion of discrepancy between all pairs from a set of given domains, we also minimize the discrepancy between any pairs of mixtures of domains. Using this result, we derive a generalization bound for our setting. We then show that low risk over unseen domains can be achieved by representing the data in a space where (i) the training distributions are indistinguishable, and (ii) relevant information for the task at hand is preserved. Minimizing the terms in our bound yields an adversarial formulation which estimates and minimizes pairwise discrepancies. We validate our proposed strategy on standard domain generalization benchmarks, outperforming a number of recently introduced methods. Notably, we tackle a real-world application where the underlying data corresponds to multi-channel electroencephalography time series from different subjects, each considered as a distinct domain.", "venue": "arXiv", "keywords": ["distribution matching", "domain generalization", "generalization quantification", "highly-analytical"]}
{"id": "alexievFractalDimensionGeneralization2020", "title": "Fractal Dimension Generalization Measure", "abstract": "Developing a robust generalization measure for the performance of machine learning models is an important and challenging task. A lot of recent research in the area focuses on the model decision boundary when predicting generalization. In this paper, as part of the \"Predicting Generalization in Deep Learning\" competition, we analyse the complexity of decision boundaries using the concept of fractal dimension and develop a generalization measure based on that technique.", "venue": "arXiv", "keywords": ["decision boundaries", "generalization quantification", "measure theory"]}
{"id": "alijaniVisionTransformersDomain2024", "title": "Vision Transformers in Domain Adaptation and Domain Generalization: A Study of Robustness", "abstract": "Deep learning models are often evaluated in scenarios where the data distribution is different from those used in the training and validation phases. The discrepancy presents a challenge for accurately predicting the performance of models once deployed on the target distribution. Domain adaptation and generalization are widely recognized as effective strategies for addressing such shifts, thereby ensuring reliable performance. The recent promising results in applying vision transformers in computer vision tasks, coupled with advancements in self-attention mechanisms, have demonstrated their significant potential for robustness and generalization in handling distribution shifts. Motivated by the increased interest from the research community, our paper investigates the deployment of vision transformers in domain adaptation and domain generalization scenarios. For domain adaptation methods, we categorize research into feature-level, instance-level, model-level adaptations, and hybrid approaches, along with other categorizations with respect to diverse strategies for enhancing domain adaptation. Similarly, for domain generalization, we categorize research into multi-domain learning, meta-learning, regularization techniques, and data augmentation strategies. We further classify diverse strategies in research, underscoring the various approaches researchers have taken to address distribution shifts by integrating vision transformers. The inclusion of comprehensive tables summarizing these categories is a distinct feature of our work, offering valuable insights for researchers. These findings highlight the versatility of vision transformers in managing distribution shifts, crucial for real-world applications, especially in critical safety and decision-making scenarios.", "venue": "Neural Computing and Applications", "keywords": []}
{"id": "alijaniVisionTransformersDomain2024a", "title": "Vision Transformers in Domain Adaptation and Domain Generalization: A Study of Robustness", "abstract": "Deep learning models are often evaluated in scenarios where the data distribution is different from those used in the training and validation phases. The discrepancy presents a challenge for accurately predicting the performance of models once deployed on the target distribution. Domain adaptation and generalization are widely recognized as effective strategies for addressing such shifts, thereby ensuring reliable performance. The recent promising results in applying vision transformers in computer vision tasks, coupled with advancements in self-attention mechanisms, have demonstrated their significant potential for robustness and generalization in handling distribution shifts. Motivated by the increased interest from the research community, our paper investigates the deployment of vision transformers in domain adaptation and domain generalization scenarios. For domain adaptation methods, we categorize research into feature-level, instance-level, model-level adaptations, and hybrid approaches, along with other categorizations with respect to diverse strategies for enhancing domain adaptation. Similarly, for domain generalization, we categorize research into multi-domain learning, meta-learning, regularization techniques, and data augmentation strategies. We further classify diverse strategies in research, underscoring the various approaches researchers have taken to address distribution shifts by integrating vision transformers. The inclusion of comprehensive tables summarizing these categories is a distinct feature of our work, offering valuable insights for researchers. These findings highlight the versatility of vision transformers in managing distribution shifts, crucial for real-world applications, especially in critical safety and decision-making scenarios.", "venue": "arXiv", "keywords": ["domain adaptation", "domain generalization", "robustness analysis", "surveys", "transformers"]}
{"id": "almaliogluDeepLearningbasedRobust2022", "title": "Deep Learning-Based Robust Positioning for All-Weather Autonomous Driving", "abstract": "Abstract Interest in autonomous vehicles (AVs) is growing at a rapid pace due to increased convenience, safety benefits and potential environmental gains. Although several leading AV companies predicted that AVs would be on the road by 2020, they are still limited to relatively small-scale trials. The ability to know their precise location on the map is a challenging prerequisite for safe and reliable AVs due to sensor imperfections under adverse environmental and weather conditions, posing a formidable obstacle to their widespread use. Here we propose a deep learning-based self-supervised approach for ego-motion estimation that is a robust and complementary localization solution under inclement weather conditions. The proposed approach is a geometry-aware method that attentively fuses the rich representation capability of visual sensors and the weather-immune features provided by radars using an attention-based learning technique. Our method predicts reliability masks for the sensor measurements, eliminating the deficiencies in the multimodal data. In various experiments we demonstrate the robust all-weather performance and effective cross-domain generalizability under harsh weather conditions such as rain, fog and snow, as well as day and night conditions. Furthermore, we employ a game-theoretic approach to analyse the interpretability of the model predictions, illustrating the independent and uncorrelated failure modes of the multimodal system. We anticipate our work will bring AVs one step closer to safe and reliable all-weather autonomous driving.", "venue": "Nature Machine Intelligence", "keywords": ["adverse weather", "depth estimation"]}
{"id": "aminbeidokhtiDomainGeneralizationRejecting2023", "title": "Domain Generalization by Rejecting Extreme Augmentations", "abstract": "Data augmentation is one of the most effective techniques for regularizing deep learning models and improving their recognition performance in a variety of tasks and domains. However, this holds for standard in-domain settings, in which the training and test data follow the same distribution. For the out-of-domain case, where the test data follow a different and unknown distribution, the best recipe for data augmentation is unclear. In this paper, we show that for out-of-domain and domain generalization settings, data augmentation can provide a conspicuous and robust improvement in performance. To do that, we propose a simple training procedure: (i) use uniform sampling on standard data augmentation transformations; (ii) increase the strength transformations to account for the higher data variance expected when working out-of-domain, and (iii) devise a new reward function to reject extreme transformations that can harm the training. With this procedure, our data augmentation scheme achieves a level of accuracy that is comparable to or better than state-of-the-art methods on benchmark domain generalization datasets. Code: ://github.com/Masseeh/DCAug\\", "venue": "arXiv", "keywords": ["auto-augmentation policies", "consistency training", "domain generalization", "meta-learning"]}
{"id": "anArtFlowUnbiasedImage2021", "title": "ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows", "abstract": "Universal style transfer retains styles from reference images in content images. While existing methods have achieved state-of-the-art style transfer performance, they are not aware of the content leak phenomenon that the image content may corrupt after several rounds of stylization process. In this paper, we propose ArtFlow to prevent content leak during universal style transfer. ArtFlow consists of reversible neural flows and an unbiased feature transfer module. It supports both forward and backward inferences and operates in a projection-transfer-reversion scheme. The forward inference projects input images into deep features, while the backward inference remaps deep features back to input images in a lossless and unbiased way. Extensive experiments demonstrate that ArtFlow achieves comparable performance to state-of-the-art style transfer methods while avoiding content leak.", "venue": "arXiv", "keywords": ["normalizing flows", "style ind learning", "style transfer"]}
{"id": "anArtFlowUnbiasedImage2021a", "title": "ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows", "abstract": "", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": []}
{"id": "anwarMultiTaskFederatedReinforcement2021", "title": "Multi-Task Federated Reinforcement Learning with Adversaries", "abstract": "Reinforcement learning algorithms, just like any other Machine learning algorithm pose a serious threat from adversaries. The adversaries can manipulate the learning algorithm resulting in non-optimal policies. In this paper, we analyze the Multi-task Federated Reinforcement Learning algorithms, where multiple collaborative agents in various environments are trying to maximize the sum of discounted return, in the presence of adversarial agents. We argue that the common attack methods are not guaranteed to carry out a successful attack on Multi-task Federated Reinforcement Learning and propose an adaptive attack method with better attack performance. Furthermore, we modify the conventional federated reinforcement learning algorithm to address the issue of adversaries that works equally well with and without the adversaries. Experimentation on different small to mid-size reinforcement learning problems show that the proposed attack method outperforms other general attack methods and the proposed modification to federated reinforcement learning algorithm was able to achieve near-optimal policies in the presence of adversarial agents.", "venue": "arXiv", "keywords": ["federated learning", "multi-task learning", "reinforcement learning"]}
{"id": "araslanovSelfsupervisedAugmentationConsistency2021", "title": "Self-Supervised Augmentation Consistency for Adapting Semantic Segmentation", "abstract": "We propose an approach to domain adaptation for semantic segmentation that is both practical and highly accurate. In contrast to previous work, we abandon the use of computationally involved adversarial objectives, network ensembles and style transfer. Instead, we employ standard data augmentation techniques -- photometric noise, flipping and scaling -- and ensure consistency of the semantic predictions across these image transformations. We develop this principle in a lightweight self-supervised framework trained on co-evolving pseudo labels without the need for cumbersome extra training rounds. Simple in training from a practitioner's standpoint, our approach is remarkably effective. We achieve significant improvements of the state-ofthe-art segmentation accuracy after adaptation, consistent both across different choices of the backbone architecture and adaptation scenarios.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "consistency training", "promising", "self-supervised learning"]}
{"id": "arjovskyInvariantRiskMinimization2020", "title": "Invariant Risk Minimization", "abstract": "We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.", "venue": "arXiv", "keywords": ["highly-analytical", "measure theory", "risk optimization"]}
{"id": "arjovskyWassersteinGAN2017", "title": "Wasserstein GAN", "abstract": "We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.", "venue": "arXiv", "keywords": []}
{"id": "asanoSelflabellingSimultaneousClustering2020", "title": "Self-Labelling via Simultaneous Clustering and Representation Learning", "abstract": "Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. However, doing so naively leads to ill posed learning problems with degenerate solutions. In this paper, we propose a novel and principled learning formulation that addresses these issues. The method is obtained by maximizing the information between labels and input data indices. We show that this criterion extends standard crossentropy minimization to an optimal transport problem, which we solve efficiently for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm. The resulting method is able to self-label visual data so as to train highly competitive image representations without manual labels. Our method achieves state of the art representation learning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100 and ImageNet and yields the first self-supervised AlexNet that outperforms the supervised Pascal VOC detection baseline. Code and models are available.", "venue": "arXiv", "keywords": ["clustering", "deep clustering", "representation learning", "self-supervised learning"]}
{"id": "atharComprehensivePerformanceEvaluation2019", "title": "A Comprehensive Performance Evaluation of Image Quality Assessment Algorithms", "abstract": "Image quality assessment (IQA) algorithms aim to predict perceived image quality by human observers. Over the last two decades, a large amount of work has been carried out in the field. New algorithms are being developed at a rapid rate in different areas of IQA, but are often tested and compared with limited existing models using out-of-date test data. There is a significant gap when it comes to large-scale performance evaluation studies that include a wide variety of test data and competing algorithms. In this work we aim to fill this gap by carrying out the largest performance evaluation study so far. We test the performance of 43 full-reference (FR), seven fused FR (22 versions), and 14 no-reference (NR) methods on nine subject-rated IQA datasets, of which five contain singly distorted images and four contain multiply distorted content. We use a variety of performance evaluation and statistical significance testing criteria. Our findings not only point to the top performing FR and NR IQA methods, but also highlight the performance gap between them. In addition, we have also conducted a comparative study on FR fusion methods, and an important discovery is that rank aggregation based FR fusion is able to outperform not only other FR fusion approaches but also the top performing FR methods. It may be used to annotate IQA datasets as a possible alternative to subjective ratings, especially in situations where it is not possible to obtain human opinions, such as in the case of large-scale datasets composed of thousands or even millions of images.", "venue": "IEEE Access", "keywords": ["image quality assessment"]}
{"id": "ayokunleadebisiCalibrationFishEyesCamera2021", "title": "Calibration of Fish-Eyes Camera", "abstract": "Fisheye cameras compared to conventional cameras have much larger angle or field of view. The large angle of view is associated with many issues and problem like non-linear distortions introduced near the boundaries of the image captured by fisheye cameras. Despite the problems, their use is not only in the field of photogrammetry but they have also been use in applications of computer vision, automotive application, robotics and so on. The distortion of the images captured by the cameras can be corrected through the calibration of such cameras. There are many different methods to calibrate fisheye camera based on the type of camera model. This write up present few out of the many approaches to calibration of fisheye camera, distortion correction, 3D reconstruction and highlighted the issues and problem associated with the use of fisheye cameras.", "venue": "[object Object]", "keywords": ["camera calibration", "fisheye distortion", "surveys"]}
{"id": "azadTransDeepLabConvolutionFreeTransformerbased2022", "title": "TransDeepLab: Convolution-Free Transformer-based DeepLab V3+ for Medical Image Segmentation", "abstract": "Convolutional neural networks (CNNs) have been the de facto standard in a diverse set of computer vision tasks for many years. Especially, deep neural networks based on seminal architectures such as U-shaped models with skip-connections or atrous convolution with pyramid pooling have been tailored to a wide range of medical image analysis tasks. The main advantage of such architectures is that they are prone to detaining versatile local features. However, as a general consensus, CNNs fail to capture long-range dependencies and spatial correlations due to the intrinsic property of confined receptive field size of convolution operations. Alternatively, Transformer, profiting from global information modelling that stems from the self-attention mechanism, has recently attained remarkable performance in natural language processing and computer vision. Nevertheless, previous studies prove that both local and global features are critical for a deep model in dense prediction, such as segmenting complicated structures with disparate shapes and configurations. To this end, this paper proposes TransDeepLab, a novel DeepLab-like pure Transformer for medical image segmentation. Specifically, we exploit hierarchical Swin-Transformer with shifted windows to extend the DeepLabv3 and model the Atrous Spatial Pyramid Pooling (ASPP) module. A thorough search of the relevant literature yielded that we are the first to model the seminal DeepLab model with a pure Transformer-based model. Extensive experiments on various medical image segmentation tasks verify that our approach performs superior or on par with most contemporary works on an amalgamation of Vision Transformer and CNN-based methods, along with a significant reduction of model complexity. The codes and trained models are publicly available at https://github.com/rezazad68/transdeeplab", "venue": "arXiv", "keywords": ["semantic segmentation", "transformers"]}
{"id": "aziereMultistageTemporalConvolution", "title": "Multistage Temporal Convolution Transformer for Action Segmentation", "abstract": "This paper addresses fully supervised action segmentation. Transformers have been shown to have large model capacity and powerful sequence modelling abilities, and hence seem quite suitable for capturing action grammar in videos. However, their performance in video understanding still lags behind that of temporal convolutional networks, or ConvNets for short. We hypothesize that this is because: (i) ConvNets tend to generalize better than Transformers, and (ii) Transformer's large model capacity requires significantly larger training datasets than existing action segmentation benchmarks. We specify a new hybrid model, TCTr, that combines the strengths from both frameworks. TCTr seamlessly unifies depth-wise convolution and selfattention in a principled manner. Also, TCTr addresses the Transformer's quadratic computational and memory complexity in the sequence length by learning how to adaptively estimate attention from local temporal neighborhoods, instead of all frames. Our experiments show that TCTr significantly outperforms the state of the art on the Breakfast, GTEA, and 50Salads datasets.", "venue": "", "keywords": ["temporal consistency", "transformers"]}
{"id": "babuOnlineMetaLearningLearning2021", "title": "Online Meta-Learning via Learning with Layer-Distributed Memory", "abstract": "We demonstrate that efficient meta-learning can be achieved via end-to-end training of deep neural networks with memory distributed across layers. The persistent state of this memory assumes the entire burden of guiding task adaptation. Moreover, its distributed nature is instrumental in orchestrating adaptation. Ablation experiments demonstrate that providing relevant feedback to memory units distributed across the depth of the network enables them to guide adaptation throughout the entire network. Our results show that this is a successful strategy for simplifying meta-learning -- often cast as a bi-level optimization problem -- to standard end-to-end training, while outperforming gradient-based, prototype-based, and other memory-based meta-learning strategies. Additionally, our adaptation strategy naturally handles online learning scenarios with a significant delay between observing a sample and its corresponding label -- a setting in which other approaches struggle. Adaptation via distributed memory is effective across a wide range of learning tasks, ranging from classification to online few-shot semantic segmentation.", "venue": "Advances in Neural Information Processing Systems", "keywords": []}
{"id": "bachmanLearningRepresentationsMaximizing2019", "title": "Learning Representations by Maximizing Mutual Information Across Views", "abstract": "We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -- e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.", "venue": "arXiv", "keywords": ["measure theory", "mutual information measures", "representation learning", "unsupervised learning"]}
{"id": "baiAugmentationPathwaysNetwork2023", "title": "Augmentation Pathways Network for Visual Recognition", "abstract": "Data augmentation is practically helpful for visual recognition, especially at the time of data scarcity. However, such success is only limited to quite a few light augmentations (e.g., random crop, flip). Heavy augmentations are either unstable or show adverse effects during training, owing to the big gap between the original and augmented images. This paper introduces a novel network design, noted as Augmentation Pathways (AP), to systematically stabilize training on a much wider range of augmentation policies. Notably, AP tames various heavy data augmentations and stably boosts performance without a careful selection among augmentation policies. Unlike traditional single pathway, augmented images are processed in different neural paths. The main pathway handles the light augmentations, while other pathways focus on the heavier augmentations. By interacting with multiple paths in a dependent manner, the backbone network robustly learns from shared visual patterns among augmentations, and suppresses the side effect of heavy augmentations at the same time. Furthermore, we extend AP to high-order versions for high-order scenarios, demonstrating its robustness and flexibility in practical usage. Experimental results on ImageNet demonstrate the compatibility and effectiveness on a much wider range of augmentations, while consuming fewer parameters and lower computational costs at inference time.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "keywords": ["augmentation stability", "auto-augmentation policies"]}
{"id": "baiDynamicallyPruningSegformer2021", "title": "Dynamically Pruning Segformer for Efficient Semantic Segmentation", "abstract": "As one of the successful Transformer-based models in computer vision tasks, SegFormer demonstrates superior performance in semantic segmentation. Nevertheless, the high computational cost greatly challenges the deployment of SegFormer on edge devices. In this paper, we seek to design a lightweight SegFormer for efficient semantic segmentation. Based on the observation that neurons in SegFormer layers exhibit large variances across different images, we propose a dynamic gated linear layer, which prunes the most uninformative set of neurons based on the input instance. To improve the dynamically pruned SegFormer, we also introduce twostage knowledge distillation to transfer the knowledge within the original teacher to the pruned student network. Experimental results show that our method can significantly reduce the computation overhead of SegFormer without an apparent performance drop. For instance, we can achieve 36.9% mIoU with only 3.3G FLOPs on ADE20K, saving more than 60% computation with the drop of only 0.5% in mIoU.", "venue": "arXiv", "keywords": ["knowledge distillation", "segformer", "semantic segmentation"]}
{"id": "baiImprovingLatentSpace2022", "title": "Improving the Latent Space of Image Style Transfer", "abstract": "Existing neural style transfer researches have studied to match statistical information between the deep features of content and style images, which were extracted by a pre-trained VGG, and achieved significant improvement in synthesizing artistic images. However, in some cases, the feature statistics from the pre-trained encoder may not be consistent with the visual style we perceived. For example, the style distance between images of different styles is less than that of the same style. In such an inappropriate latent space, the objective function of the existing methods will be optimized in the wrong direction, resulting in bad stylization results. In addition, the lack of content details in the features extracted by the pre-trained encoder also leads to the content leak problem. In order to solve these issues in the latent space used by style transfer, we propose two contrastive training schemes to get a refined encoder that is more suitable for this task. The style contrastive loss pulls the stylized result closer to the same visual style image and pushes it away from the content image. The content contrastive loss enables the encoder to retain more available details. We can directly add our training scheme to some existing style transfer methods and significantly improve their results. Extensive experimental results demonstrate the effectiveness and superiority of our methods.", "venue": "arXiv", "keywords": ["contrastive learning", "feature engineering", "loss functions", "style transfer"]}
{"id": "baiSlicedOptimalPartial2023", "title": "Sliced Optimal Partial Transport", "abstract": "Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show applications of our proposed Sliced OPT problem in the noisy point cloud registration and color adaptation. Our code is available at https://github.com/yikun-baio/sliced_opt.", "venue": "arXiv", "keywords": ["optimal transport", "style transfer"]}
{"id": "balestrieroCookbookSelfSupervisedLearning2023", "title": "A Cookbook of Self-Supervised Learning", "abstract": "Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.", "venue": "arXiv", "keywords": []}
{"id": "balestrieroEffectsRegularizationData2022", "title": "The Effects of Regularization and Data Augmentation Are Class Dependent", "abstract": "Regularization is a fundamental technique to prevent over-fitting and to improve generalization performances by constraining a model's complexity. Current Deep Networks heavily rely on regularizers such as Data-Augmentation (DA) or weight-decay, and employ structural risk minimization, i.e. cross-validation, to select the optimal regularization hyper-parameters. In this study, we demonstrate that techniques such as DA or weight decay produce a model with a reduced complexity that is unfair across classes. The optimal amount of DA or weight decay found from cross-validation leads to disastrous model performances on some classes e.g. on Imagenet with a resnet50, the \"barn spider\" classification test accuracy falls from \\ to \\ only by introducing random crop DA during training. Even more surprising, such performance drop also appears when introducing uninformative regularization techniques such as weight decay. Those results demonstrate that our search for ever increasing generalization performance -- averaged over all classes and samples -- has left us with models and regularizers that silently sacrifice performances on some classes. This scenario can become dangerous when deploying a model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on INaturalist sees its performances fall from \\ to \\ on class #8889 when introducing random crop DA during the Imagenet pre-training phase. Those results demonstrate that designing novel regularizers without class-dependent bias remains an open research question.", "venue": "arXiv", "keywords": ["bias sources", "class balancing"]}
{"id": "balestrieroGeometryDeepNetworks2019", "title": "The Geometry of Deep Networks: Power Diagram Subdivision", "abstract": "We study the geometry of deep (neural) networks (DNs) with piecewise affine and convex nonlinearities. The layers of such DNs have been shown to be \\ max-affine spline operators\\ (MASOs) that partition their input space and apply a region-dependent affine mapping to their input to produce their output. We demonstrate that each MASO layer's input space partitioning corresponds to a \\ power diagram\\ (an extension of the classical Voronoi tiling) with a number of regions that grows exponentially with respect to the number of units (neurons). We further show that a composition of MASO layers (e.g., the entire DN) produces a progressively subdivided power diagram and provide its analytical form. The subdivision process constrains the affine maps on the (exponentially many) power diagram regions to greatly reduce their complexity. For classification problems, we obtain a formula for a MASO DN's decision boundary in the input space plus a measure of its curvature that depends on the DN's nonlinearities, weights, and architecture. Numerous numerical experiments support and extend our theoretical results.", "venue": "arXiv.org", "keywords": ["highly-analytical", "neural splines"]}
{"id": "balestrieroMadMaxAffine2018", "title": "Mad Max: Affine Spline Insights into Deep Learning", "abstract": "We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space that is implicitly induced by a MASO directly links DNs to the theory of vector quantization (VQ) and \\ -means clustering, which opens up new geometric avenue to study how DNs organize signals in a hierarchical fashion. To validate the utility of the VQ interpretation, we develop and validate a new distance metric for signals and images that quantifies the difference between their VQ encodings. (This paper is a significantly expanded version of A Spline Theory of Deep Learning from ICML 2018.)", "venue": "arXiv", "keywords": ["highly-analytical", "neural splines"]}
{"id": "balestrieroMaxAffineSplineInsights2020", "title": "Max-Affine Spline Insights into Deep Generative Networks", "abstract": "We connect a large class of Generative Deep Networks (GDNs) with spline operators in order to derive their properties, limitations, and new opportunities. By characterizing the latent space partition, dimension and angularity of the generated manifold, we relate the manifold dimension and approximation error to the sample size. The manifold-per-region affine subspace defines a local coordinate basis; we provide necessary and sufficient conditions relating those basis vectors with disentanglement. We also derive the output probability density mapped onto the generated manifold in terms of the latent space density, which enables the computation of key statistics such as its Shannon entropy. This finding also enables the computation of the GDN likelihood, which provides a new mechanism for model comparison as well as providing a quality measure for (generated) samples under the learned distribution. We demonstrate how low entropy and/or multimodal distributions are not naturally modeled by DGNs and are a cause of training instabilities.", "venue": "arXiv", "keywords": ["feature engineering", "highly-analytical", "manifold learning", "neural splines", "normalizing flows"]}
{"id": "ballesterPredictingGeneralizationGap2023", "title": "Predicting the Generalization Gap in Neural Networks Using Topological Data Analysis", "abstract": "Understanding how neural networks generalize on unseen data is crucial for designing more robust and reliable models. In this paper, we study the generalization gap of neural networks using methods from topological data analysis. For this purpose, we compute homological persistence diagrams of weighted graphs constructed from neuron activation correlations after a training phase, aiming to capture patterns that are linked to the generalization capacity of the network. We compare the usefulness of different numerical summaries from persistence diagrams and show that a combination of some of them can accurately predict and partially explain the generalization gap without the need of a test set. Evaluation on two computer vision recognition tasks (CIFAR10 and SVHN) shows competitive generalization gap prediction when compared against state-of-the-art methods.", "venue": "arXiv", "keywords": ["entropic optimization", "generalization quantification", "highly-analytical", "topological analysis"]}
{"id": "ballesterTopologicalDataAnalysis2024", "title": "Topological Data Analysis for Neural Network Analysis: A Comprehensive Survey", "abstract": "This survey provides a comprehensive exploration of applications of Topological Data Analysis (TDA) within neural network analysis. Using TDA tools such as persistent homology and Mapper, we delve into the intricate structures and behaviors of neural networks and their datasets. We discuss different strategies to obtain topological information from data and neural networks by means of TDA. Additionally, we review how topological information can be leveraged to analyze properties of neural networks, such as their generalization capacity or expressivity. We explore practical implications of deep learning, specifically focusing on areas like adversarial detection and model selection. Our survey organizes the examined works into four broad domains: 1. Characterization of neural network architectures; 2. Analysis of decision regions and boundaries; 3. Study of internal representations, activations, and parameters; 4. Exploration of training dynamics and loss functions. Within each category, we discuss several articles, offering background information to aid in understanding the various methodologies. We conclude with a synthesis of key insights gained from our study, accompanied by a discussion of challenges and potential advancements in the field.", "venue": "arXiv", "keywords": ["feature engineering", "surveys", "topological data analysis"]}
{"id": "balschScientificWritingMarkdown2018", "title": "Scientific Writing with Markdown", "abstract": "Read on to learn how to use Markdown and Pandoc for writing scientific documents with equations, citations, code blocks, Unicode characters, and vector graphics.", "venue": "Jaan Tollander de Balsch", "keywords": []}
{"id": "bamaniaAIJustMade2023", "title": "AI Just Made YouTube Video Compression More Efficient Than Ever. Here Is How!", "abstract": "A Deep Dive Into How MuZero-RC, a Deep RL Algorithm Saves Millions of Dollars For YouTube & Makes User Experience Better Than Ever!", "venue": "Medium", "keywords": []}
{"id": "banerjeeImprovedSoftActorCritic2021", "title": "Improved Soft Actor-Critic: Mixing Prioritized Off-Policy Samples with On-Policy Experience", "abstract": "Soft Actor-Critic (SAC) is an off-policy actor-critic reinforcement learning algorithm, essentially based on entropy regularization. SAC trains a policy by maximizing the tradeoff between expected return and entropy (randomness in the policy). It has achieved state-of-the-art performance on a range of continuous-control benchmark tasks, outperforming prior onpolicy and off-policy methods. SAC works in an off-policy fashion where data are sampled uniformly from past experiences (stored in a buffer) using which parameters of the policy and value function networks are updated. We propose certain crucial modifications for boosting the performance of SAC and make it more sample efficient. In our proposed improved SAC, we firstly introduce a new prioritization scheme for selecting better samples from the experience replay buffer. Secondly we use a mixture of the prioritized off-policy data with the latest on-policy data for training the policy and the value function networks. We compare our approach with the vanilla SAC and some recent variants of SAC and show that our approach outperforms the said algorithmic benchmarks. It is comparatively more stable and sample efficient when tested on a number of continuous control tasks in MuJoCo environments.", "venue": "arXiv", "keywords": ["reinforcement learning"]}
{"id": "baoBEiTBERTPreTraining2022", "title": "BEiT: BERT Pre-Training of Image Transformers", "abstract": "We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.", "venue": "arXiv", "keywords": []}
{"id": "barannikovManifoldTopologyDivergence", "title": "Manifold Topology Divergence: A Framework for Comparing Data Manifolds", "abstract": "We develop a framework for comparing data manifolds, aimed, in particular, towards the evaluation of deep generative models. We describe a novel tool, Cross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional space, tracks multiscale topology spacial discrepancies between manifolds on which the distributions are concentrated. Based on the Cross-Barcode, we introduce the Manifold Topology Divergence score (MTop-Divergence) and apply it to assess the performance of deep generative models in various domains: images, 3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN, CIFAR10, FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate that the MTop-Divergence accurately detects various degrees of mode-dropping, intramode collapse, mode invention, and image disturbance. Our algorithm scales well (essentially linearly) with the increase of the dimension of the ambient highdimensional space. It is one of the first TDA-based practical methodologies that can be applied universally to datasets of different sizes and dimensions, including the ones on which the most recent GANs in the visual domain are trained. The proposed method is domain agnostic and does not rely on pre-trained networks.", "venue": "", "keywords": ["feature engineering", "manifold learning", "topological analysis"]}
{"id": "barbaraUsePersistentHomology2024", "title": "On the~Use of~Persistent Homology to~Control the~Generalization Capacity of~a~Neural Network", "abstract": "Analyzing neural network (NN) generalization is vital for ensuring effective performance on new, unseen data, beyond the training set. Traditional methods involve evaluating NN across multiple testing datasets, a resource-intensive process involving data acquisition, preprocessing, and labeling. The primary challenge is determining the optimal capacity for training observations, requiring adaptable adjustments based on the task and available data information. This paper leverages Algebraic Topology and relevance measures to investigate NN behavior during learning. We define NN on a topological space as a functional topology graph and compute topological summaries to estimate generalization gaps. Simultaneously, we assess the relevance of NN units, progressively pruning network units. The generalization gap estimation helps identify overfitting, enabling timely early-stopping decisions and identifying the architecture with optimal generalization. This approach offers a comprehensive insight into NN generalization and supports the exploration of NN extensibility and interpretability.", "venue": "Neural Information Processing", "keywords": ["domain generalization", "persistent homology", "topological data analysis"]}
{"id": "barrogaPracticalGuideWriting2022", "title": "A Practical Guide to Writing Quantitative and Qualitative Research Questions and Hypotheses in Scholarly Articles", "abstract": "The development of research questions and the subsequent hypotheses are prerequisites to defining the main research purpose and specific objectives of a study. Consequently, these objectives determine the study design and research outcome. The development of research questions is a process based on knowledge of current trends, cutting-edge studies, and technological advances in the research field. Excellent research questions are focused and require a comprehensive literature search and in-depth understanding of the problem being investigated. Initially, research questions may be written as descriptive questions which could be developed into inferential questions. These questions must be specific and concise to provide a clear foundation for developing hypotheses. Hypotheses are more formal predictions about the research outcomes. These specify the possible results that may or may not be expected regarding the relationship between groups. Thus, research questions and hypotheses clarify the main purpose and specific objectives of the study, which in turn dictate the design of the study, its direction, and outcome. Studies developed from good research questions and hypotheses will have trustworthy outcomes with wide-ranging social and health implications.", "venue": "Journal of Korean Medical Science", "keywords": []}
{"id": "barronGeneralAdaptiveRobust2019", "title": "A General and Adaptive Robust Loss Function", "abstract": "We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.", "venue": "arXiv", "keywords": ["loss functions"]}
{"id": "barzDeepLearningSmall2019", "title": "Deep Learning on Small Datasets without Pre-Training Using Cosine Loss", "abstract": "Two things seem to be indisputable in the contemporary deep learning discourse: 1. The categorical cross-entropy loss after softmax activation is the method of choice for classification. 2. Training a CNN classifier from scratch on small datasets does not work well. In contrast to this, we show that the cosine loss function provides significantly better performance than cross-entropy on datasets with only a handful of samples per class. For example, the accuracy achieved on the CUB-200-2011 dataset without pre-training is by 30% higher than with the cross-entropy loss. Further experiments on other popular datasets confirm our findings. Moreover, we demonstrate that integrating prior knowledge in the form of class hierarchies is straightforward with the cosine loss and improves classification performance further.", "venue": "arXiv", "keywords": ["loss functions"]}
{"id": "batesonSourcefreeDomainAdaptation2022", "title": "Source-Free Domain Adaptation for Image Segmentation", "abstract": "", "venue": "Medical Image Analysis", "keywords": ["domain adaptation", "entropic optimization", "loss functions", "mutual information measures", "semantic segmentation", "source-free da"]}
{"id": "bayramConceptDriftModel2022", "title": "From Concept Drift to Model Degradation: An Overview on Performance-Aware Drift Detectors", "abstract": "The dynamicity of real-world systems poses a significant challenge to deployed predictive machine learning (ML) models. Changes in the system on which the ML model has been trained may lead to performance degradation during the system's life cycle. Recent advances that study non-stationary environments have mainly focused on identifying and addressing such changes caused by a phenomenon called concept drift. Different terms have been used in the literature to refer to the same type of concept drift and the same term for various types. This lack of unified terminology is set out to create confusion on distinguishing between different concept drift variants. In this paper, we start by grouping concept drift types by their mathematical definitions and survey the different terms used in the literature to build a consolidated taxonomy of the field. We also review and classify performance-based concept drift detection methods proposed in the last decade. These methods utilize the predictive model's performance degradation to signal substantial changes in the systems. The classification is outlined in a hierarchical diagram to provide an orderly navigation between the methods. We present a comprehensive analysis of the main attributes and strategies for tracking and evaluating the model's performance in the predictive system. The paper concludes by discussing open research challenges and possible research directions.", "venue": "arXiv", "keywords": ["domain adaptation", "surveys"]}
{"id": "behpourADAGameTheoreticPerspective2017", "title": "ADA: A Game-Theoretic Perspective on Data Augmentation for Object Detection", "abstract": "The use of random perturbations of ground truth data, such as random translation or scaling of bounding boxes, is a common heuristic used for data augmentation that has been shown to prevent overfitting and improve generalization. Since the design of data augmentation is largely guided by reported best practices, it is difficult to understand if those design choices are optimal. To provide a more principled perspective, we develop a game-theoretic interpretation of data augmentation in the context of object detection. We aim to find an optimal adversarial perturbations of the ground truth data (i.e., the worst case perturbations) that forces the object bounding box predictor to learn from the hardest distribution of perturbed examples for better test-time performance. We establish that the game theoretic solution, the Nash equilibrium, provides both an optimal predictor and optimal data augmentation distribution. We show that our adversarial method of training a predictor can significantly improve test time performance for the task of object detection. On the ImageNet object detection task, our adversarial approach improves performance by over 16 % compared to the best performing data augmentation method", "venue": "arXiv", "keywords": []}
{"id": "ben-davidAnalysisRepresentationsDomain2006", "title": "Analysis of Representations for Domain Adaptation", "abstract": "Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. In many situations, though, we have labeled training data for a source domain, and we wish to learn a classifier which performs well on a target domain with a different distribution. Under what conditions can we adapt a classifier trained on the source domain for use in the target domain? Intuitively, a good feature representation is a crucial factor in the success of domain adaptation. We formalize this intuition theoretically with a generalization bound for domain adaption. Our theory illustrates the tradeoffs inherent in designing a representation for domain adaptation and gives a new justification for a recently proposed model. It also points toward a promising new model for domain adaptation: one which explicitly minimizes the difference between the source and target domains, while at the same time maximizing the margin of the training set.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["highly-analytical", "representation learning"]}
{"id": "ben-davidAnalysisRepresentationsDomain2006a", "title": "Analysis of Representations for Domain Adaptation", "abstract": "Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. In many situations, though, we have labeled training data for a source domain, and we wish to learn a classifier which performs well on a target domain with a different distribution. Under what conditions can we adapt a classifier trained on the source domain for use in the target domain? Intuitively, a good feature representation is a crucial factor in the success of domain adaptation. We formalize this intuition theoretically with a generalization bound for domain adaption. Our theory illustrates the tradeoffs inherent in designing a representation for domain adaptation and gives a new justification for a recently proposed model. It also points toward a promising new model for domain adaptation: one which explicitly minimizes the difference between the source and target domains, while at the same time maximizing the margin of the training set.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["domain adaptation", "generalization quantification", "highly-analytical"]}
{"id": "ben-davidAnalysisRepresentationsDomain2006b", "title": "Analysis of Representations for Domain Adaptation", "abstract": "Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. In many situations, though, we have labeled training data for a source domain, and we wish to learn a classifier which performs well on a target domain with a different distribution. Under what conditions can we adapt a classifier trained on the source domain for use in the target domain? Intuitively, a good feature representation is a crucial factor in the success of domain adaptation. We formalize this intuition theoretically with a generalization bound for domain adaption. Our theory illustrates the tradeoffs inherent in designing a representation for domain adaptation and gives a new justification for a recently proposed model. It also points toward a promising new model for domain adaptation: one which explicitly minimizes the difference between the source and target domains, while at the same time maximizing the margin of the training set.", "venue": "Advances in Neural Information Processing Systems", "keywords": []}
{"id": "ben-davidTheoryLearningDifferent2010", "title": "A Theory of Learning from Different Domains", "abstract": "Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. Often, however, we have plentiful labeled training data from a source domain but wish to learn a classifier which performs well on a target domain with a different distribution and little or no labeled training data. In this work we investigate two questions. First, under what conditions can a classifier trained from source data be expected to perform well on target data? Second, given a small amount of labeled target data, how should we combine it during training with the large amount of labeled source data to achieve the lowest target error at test time?", "venue": "Machine Learning", "keywords": ["foundational", "generalization certification", "generalization quantification"]}
{"id": "bengioRepresentationLearningReview2014", "title": "Representation Learning: A Review and New Perspectives", "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.", "venue": "arXiv", "keywords": ["foundational", "representation learning", "surveys"]}
{"id": "bengioRepresentationLearningReview2014a", "title": "Representation Learning: A Review and New Perspectives", "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.", "venue": "arXiv", "keywords": []}
{"id": "benoitUNRAVELINGKEYCOMPONENTS2024", "title": "UNRAVELING THE KEY COMPONENTS OF OOD GENERALIZATION VIA DIVERSIFICATION", "abstract": "Supervised learning datasets may contain multiple cues that explain the training set equally well, i.e., learning any of them would lead to the correct predictions on the training data. However, many of them can be spurious, i.e., lose their predictive power under a distribution shift and consequently fail to generalize to out-ofdistribution (OOD) data. Recently developed \"diversification\" methods (Lee et al., 2023; Pagliardini et al., 2023) approach this problem by finding multiple diverse hypotheses that rely on different features. This paper aims to study this class of methods and identify the key components contributing to their OOD generalization abilities. We show that (1) diversification methods are highly sensitive to the distribution of the unlabeled data used for diversification and can underperform significantly when away from a method-specific sweet spot. (2) Diversification alone is insufficient for OOD generalization. The choice of the used learning algorithm, e.g., the model's architecture and pretraining, is crucial. In standard experiments (classification on Waterbirds and Office-Home datasets), using the second-best choice leads to an up to 20% absolute drop in accuracy. (3) The optimal choice of learning algorithm depends on the unlabeled data and vice versa, i.e., they are co-dependent. (4) Finally, we show that, in practice, the above pitfalls cannot be alleviated by increasing the number of diverse hypotheses, the major feature of diversification methods. These findings provide a clearer understanding of the critical design factors influencing the OOD generalization abilities of diversification methods. They can guide practitioners in how to use the existing methods best and guide researchers in developing new, better ones.", "venue": "", "keywords": []}
{"id": "bernsteinTopologicalDataAnalysis2020", "title": "Topological Data Analysis in Computer Vision", "abstract": "The paper will provide examples of computer vision tasks in which topological data analysis gave new effective solutions. Ideas underlying topological data analysis and its basic methods will be briefly described and illustrated with examples of computer vision problems. No prior knowledge in topological data analysis and computational geometry is assumed, a brief introduction to subject is given throughout the text.", "venue": "Twelfth International Conference on Machine Vision (ICMV 2019)", "keywords": []}
{"id": "berryConsistentManifoldRepresentation2019", "title": "Consistent Manifold Representation for Topological Data Analysis", "abstract": "For data sampled from an arbitrary density on a manifold embedded in Euclidean space, the Continuous k-Nearest Neighbors (CkNN) graph construction is introduced. It is shown that CkNN is geometrically consistent in the sense that under certain conditions, the unnormalized graph Laplacian converges to the Laplace-Beltrami operator, spectrally as well as pointwise. It is proved for compact (and conjectured for noncompact) manifolds that CkNN is the unique unweighted construction that yields a geometry consistent with the connected components of the underlying manifold in the limit of large data. Thus CkNN produces a single graph that captures all topological features simultaneously, in contrast to persistent homology, which represents each homology generator at a separate scale. As applications we derive a new fast clustering algorithm and a method to identify patterns in natural images topologically. Finally, we conjecture that CkNN is topologically consistent, meaning that the homology of the Vietoris-Rips complex (implied by the graph Laplacian) converges to the homology of the underlying manifold (implied by the Laplace-de Rham operators) in the limit of large data.", "venue": "arXiv", "keywords": ["manifold learning", "topological data analysis"]}
{"id": "bethunePayAttentionYour", "title": "Pay Attention to Your Loss: Understanding Misconceptions about 1-Lipschitz Neural Networks", "abstract": "Lipschitz constrained networks have gathered considerable attention in the deep learning community, with usages ranging from Wasserstein distance estimation to the training of certifiably robust classifiers. However they remain commonly considered as less accurate, and their properties in learning are still not fully understood. In this paper we clarify the matter: when it comes to classification 1-Lipschitz neural networks enjoy several advantages over their unconstrained counterpart. First, we show that these networks are as accurate as classical ones, and can fit arbitrarily difficult boundaries. Then, relying on a robustness metric that reflects operational needs we characterize the most robust classifier: the WGAN discriminator. Next, we show that 1-Lipschitz neural networks generalize well under milder assumptions. Finally, we show that hyper-parameters of the loss are crucial for controlling the accuracy-robustness trade-off. We conclude that they exhibit appealing properties to pave the way toward provably accurate, and provably robust neural networks.", "venue": "", "keywords": ["lipschitz-constraints"]}
{"id": "beznosikovBiasedCompressionDistributed", "title": "On Biased Compression for Distributed Learning", "abstract": "", "venue": "", "keywords": []}
{"id": "bhattacharyyaHybridStyleSiamese2020", "title": "Hybrid Style Siamese Network: Incorporating Style Loss in Complementary Apparels Retrieval", "abstract": "Image Retrieval grows to be an integral part of fashion e-commerce ecosystem as it keeps expanding in multitudes. Other than the retrieval of visually similar items, the retrieval of visually compatible or complementary items is also an important aspect of it. Normal Siamese Networks tend to work well on complementary items retrieval. But it fails to identify low level style features which make items compatible in human eyes. These low level style features are captured to a large extent in techniques used in neural style transfer. This paper proposes a mechanism of utilising those methods in this retrieval task and capturing the low level style features through a hybrid siamese network coupled with a hybrid loss. The experimental results indicate that the proposed method outperforms traditional siamese networks in retrieval tasks for complementary items.", "venue": "arXiv", "keywords": ["siamese networks", "style transfer"]}
{"id": "bijelicBenchmarkingImageSensors2018", "title": "Benchmarking Image Sensors Under Adverse Weather Conditions for Autonomous Driving", "abstract": "Adverse weather conditions are very challenging for autonomous driving because most of the state-of-the-art sensors stop working reliably under these conditions. In order to develop robust sensors and algorithms, tests with current sensors in defined weather conditions are crucial for determining the impact of bad weather for each sensor. This work describes a testing and evaluation methodology that helps to benchmark novel sensor technologies and compare them to state-of-the-art sensors. As an example, gated imaging is compared to standard imaging under foggy conditions. It is shown that gated imaging outperforms state-of-the-art standard passive imaging due to time-synchronized active illumination.", "venue": "2018 IEEE Intelligent Vehicles Symposium (IV)", "keywords": ["adverse weather", "motivation citation"]}
{"id": "blanchardDomainGeneralizationMarginal2017", "title": "Domain Generalization by Marginal Transfer Learning", "abstract": "In the problem of domain generalization (DG), there are labeled training data sets from several related prediction problems, and the goal is to make accurate predictions on future unlabeled data sets that are not known to the learner. This problem arises in several applications where data distributions fluctuate because of environmental, technical, or other sources of variation. We introduce a formal framework for DG, and argue that it can be viewed as a kind of supervised learning problem by augmenting the original feature space with the marginal distribution of feature vectors. While our framework has several connections to conventional analysis of supervised learning algorithms, several unique aspects of DG require new methods of analysis. This work lays the learning theoretic foundations of domain generalization, building on our earlier conference paper where the problem of DG was introduced (Blanchard et al., 2011). We present two formal models of data generation, corresponding notions of risk, and distribution-free generalization error analysis. By focusing our attention on kernel methods, we also provide more quantitative results and a universally consistent algorithm. An efficient implementation is provided for this algorithm, which is experimentally compared to a pooling strategy on one synthetic and three real-world data sets.", "venue": "arXiv.org", "keywords": ["domain generalization", "generalization certification", "highly-analytical", "kernel methods", "risk optimization", "transfer learning"]}
{"id": "blanchardGeneralizingSeveralRelated2011", "title": "Generalizing from Several Related Classification Tasks to a New Unlabeled Sample", "abstract": "We consider the problem of assigning class labels to an unlabeled test data set, given several labeled training data sets drawn from similar distributions. This problem arises in several applications where data distributions fluctuate because of biological, technical, or other sources of variation. We develop a distribution-free, kernel-based approach to the problem. This approach involves identifying an appropriate reproducing kernel Hilbert space and optimizing a regularized empirical risk over the space. We present generalization error analysis, describe universal kernels, and establish universal consistency of the proposed methodology. Experimental results on flow cytometry data are presented.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["generalization quantification", "highly-analytical", "kernel methods"]}
{"id": "bleiVariationalInferenceReview2017", "title": "Variational Inference: A Review for Statisticians", "abstract": "One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.", "venue": "Journal of the American Statistical Association", "keywords": ["distribution estimation", "manifold learning", "surveys", "variational inference"]}
{"id": "blottSemanticSegmentationFisheye2019", "title": "Semantic Segmentation of Fisheye Images", "abstract": "Semantic segmentation of fisheye images (e.g., from actioncameras or smartphones) requires different training approaches and data than those of rectilinear images obtained using central projection. The shape of objects is distorted depending on the distance between the principal point and the object position in the image. Therefore, classical semantic segmentation approaches fall short in terms of performance compared to rectilinear data. A potential solution to this problem is the recording and annotation of a new dataset, however this is expensive and tedious. In this study, an alternative approach that modifies the augmentation stage of deep learning training to re-use rectilinear training data is presented. In this way we obtain a considerably higher semantic segmentation performance on the fisheye images: +18.3% intersection over union (IoU) for action-camera test images, +8.3% IoU for artificially generated fisheye data, and +18.0% IoU for challenging security scenes acquired in bird's eye view.", "venue": "Computer Vision -- ECCV 2018 Workshops", "keywords": ["fisheye distortion", "semantic segmentation"]}
{"id": "bogdanDeepCalibDeepLearning2018", "title": "DeepCalib: A Deep Learning Approach for Automatic Intrinsic Calibration of Wide Field-of-View Cameras", "abstract": "Calibration of wide field-of-view cameras is a fundamental step for numerous visual media production applications, such as 3D reconstruction, image undistortion, augmented reality and camera motion estimation. However, existing calibration methods require multiple images of a calibration pattern (typically a checkerboard), assume the presence of lines, require manual interaction and/or need an image sequence. In contrast, we present a novel fully automatic deep learning-based approach that overcomes all these limitations and works with a single image of general scenes. Our approach builds upon the recent developments in deep Convolutional Neural Networks (CNN): our network automatically estimates the intrinsic parameters of the camera (focal length and distortion parameter) from a single input image. In order to train the CNN, we leverage the great amount of omnidirectional images available on the Internet to automatically generate a large-scale dataset composed of millions of wide field-of-view images with ground truth intrinsic parameters. Experiments successfully demonstrated the quality of our results, both quantitatively and qualitatively.", "venue": "Proceedings of the 15th ACM SIGGRAPH European Conference on Visual Media Production", "keywords": ["camera calibration", "fisheye distortion"]}
{"id": "bohraLearningActivationFunctions2020", "title": "Learning Activation Functions in Deep (Spline) Neural Networks", "abstract": "", "venue": "IEEE Open Journal of Signal Processing", "keywords": ["neural splines"]}
{"id": "bonneelSlicedRadonWasserstein2015", "title": "Sliced and Radon Wasserstein Barycenters of Measures", "abstract": "This article details two approaches to compute barycenters of measures using 1-D Wasserstein distances along radial projections of the input measures. The first method makes use of the Radon transform of the measures, and the second is the solution of a convex optimization problem over the space of measures. We show several properties of these barycenters and explain their relationship. We show numerical approximation schemes based on a discrete Radon transform and on the resolution of a non-convex optimization problem. We explore the respective merits and drawbacks of each approach on applications to two image processing problems: color transfer and texture mixing.", "venue": "Journal of Mathematical Imaging and Vision", "keywords": ["optimal transport", "texture transfer"]}
{"id": "bonneelSPOTSlicedPartial2019", "title": "SPOT: Sliced Partial Optimal Transport", "abstract": "Optimal transport research has surged in the last decade with wide applications in computer graphics. In most cases, however, it has focused on the special case of the so-called \"balanced\" optimal transport problem, that is, the problem of optimally matching positive measures of equal total mass. While this approach is suitable for handling probability distributions as their total mass is always equal to one, it precludes other applications manipulating disparate measures. Our paper proposes a fast approach to the optimal transport of constant distributions supported on point sets of different cardinality via one-dimensional slices. This leads to one-dimensional partial assignment problems akin to alignment problems encountered in genomics or text comparison. Contrary to one-dimensional balanced optimal transport that leads to a trivial linear-time algorithm, such partial optimal transport, even in 1-d, has not seen any closed-form solution nor very efficient algorithms to date. We provide the first efficient 1-d partial optimal transport solver. Along with a quasilinear time problem decomposition algorithm, it solves 1-d assignment problems consisting of up to millions of Dirac distributions within fractions of a second in parallel. We handle higher dimensional problems via a slicing approach, and further extend the popular iterative closest point algorithm using optimal transport - an algorithm we call Fast Iterative Sliced Transport. We illustrate our method on computer graphics applications such a color transfer and point cloud registration.", "venue": "ACM Transactions on Graphics", "keywords": ["optimal transport"]}
{"id": "bonneelSurveyOptimalTransport2023", "title": "A Survey of Optimal Transport for Computer Graphics and Computer Vision", "abstract": "Optimal transport is a long-standing theory that has been studied in depth from both theoretical and numerical point of views. Starting from the 50s this theory has also found a lot of applications in operational research. Over the last 30 years it has spread to computer vision and computer graphics and is now becoming hard to ignore. Still, its mathematical complexity can make it difficult to comprehend, and as such, computer vision and computer graphics researchers may find it hard to follow recent developments in their field related to optimal transport. This survey first briefly introduces the theory of optimal transport in layman's terms as well as most common numerical techniques to solve it. More importantly, it presents applications of these numerical techniques to solve various computer graphics and vision related problems. This involves applications ranging from image processing, geometry processing, rendering, fluid simulation, to computational optics, and many more. It is aimed at computer graphics researchers desiring to follow optimal transport research in their field as well as optimal transport researchers willing to find applications for their numerical algorithms.", "venue": "Computer Graphics Forum", "keywords": ["optimal transport", "surveys"]}
{"id": "bordeGromovHausdorffDistancesComparing2023", "title": "Gromov-Hausdorff Distances for Comparing Product Manifolds of Model Spaces", "abstract": "Recent studies propose enhancing machine learning models by aligning the geometric characteristics of the latent space with the underlying data structure. Instead of relying solely on Euclidean space, researchers have suggested using hyperbolic and spherical spaces with constant curvature, or their combinations (known as product manifolds), to improve model performance. However, there exists no principled technique to determine the best latent product manifold signature, which refers to the choice and dimensionality of manifold components. To address this, we introduce a novel notion of distance between candidate latent geometries using the GromovHausdorff distance from metric geometry. We propose using a graph search space that uses the estimated Gromov-Hausdorff distances to search for the optimal latent geometry. In this work we focus on providing a description of an algorithm to compute the Gromov-Hausdorff distance between model spaces and its computational implementation.", "venue": "arXiv", "keywords": ["manifold learning"]}
{"id": "bordeNeuralLatentGeometry2023", "title": "Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization", "abstract": "Recent research indicates that the performance of machine learning models can be improved by aligning the geometry of the latent space with the underlying data structure. Rather than relying solely on Euclidean space, researchers have proposed using hyperbolic and spherical spaces with constant curvature, or combinations thereof, to better model the latent space and enhance model performance. However, little attention has been given to the problem of automatically identifying the optimal latent geometry for the downstream task. We mathematically define this novel formulation and coin it as neural latent geometry search (NLGS). More specifically, we introduce an initial attempt to search for a latent geometry composed of a product of constant curvature model spaces with a small number of query evaluations, under some simplifying assumptions. To accomplish this, we propose a novel notion of distance between candidate latent geometries based on the Gromov-Hausdorff distance from metric geometry. In order to compute the Gromov-Hausdorff distance, we introduce a mapping function that enables the comparison of different manifolds by embedding them in a common highdimensional ambient space. We then design a graph search space based on the notion of smoothness between latent geometries, and employ the calculated distances as an additional inductive bias. Finally, we use Bayesian optimization to search for the optimal latent geometry in a query-efficient manner. This is a general method which can be applied to search for the optimal latent geometry for a variety of models and downstream tasks. We perform experiments on synthetic and real-world datasets to identify the optimal latent geometry for multiple machine learning problems.", "venue": "arXiv", "keywords": ["manifold learning", "neural architecture search", "topological analysis"]}
{"id": "borsdorfComputingNearestCorrelation2010", "title": "Computing a Nearest Correlation Matrix with Factor Structure", "abstract": "Indefinite approximations of positive semidefinite matrices arise in various data analysis applications involving covariance matrices and correlation matrices. We propose a method for restoring positive semidefiniteness of an indefinite matrix \\ that constructs a convex linear combination \\ of \\ and a positive semidefinite target matrix \\ . In statistics, this construction for improving an estimate \\ by combining it with new information in \\ is known as shrinking. We make no statistical assumptions about \\ and define the optimal shrinking parameter as \\ S( )\\ . We describe three s for computing \\ . One algorithm is based on the bisection method, with the use of Cholesky factorization to test definiteness; a second employs Newton's method; and a third finds the smallest eigenvalue of a symmetric definite generalized eigenvalue problem. We show that weights that reflect confidence in the individual entries of \\ can be used to construct a natural choice of the target matrix \\ . We treat in detail a problem variant in which a positive semidefinite leading principal submatrix of \\ remains fixed, showing how the fixed block can be exploited to reduce the cost of the bisection and generalized eigenvalue methods. Numerical experiments show that when applied to indefinite approximations of correlation matrices shrinking can be at least an order of magnitude faster than computing the nearest correlation matrix.", "venue": "SIAM Journal on Matrix Analysis and Applications", "keywords": ["feature engineering"]}
{"id": "brahmaProbabilisticFrameworkLifelong2023", "title": "A Probabilistic Framework for Lifelong Test-Time Adaptation", "abstract": "", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": ["domain adaptation", "lifelong learning", "self-training", "test-time da"]}
{"id": "brandstetterMessagePassingNeural2023", "title": "Message Passing Neural PDE Solvers", "abstract": "The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, equation parameters, discretizations, etc., in 1D and 2D.", "venue": "arXiv", "keywords": []}
{"id": "braunVariationalInferenceLargeScale2010", "title": "Variational Inference for Large-Scale Models of Discrete Choice", "abstract": "", "venue": "Journal of the American Statistical Association", "keywords": ["distribution estimation", "variational inference"]}
{"id": "bruggemannRefignAlignRefine2023", "title": "Refign: Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions", "abstract": "", "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision", "keywords": ["adverse weather", "domain adaptation", "self-training", "semantic segmentation", "unsupervised da"]}
{"id": "bruintjesVIPriorsVisualInductive2021", "title": "VIPriors 1: Visual Inductive Priors for Data-Efficient Deep Learning Challenges", "abstract": "We present the first edition of \"VIPriors: Visual Inductive Priors for Data-Efficient Deep Learning\" challenges. We offer four data-impaired challenges, where models are trained from scratch, and we reduce the number of training samples to a fraction of the full set. Furthermore, to encourage data efficient solutions, we prohibited the use of pre-trained models and other transfer learning techniques. The majority of top ranking solutions make heavy use of data augmentation, model ensembling, and novel and efficient network architectures to achieve significant performance increases compared to the provided baselines.", "venue": "arXiv", "keywords": ["challenges", "feature-level augmentation"]}
{"id": "bruntonDataDrivenScience2017", "title": "Data Driven Science & Engineering - Machine Learning, Dynamical Systems, and Control", "abstract": "", "venue": "", "keywords": []}
{"id": "bubenikStatisticalTopologicalData", "title": "Statistical Topological Data Analysis Using Persistence Landscapes", "abstract": "We define a new topological summary for data that we call the persistence landscape. Since this summary lies in a vector space, it is easy to combine with tools from statistics and machine learning, in contrast to the standard topological summaries. Viewed as a random variable with values in a Banach space, this summary obeys a strong law of large numbers and a central limit theorem. We show how a number of standard statistical tests can be used for statistical inference using this summary. We also prove that this summary is stable and that it can be used to provide lower bounds for the bottleneck and Wasserstein distances.", "venue": "", "keywords": ["persistent homology", "topological data analysis"]}
{"id": "buitelaarTopicExtractionScientific", "title": "Topic Extraction from Scientific Literature for Competency Management", "abstract": "We describe an approach towards automatic, dynamic and timecritical support for competency management and expertise search through topic extraction from scientific publications. In the use case we present, we focus on the automatic extraction of scientific topics and technologies from publicly available publications using web sites like Google Scholar. We discuss an experiment for our own organization, DFKI, as example of a knowledge organization. The paper presents evaluation results over a sample of 48 DFKI researchers that responded to our request for a-posteriori evaluation of automatically extracted topics. The results of this evaluation are encouraging and provided us with useful feedback for further improving our methods. The extracted topics can be organized in an association network that can be used further to analyze how competencies are interconnected, thereby enabling also a better exchange of expertise and competence between researchers.", "venue": "", "keywords": []}
{"id": "buslaevAlbumentationsFastFlexible2020", "title": "Albumentations: Fast and Flexible Image Augmentations", "abstract": "Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve corresponding output labels. In computer vision, image augmentations have become a common implicit regularization technique to combat overfitting in deep learning models and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations of flipping, rotating, scaling, and cropping. Moreover, image processing speed varies in existing image augmentation libraries. We present Albumentations, a fast and flexible open source library for image augmentation with many various image transform operations available that is also an easy-to-use wrapper around other augmentation libraries. We discuss the design principles that drove the implementation of Albumentations and give an overview of the key features and distinct capabilities. Finally, we provide examples of image augmentations for different computer vision tasks and demonstrate that Albumentations is faster than other commonly used image augmentation tools on most image transform operations.", "venue": "Information", "keywords": ["surveys", "traditional augmentation"]}
{"id": "caiImageMultiInpaintingProgressive2022", "title": "Image Multi-Inpainting via Progressive Generative Adversarial Networks", "abstract": "Image inpainting aims to inpaint missing pixels of an image naturally and realistically. Previous deep learning approaches typically require specific design for different types of masks and cannot generalize well to multiple inpainting scenarios simultaneously. Thus on top of most common stroke-type mask approaches, we in this paper propose a unified framework to handle multiple types of masks simultaneously (e.g. strokes, object shapes, extrapolation, dense and periodic grids et al). We address this problem by proposing a progressive learning scheme to an Semantic Aware Generative Adversarial Network (SA-PatchGAN). Specifically, the overall training proceeds in multiple stages with different type of mask inputs, so that the model can gradually generate an output image from coarse to fine with mask independent property. In our experiments, we show that this strategy yields a large performance gain compared to the single-scale learning methods. We also introduce additional semantic conditioning to the discriminator which encourage high quality local style statistics, and show that this approach is effective on a wider scenario/tasks and could better adapt to various types of mask. Our method produces promising results on various mask types using one single model.", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "keywords": ["adversarial learning", "gans", "image inpainting", "image-to-image"]}
{"id": "CameraModelingExploring", "title": "Camera Modeling: Exploring Distortion and Distortion Models, Part II", "abstract": "Despite the proliferation and prevalence of cameras and vision-enabled devices over the past century or so, there have been two primary distortion models that have gained widespread adoption to provide correction. We'll go over these, and dive into the math and approach to ground you in these techniques.", "venue": "", "keywords": []}
{"id": "caoSurveyImageSemantic2020", "title": "A Survey On Image Semantic Segmentation Methods With Convolutional Neural Network", "abstract": "", "venue": "2020 International Conference on Communications, Information System and Computer Engineering (CISCE)", "keywords": ["cnns", "semantic segmentation", "surveys"]}
{"id": "caoUseSquareRoot2021", "title": "Use Square Root Affinity to Regress Labels in Semantic Segmentation", "abstract": "Semantic segmentation is a basic but non-trivial task in computer vision. Many previous work focus on utilizing affinity patterns to enhance segmentation networks. Most of these studies use the affinity matrix as a kind of feature fusion weights, which is part of modules embedded in the network, such as attention models and non-local models. In this paper, we associate affinity matrix with labels, exploiting the affinity in a supervised way. Specifically, we utilize the label to generate a multi-scale label affinity matrix as a structural supervision, and we use a square root kernel to compute a non-local affinity matrix on output layers. With such two affinities, we define a novel loss called Affinity Regression loss (AR loss), which can be an auxiliary loss providing pair-wise similarity penalty. Our model is easy to train and adds little computational burden without run-time inference. Extensive experiments on NYUv2 dataset and Cityscapes dataset demonstrate that our proposed method is sufficient in promoting semantic segmentation networks.", "venue": "arXiv", "keywords": ["affinity modeling", "semantic segmentation"]}
{"id": "caponnettoUniversalMultiTaskKernels", "title": "Universal Multi-Task Kernels", "abstract": "In this paper we are concerned with reproducing kernel Hilbert spaces HK of functions from an input space into a Hilbert space Y , an environment appropriate for multi-task learning. The reproducing kernel K associated to HK has its values as operators on Y . Our primary goal here is to derive conditions which ensure that the kernel K is universal. This means that on every compact subset of the input space, every continuous function with values in Y can be uniformly approximated by sections of the kernel. We provide various characterizations of universal kernels and highlight them with several concrete examples of some practical importance. Our analysis uses basic principles of functional analysis and especially the useful notion of vector measures which we describe in sufficient detail to clarify our results.", "venue": "", "keywords": ["highly-analytical", "kernel methods"]}
{"id": "carlsonModelingCameraEffects2018", "title": "Modeling Camera Effects to Improve Visual Learning from Synthetic Data", "abstract": "", "venue": "Proceedings of the European Conference on Computer Vision (ECCV) Workshops", "keywords": ["sim-to-real"]}
{"id": "carlssonTopologicalDataAnalysis", "title": "Topological Data Analysis and Machine Learning Theory", "abstract": "Biological applications. The past decade has witnessed developments in the field of biology that have brought about profound changes in understanding the dynamic of disease and of biological systems in general. New technology has given biologists an unprecedented wealth of information, but it has generated data that is hard to analyze mathematically, thereby making its biological interpretation difficult. These challenges, stemming in part from the very high dimensionality of the data, have given rise to a myriad novel exciting mathematical problems and have provided an impetus to modify and adapt traditional mathematics tools, as well as develop novel techniques to tackle the data analysis problems raised in biology. Monica Nicolau discussed data transformations and topological methods for solving biology-driven problems. The general approach of her work was to address some of the computational challenges of these large data types by combining data transformations and topological methods. Through the definition of high-dimensional mathematical models for different biological states, for example healthy vs. disease, or various developmental stages of cells, data can be transformed to mod out all characteristics but those relevant or statistically significant to the problem under study. Once data has been transformed, analysis of significance involves analysis of the shape of the data. Nicolau described how adaptation of topological methods in the context of discrete point clouds has proved to be a powerful tool for identifying statistical significance as well as providing methods for visualizing data.", "venue": "", "keywords": []}
{"id": "carlssonTopologicalMethodsData2020", "title": "Topological Methods for Data Modelling", "abstract": "The analysis of large and complex data sets is one of the most important problems facing the scientific community, and physics in particular. One response to this challenge has been the development of topological data analysis (TDA), which models data by graphs or networks rather than by linear algebraic (matrix) methods or cluster analysis. TDA represents the shape of the data (suitably defined) in a combinatorial fashion. Methods for measuring shape have been developed within mathematics, providing a toolkit referred to as homology. In working with data, one can use this kind of modelling to obtain an understanding of the overall structure of the data set. There is a suite of methods for constructing vector representations of various kinds of unstructured data. In this Review, we sketch the basics of TDA and provide examples where this kind of analysis has been carried out.", "venue": "Nature Reviews Physics", "keywords": []}
{"id": "carpentieroDistributedAdaptiveLearning2021", "title": "Distributed Adaptive Learning Under Communication Constraints", "abstract": "This work examines adaptive distributed learning strategies designed to operate under communication constraints. We consider a network of agents that must solve an online optimization problem from continual observation of streaming data. The agents implement a distributed cooperative strategy where each agent is allowed to perform local exchange of information with its neighbors. In order to cope with communication constraints, the exchanged information must be unavoidably compressed. We propose a diffusion strategy nicknamed as ACTC (Adapt-Compress-Then-Combine), which relies on the following steps: i) an adaptation step where each agent performs an individual stochastic-gradient update with constant step-size; ii) a compression step that leverages a recently introduced class of stochastic compression operators; and iii) a combination step where each agent combines the compressed updates received from its neighbors. The distinguishing elements of this work are as follows. First, we focus on adaptive strategies, where constant (as opposed to diminishing) step-sizes are critical to respond in real time to nonstationary variations. Second, we consider the general class of directed graphs and left-stochastic combination policies, which allow us to enhance the interplay between topology and learning. Third, in contrast with related works that assume strong convexity for all individual agents' cost functions, we require strong convexity only at a network level, a condition satisfied even if a single agent has a strongly-convex cost and the remaining agents have non-convex costs. Fourth, we focus on a diffusion (as opposed to consensus) strategy. Under the demanding setting of compressed information, we establish that the ACTC iterates fluctuate around the desired optimizer, achieving remarkable savings in terms of bits exchanged between neighboring agents.", "venue": "arXiv", "keywords": ["adaptive learning"]}
{"id": "casarinYourImageMy2024", "title": "Your Image Is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion", "abstract": "", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": []}
{"id": "CausalInferenceMeets", "title": "Causal Inference Meets Deep Learning: A Comprehensive Survey", "abstract": "", "venue": "", "keywords": []}
{"id": "chadebecDataAugmentationVariational2021", "title": "Data Augmentation with Variational Autoencoders and Manifold Sampling", "abstract": "We propose a new efficient way to sample from a Variational Autoencoder in the challenging low sample size setting. This method reveals particularly well suited to perform data augmentation in such a low data regime and is validated across various standard and real-life data sets. In particular, this scheme allows to greatly improve classification results on the OASIS database where balanced accuracy jumps from 80.7% for a classifier trained with the raw data to 88.6% when trained only with the synthetic data generated by our method. Such results were also observed on 3 standard data sets and with other classifiers. A code is available at https://github.com/clementchadebec/Data_Augmentation_with_VAE-DALI.", "venue": "arXiv", "keywords": []}
{"id": "changDomainSpecificBatchNormalization2019", "title": "Domain-Specific Batch Normalization for Unsupervised Domain Adaptation", "abstract": "We propose a novel unsupervised domain adaptation framework based on domain-specific batch normalization in deep neural networks. We aim to adapt to both domains by specializing batch normalization layers in convolutional neural networks while allowing them to share all other model parameters, which is realized by a twostage algorithm. In the first stage, we estimate pseudolabels for the examples in the target domain using an external unsupervised domain adaptation algorithm---for example, MSTN or CPUA ---integrating the proposed domain-specific batch normalization. The second stage learns the final models using a multi-task classification loss for the source and target domains. Note that the two domains have separate batch normalization layers in both stages. Our framework can be easily incorporated into the domain adaptation techniques based on deep neural networks with batch normalization layers. We also present that our approach can be extended to the problem with multiple source domains. The proposed algorithm is evaluated on multiple benchmark datasets and achieves the state-of-theart accuracy in the standard setting and the multi-source domain adaption scenario.", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["distribution estimation", "domain adaptation", "unsupervised da"]}
{"id": "chanImprovedMethodFisheye2016", "title": "An Improved Method for Fisheye Camera Calibration and Distortion Correction", "abstract": "The fisheye camera has been widely studied in the field of robot vision since it can capture a wide view of the scene at one time. However, serious image distortion handers it from being widely used. To remedy this, this paper proposes an improved fisheye lens calibration and distortion correction method. First, an improved automatic detection of checkerboards is presented to avoid the original constraint and user intervention that usually existed in the conventional methods. A state-of-the-art corner detection method is evaluated and its strengths and shortcomings are analyzed. An adaptively automatic corner detection algorithm is implemented to overcome the shortcomings. Then, a precise mathematical model based on the law of fisheye lens imaging is modeled, which assumes that the imaging function can be described by a Taylor series expansion, followed by a nonlinear refinement based on the maximum likelihood criterion. With the proposed corner detection and mathematical model of fisheye imaging, both intrinsic and external parameters of the fisheye camera can be correctly calibrated. Finally, the radial distortion of the fisheye image can be corrected by incorporating the calibrated parameters. Experimental results validate the effectiveness of the proposed method.", "venue": "", "keywords": ["camera calibration", "fisheye distortion"]}
{"id": "chazalIntroductionTopologicalData2021", "title": "An Introduction to Topological Data Analysis: Fundamental and Practical Aspects for Data Scientists", "abstract": "Topological Data Analysis (TDA ) is a recent and fast growing field providing a set of new topological and geometric tools to infer relevant features for possibly complex data. This paper is a brief introduction, through a few selected topics, to basic fundamental and practical aspects of TDA for non experts.", "venue": "Frontiers in Artificial Intelligence", "keywords": ["topological data analysis"]}
{"id": "cheD$^2$CityLargeScaleDashcam2019", "title": "D\\ -City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios", "abstract": "Driving datasets accelerate the development of intelligent driving and related computer vision technologies, while substantial and detailed annotations serve as fuels and powers to boost the efficacy of such datasets to improve learning-based models. We propose D\\ -City, a large-scale comprehensive collection of dashcam videos collected by vehicles on DiDi's platform. D\\ -City contains more than 10000 video clips which deeply reflect the diversity and complexity of real-world traffic scenarios in China. We also provide bounding boxes and tracking annotations of 12 classes of objects in all frames of 1000 videos and detection annotations on keyframes for the remainder of the videos. Compared with existing datasets, D\\ -City features data in varying weather, road, and traffic conditions and a huge amount of elaborate detection and tracking annotations. By bringing a diverse set of challenging cases to the community, we expect the D\\ -City dataset will advance the perception and related areas of intelligent driving.", "venue": "arXiv", "keywords": ["dataset debut"]}
{"id": "chenArtisticStyleTransfer2021", "title": "Artistic Style Transfer with Internal-external Learning and Contrastive Learning", "abstract": "Although existing artistic style transfer methods have achieved significant improvement with deep neural networks, they still suffer from artifacts such as disharmonious colors and repetitive patterns. Motivated by this, we propose an internal-external style transfer method with two contrastive losses. Specifically, we utilize internal statistics of a single style image to determine the colors and texture patterns of the stylized image, and in the meantime, we leverage the external information of the large-scale style dataset to learn the human-aware style information, which makes the color distributions and texture patterns in the stylized image more reasonable and harmonious. In addition, we argue that existing style transfer methods only consider the content-to-stylization and style-to-stylization relations, neglecting the stylization-to-stylization relations. To address this issue, we introduce two contrastive losses, which pull the multiple stylization embeddings closer to each other when they share the same content or style, but push far away otherwise. We conduct extensive experiments, showing that our proposed method can not only produce visually more harmonious and satisfying artistic images, but also promote the stability and consistency of rendered video clips.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["contrastive learning", "style transfer"]}
{"id": "chenCapturingSpatiotemporalContinuity2019", "title": "Capturing the Spatio-Temporal Continuity for Video Semantic Segmentation", "abstract": "In recent years, image semantic segmentation based on a convolutional neural network has achieved many advances. However, the development of video semantic segmentation is relatively slow. Directly applying the image segmentation algorithms to each video frame separately may ignore the temporal region continuity inherent in videos. In this study, the authors propose a novel deep neural network architecture with a newly devised spatio-temporal continuity (STC) module for video semantic segmentation. Particularly, the architecture includes an encoding network, an STC module, and a decoding network. The encoding network is used to extract a high-level feature map. The STC module then uses the high-level feature map as input to extract the STC feature map. For decoding, they use four dilated convolutional layers to obtain more abstract representation and a deconvolutional layer to increase the size of the representation. Finally, they fuse the current feature representation and the previous feature representation and get the class probabilities. Thus, this architecture receives a sequence of consecutive video frames and outputs the segmentation result of the current frame. They extensively evaluate the proposed approach on the CamVid and KITTI datasets. Compared with other methods, the authors' approach not only achieves competitive performance but also has lower complexity.", "venue": "IET Image Processing", "keywords": ["semantic segmentation", "temporal consistency"]}
{"id": "chenDistributedLearningWireless2021", "title": "Distributed Learning in Wireless Networks: Recent Progress and Future Challenges", "abstract": "The next-generation of wireless networks will enable many machine learning (ML) tools and applications to efficiently analyze various types of data collected by edge devices for inference, autonomy, and decision making purposes. However, due to resource constraints, delay limitations, and privacy challenges, edge devices cannot offload their entire collected datasets to a cloud server for centrally training their ML models or inference purposes. To overcome these challenges, distributed learning and inference techniques have been proposed as a means to enable edge devices to collaboratively train ML models without raw data exchanges, thus reducing the communication overhead and latency as well as improving data privacy. However, deploying distributed learning over wireless networks faces several challenges including the uncertain wireless environment, limited wireless resources (e.g., transmit power and radio spectrum), and hardware resources. This paper provides a comprehensive study of how distributed learning can be efficiently and effectively deployed over wireless edge networks. We present a detailed overview of several emerging distributed learning paradigms, including federated learning, federated distillation, distributed inference, and multi-agent reinforcement learning. For each learning framework, we first introduce the motivation for deploying it over wireless networks. Then, we present a detailed literature review on the use of communication techniques for its efficient deployment. We then introduce an illustrative example to show how to optimize wireless networks to improve its performance. Finally, we introduce future research opportunities. In a nutshell, this paper provides a holistic set of guidelines on how to deploy a broad range of distributed learning frameworks over real-world wireless communication networks.", "venue": "arXiv", "keywords": []}
{"id": "chenDomainAdaptationSemantic2019", "title": "Domain Adaptation for Semantic Segmentation With Maximum Squares Loss", "abstract": "Deep neural networks for semantic segmentation always require a large number of samples with pixel-level labels, which becomes the major difficulty in their real-world applications. To reduce the labeling cost, unsupervised domain adaptation (UDA) approaches are proposed to transfer knowledge from labeled synthesized datasets to unlabeled real-world datasets. Recently, some semi-supervised learning methods have been applied to UDA and achieved state-of-the-art performance. One of the most popular approaches in semi-supervised learning is the entropy minimization method. However, when applying the entropy minimization to UDA for semantic segmentation, the gradient of the entropy is biased towards samples that are easy to transfer. To balance the gradient of well-classified target samples, we propose the maximum squares loss. Our maximum squares loss prevents the training process being dominated by easy-to-transfer samples in the target domain. Besides, we introduce the image-wise weighting ratio to alleviate the class imbalance in the unlabeled target domain. Both synthetic-to-real and cross-city adaptation experiments demonstrate the effectiveness of our proposed approach. The code is released at https://github. com/ZJULearning/MaxSquareLoss.", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "keywords": ["class balancing", "domain adaptation", "loss functions", "semantic segmentation", "unsupervised da"]}
{"id": "chenEncoderDecoderAtrousSeparable2018", "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation", "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0 % and 82.1 % without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at ://github.com/tensorflow/models/tree/master/research/deeplab\\.", "venue": "arXiv", "keywords": []}
{"id": "chenEncoderDecoderAtrousSeparable2018a", "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation", "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0 % and 82.1 % without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at ://github.com/tensorflow/models/tree/master/research/deeplab\\.", "venue": "arXiv", "keywords": ["cnns", "foundational", "semantic segmentation"]}
{"id": "chenExploringSimpleSiamese2020", "title": "Exploring Simple Siamese Representation Learning", "abstract": "Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our \"SimSiam\" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.", "venue": "arXiv", "keywords": ["representation learning", "siamese networks"]}
{"id": "chenFastPatchbasedStyle2016", "title": "Fast Patch-based Style Transfer of Arbitrary Style", "abstract": "Artistic style transfer is an image synthesis problem where the content of an image is reproduced with the style of another. Recent works show that a visually appealing style transfer can be achieved by using the hidden activations of a pretrained convolutional neural network. However, existing methods either apply (i) an optimization procedure that works for any style image but is very expensive, or (ii) an efficient feedforward network that only allows a limited number of trained styles. In this work we propose a simpler optimization objective based on local matching that combines the content structure and style textures in a single layer of the pretrained network. We show that our objective has desirable properties such as a simpler optimization landscape, intuitive parameter tuning, and consistent frame-by-frame performance on video. Furthermore, we use 80,000 natural images and 80,000 paintings to train an inverse network that approximates the result of the optimization. This results in a procedure for artistic style transfer that is efficient but also allows arbitrary content and style images.", "venue": "arXiv", "keywords": ["cnns", "loss functions", "style transfer"]}
{"id": "chenFedBoneLargeScaleFederated2023", "title": "FedBone: Towards Large-Scale Federated Multi-Task Learning", "abstract": "Heterogeneous federated multi-task learning (HFMTL) is a federated learning technique that combines heterogeneous tasks of different clients to achieve more accurate, comprehensive predictions. In real-world applications, visual and natural language tasks typically require large-scale models to extract high-level abstract features. However, large-scale models cannot be directly applied to existing federated multi-task learning methods. Existing HFML methods also disregard the impact of gradient conflicts on multi-task optimization during the federated aggregation process. In this work, we propose an innovative framework called FedBone, which enables the construction of large-scale models with better generalization from the perspective of server-client split learning and gradient projection. We split the entire model into two components: a large-scale general model (referred to as the general model) on the cloud server and multiple task-specific models (referred to as the client model) on edge clients, solving the problem of insufficient computing power on edge clients. The conflicting gradient projection technique is used to enhance the generalization of the large-scale general model between different tasks. The proposed framework is evaluated on two benchmark datasets and a real ophthalmic dataset. Comprehensive results demonstrate that FedBone efficiently adapts to heterogeneous local tasks of each client and outperforms existing federated learning algorithms in most dense prediction and classification tasks with off-the-shelf computational resources on the client side.", "venue": "arXiv", "keywords": ["federated learning", "multi-task learning"]}
{"id": "chenFedMSplitCorrelationAdaptiveFederated2022", "title": "FedMSplit: Correlation-Adaptive Federated Multi-Task Learning across Multimodal Split Networks", "abstract": "With the advancement of data collection techniques, end users are interested in how different types of data can collaborate to improve our life experiences. Multimodal Federated Learning (MFL) is an emerging area allowing many distributed clients, each of which can collect data from multiple types of sensors, to participate in the training of some multimodal data-related models without sharing their data. In this paper, we address a novel challenging issue in MFL, the modality incongruity, where clients may have heterogeneous setups of sensors and their local data consists of different combinations of modalities. With the modality incongruity, clients may solve different tasks on different parameter spaces, which escalates the difficulties in dealing with the statistical heterogeneity problem of federated learning; also, it would be hard to perform accurate model aggregation across different types of clients. To tackle these challenges, in this work, we propose the FedMSplit framework, which allows federated training over multimodal distributed data without assuming similar active sensors in all clients. The key idea is to employ a dynamic and multi-view graph structure to adaptively capture the correlations amongst multimodal client models. More specifically, we split client models into smaller shareable blocks and allow each type of blocks to provide a specific view on client relationships. With the graph representation, the underlying correlations between clients can be captured as the edge features in the multi-view graph, and then be utilized to promote local model relations through the neighborhood message passing in the graph. Our experimental results demonstrate the effectiveness of our method under different sensor setups with statistical heterogeneity.", "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "keywords": ["adaptive learning", "federated learning", "multi-task learning"]}
{"id": "chengLeveragingSemanticSegmentation2019", "title": "Leveraging Semantic Segmentation with Learning-Based Confidence Measure", "abstract": "", "venue": "Neurocomputing", "keywords": []}
{"id": "chengSwiftNetUsingGraph2019", "title": "SwiftNet: Using Graph Propagation as Meta-knowledge to Search Highly Representative Neural Architectures", "abstract": "Designing neural architectures for edge devices is subject to constraints of accuracy, inference latency, and computational cost. Traditionally, researchers manually craft deep neural networks to meet the needs of mobile devices. Neural Architecture Search (NAS) was proposed to automate the neural architecture design without requiring extensive domain expertise and significant manual efforts. Recent works utilized NAS to design mobile models by taking into account hardware constraints and achieved state-ofthe-art accuracy with fewer parameters and less computational cost measured in Multiply-accumulates (MACs). To find highly compact neural architectures, existing works relies on predefined cells and directly applying width multiplier, which may potentially limit the model flexibility, reduce the useful feature map information, and cause accuracy drop. To conquer this issue, we propose GRAM (GRAph propagation as Meta-knowledge) that adopts finegrained (node-wise) search method and accumulates the knowledge learned in updates into a meta-graph. As a result, GRAM can enable more flexible search space and achieve higher search efficiency. Without the constraints of predefined cell or blocks, we propose a new structure-level pruning method to remove redundant operations in neural architectures. SwiftNet, which is a set of models discovered by GRAM, outperforms MobileNet-V2 by 2.15 higher accuracy density and 2.42 faster with similar accuracy. Compared with FBNet, SwiftNet reduces the search cost by 26 and achieves 2.35 higher accuracy density and 1.47 speedup while preserving similar accuracy. SwiftNet can obtain 63.28% top-1 accuracy on ImageNet-1K with only 53M MACs and 2.07M parameters. The corresponding inference latency is only 19.09 ms on Google Pixel 1.", "venue": "arXiv", "keywords": ["meta-learning", "neural architecture search"]}
{"id": "chenImprovedBaselinesMomentum2020", "title": "Improved Baselines with Momentum Contrastive Learning", "abstract": "Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.", "venue": "arXiv", "keywords": ["contrastive learning", "momentum encoders"]}
{"id": "chenKernelizedSimilarityLearning2022", "title": "Kernelized Similarity Learning and Embedding for Dynamic Texture Synthesis", "abstract": "Dynamic texture (DT) exhibits statistical stationarity in the spatial domain and stochastic repetitiveness in the temporal dimension, indicating that different frames of DT possess a high similarity correlation that is critical prior knowledge. However, existing methods cannot effectively learn a synthesis model for high-dimensional DT from a small number of training samples. In this paper, we propose a novel DT synthesis method, which makes full use of similarity as prior knowledge to address this issue. Our method is based on the proposed kernel similarity embedding, which can not only mitigate the high-dimensionality and small sample issues, but also has the advantage of modeling nonlinear feature relationships. Specifically, we first put forward two hypotheses that are essential for the DT model to generate new frames using similarity correlations. Then, we integrate kernel learning and the extreme learning machine into a unified synthesis model to learn kernel similarity embeddings for representing DTs. Extensive experiments on DT videos collected from the internet and two benchmark datasets, i.e., Gatech Graphcut Textures and Dyntex, demonstrate that the learned kernel similarity embeddings can provide discriminative representations for DTs. Further, our method can preserve the long-term temporal continuity of the synthesized DT sequences with excellent sustainability and generalization. Meanwhile, it effectively generates realistic DT videos with higher speed and lower computation than the current state-of-the-art methods. The code and more synthesis videos are available at our project page https://shiming-chen.github.io/Similarity-page/Similarit.html.", "venue": "arXiv", "keywords": ["kernel methods", "texture transfer"]}
{"id": "chenMultiSiamSelfsupervisedMultiinstance2021", "title": "MultiSiam: Self-supervised Multi-instance Siamese Representation Learning for Autonomous Driving", "abstract": "Autonomous driving has attracted much attention over the years but turns out to be harder than expected, probably due to the difficulty of labeled data collection for model training. Self-supervised learning (SSL), which leverages unlabeled data only for representation learning, might be a promising way to improve model performance. Existing SSL methods, however, usually rely on the single-centric-object guarantee, which may not be applicable for multi-instance datasets such as street scenes. To alleviate this limitation, we raise two issues to solve: (1) how to define positive samples for cross-view consistency and (2) how to measure similarity in multi-instance circumstances. We first adopt an IoU threshold during random cropping to transfer global-inconsistency to local-consistency. Then, we propose two feature alignment methods to enable 2D feature maps for multi-instance similarity measurement. Additionally, we adopt intra-image clustering with self-attention for further mining intra-image similarity and translation-invariance. Experiments show that, when pre-trained on Waymo dataset, our method called Multi-instance Siamese Network (MultiSiam) remarkably improves generalization ability and achieves state-of-the-art transfer performance on autonomous driving benchmarks, including Cityscapes and BDD100K, while existing SSL counterparts like MoCo, MoCo-v2, and BYOL show significant performance drop. By pre-training on SODA10M, a large-scale autonomous driving dataset, MultiSiam exceeds the ImageNet pre-trained MoCo-v2, demonstrating the potential of domain-specific pre-training. Code will be available at https://github.com/KaiChen1998/MultiSiam.", "venue": "arXiv", "keywords": ["semantic segmentation", "semi-supervised learning", "siamese networks"]}
{"id": "chenNeuralOrdinaryDifferential2019", "title": "Neural Ordinary Differential Equations", "abstract": "We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.", "venue": "arXiv", "keywords": ["foundational"]}
{"id": "chenNoReferenceColorImage2018", "title": "No-Reference Color Image Quality Assessment: From Entropy to Perceptual Quality", "abstract": "This paper presents a high-performance general-purpose no-reference (NR) image quality assessment (IQA) method based on image entropy. The image features are extracted from two domains. In the spatial domain, the mutual information between the color channels and the two-dimensional entropy are calculated. In the frequency domain, the two-dimensional entropy and the mutual information of the filtered sub-band images are computed as the feature set of the input color image. Then, with all the extracted features, the support vector classifier (SVC) for distortion classification and support vector regression (SVR) are utilized for the quality prediction, to obtain the final quality assessment score. The proposed method, which we call entropy-based no-reference image quality assessment (ENIQA), can assess the quality of different categories of distorted images, and has a low complexity. The proposed ENIQA method was assessed on the LIVE and TID2013 databases and showed a superior performance. The experimental results confirmed that the proposed ENIQA method has a high consistency of objective and subjective assessment on color images, which indicates the good overall performance and generalization ability of ENIQA. The source code is available on github https://github.com/jacob6/ENIQA.", "venue": "arXiv", "keywords": ["entropic optimization", "image quality assessment"]}
{"id": "chenPixelSNAILImprovedAutoregressive2017", "title": "PixelSNAIL: An Improved Autoregressive Generative Model", "abstract": "Autoregressive generative models consistently achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions, which offer better access to earlier parts of the sequence than conventional RNNs. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this note, we describe the resulting model and present state-of-the-art log-likelihood results on CIFAR-10 (2.85 bits per dim) and \\ ImageNet (3.80 bits per dim). Our implementation is available at https://github.com/neocxi/pixelsnail-public", "venue": "arXiv", "keywords": []}
{"id": "chenPixelSNAILImprovedAutoregressive2017a", "title": "PixelSNAIL: An Improved Autoregressive Generative Model", "abstract": "Autoregressive generative models consistently achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions, which offer better access to earlier parts of the sequence than conventional RNNs. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this note, we describe the resulting model and present state-of-the-art log-likelihood results on CIFAR-10 (2.85 bits per dim) and \\ ImageNet (3.80 bits per dim). Our implementation is available at https://github.com/neocxi/pixelsnail-public", "venue": "arXiv", "keywords": []}
{"id": "chenRegistrationUncertaintySegmentation2024", "title": "From Registration Uncertainty to Segmentation Uncertainty", "abstract": "Understanding the uncertainty inherent in deep learning-based image registration models has been an ongoing area of research. Existing methods have been developed to quantify both transformation and appearance uncertainties related to the registration process, elucidating areas where the model may exhibit ambiguity regarding the generated deformation. However, our study reveals that neither uncertainty effectively estimates the potential errors when the registration model is used for label propagation. Here, we propose a novel framework to concurrently estimate both the epistemic and aleatoric segmentation uncertainties for image registration. To this end, we implement a compact deep neural network (DNN) designed to transform the appearance discrepancy in the warping into aleatoric segmentation uncertainty by minimizing a negative log-likelihood loss function. Furthermore, we present epistemic segmentation uncertainty within the label propagation process as the entropy of the propagated labels. By introducing segmentation uncertainty along with existing methods for estimating registration uncertainty, we offer vital insights into the potential uncertainties at different stages of image registration. We validated our proposed framework using publicly available datasets, and the results prove that the segmentation uncertainties estimated with the proposed method correlate well with errors in label propagation, all while achieving superior registration performance.", "venue": "arXiv", "keywords": ["semantic segmentation", "uncertainty quantification"]}
{"id": "chenResidualFlowsInvertible2020", "title": "Residual Flows for Invertible Generative Modeling", "abstract": "Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estimate of the log density using a \"Russian roulette\" estimator, and reduce the memory required during training by using an alternative infinite series for the gradient. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid derivative saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.", "venue": "arXiv", "keywords": ["lipschitz-constraints"]}
{"id": "chenRethinkingAtrousConvolution2017", "title": "Rethinking Atrous Convolution for Semantic Image Segmentation", "abstract": "In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.", "venue": "arXiv", "keywords": ["cnns", "foundational", "semantic segmentation"]}
{"id": "chenSelfContainedStylizationSteganography2020", "title": "Self-Contained Stylization via Steganography for Reverse and Serial Style Transfer", "abstract": "Style transfer has been widely applied to give real-world images a new artistic look. However, given a stylized image, the attempts to use typical style transfer methods for de-stylization or transferring it again into another style usually lead to artifacts or undesired results. We realize that these issues are originated from the content inconsistency between the original image and its stylized output. Therefore, in this paper we advance to keep the content information of the input image during the process of style transfer by the power of steganography, with two approaches proposed: a two-stage model and an end-to-end model. We conduct extensive experiments to successfully verify the capacity of our models, in which both of them are able to not only generate stylized images of quality comparable with the ones produced by typical style transfer methods, but also effectively eliminate the artifacts introduced in reconstructing original input from a stylized image as well as performing multiple times of style transfer in series.", "venue": "arXiv", "keywords": ["generative inversion", "style ind learning", "style transfer"]}
{"id": "chenSemFormerSemanticGuided2022", "title": "SemFormer: Semantic Guided Activation Transformer for Weakly Supervised Semantic Segmentation", "abstract": "Recent mainstream weakly supervised semantic segmentation (WSSS) approaches are mainly based on Class Activation Map (CAM) generated by a CNN (Convolutional Neural Network) based image classifier. In this paper, we propose a novel transformer-based framework, named Semantic Guided Activation Transformer (SemFormer), for WSSS. We design a transformer-based Class-Aware AutoEncoder (CAAE) to extract the class embeddings for the input image and learn class semantics for all classes of the dataset. The class embeddings and learned class semantics are then used to guide the generation of activation maps with four losses, i.e., class-foreground, class-background, activation suppression, and activation complementation loss. Experimental results show that our SemFormer achieves 74.3% mIoU and surpasses many recent mainstream WSSS approaches by a large margin on PASCAL VOC 2012 dataset. Code will be available at https://github.com/JLChen-C/SemFormer.", "venue": "arXiv", "keywords": ["semantic segmentation", "semi-supervised learning", "transformers"]}
{"id": "chenSimpleFrameworkContrastive2020", "title": "A Simple Framework for Contrastive Learning of Visual Representations", "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.", "venue": "arXiv", "keywords": ["contrastive learning", "representation learning"]}
{"id": "chenStereoscopicNeuralStyle2018", "title": "Stereoscopic Neural Style Transfer", "abstract": "This paper presents the first attempt at stereoscopic neural style transfer, which responds to the emerging demand for 3D movies or AR/VR. We start with a careful examination of applying existing monocular style transfer methods to left and right views of stereoscopic images separately. This reveals that the original disparity consistency cannot be well preserved in the final stylization results, which causes 3D fatigue to the viewers. To address this issue, we incorporate a new disparity loss into the widely adopted style loss function by enforcing the bidirectional disparity constraint in non-occluded regions. For a practical realtime solution, we propose the first feed-forward network by jointly training a stylization sub-network and a disparity sub-network, and integrate them in a feature level middle domain. Our disparity sub-network is also the first end-toend network for simultaneous bidirectional disparity and occlusion mask estimation. Finally, our network is effectively extended to stereoscopic videos, by considering both temporal coherence and disparity consistency. We will show that the proposed method clearly outperforms the baseline algorithms both quantitatively and qualitatively.", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": ["siamese networks", "style transfer"]}
{"id": "chenThesisProposalPresented", "title": "Thesis Proposal Presented to The Academic Faculty", "abstract": "", "venue": "", "keywords": []}
{"id": "chenTransUNetTransformersMake2021", "title": "TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation", "abstract": "Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the ushaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization.", "venue": "arXiv", "keywords": ["semantic segmentation", "transformers"]}
{"id": "chenUPSTNeRFUniversalPhotorealistic2022", "title": "UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance Fields for 3D Scene", "abstract": "3D scenes photorealistic stylization aims to generate photorealistic images from arbitrary novel views according to a given style image while ensuring consistency when rendering from different viewpoints. Some existing stylization methods with neural radiance fields can effectively predict stylized scenes by combining the features of the style image with multi-view images to train 3D scenes. However, these methods generate novel view images that contain objectionable artifacts. Besides, they cannot achieve universal photorealistic stylization for a 3D scene. Therefore, a styling image must retrain a 3D scene representation network based on a neural radiation field. We propose a novel 3D scene photorealistic style transfer framework to address these issues. It can realize photorealistic 3D scene style transfer with a 2D style image. We first pre-trained a 2D photorealistic style transfer network, which can meet the photorealistic style transfer between any given content image and style image. Then, we use voxel features to optimize a 3D scene and get the geometric representation of the scene. Finally, we jointly optimize a hyper network to realize the scene photorealistic style transfer of arbitrary style images. In the transfer stage, we use a pre-trained 2D photorealistic network to constrain the photorealistic style of different views and different style images in the 3D scene. The experimental results show that our method not only realizes the 3D photorealistic style transfer of arbitrary style images but also outperforms the existing methods in terms of visual quality and consistency. Project page:https://semchan.github.io/UPST_NeRF.", "venue": "arXiv", "keywords": ["3d scenes", "3d style transfer", "neural radiance fields"]}
{"id": "chenValidationFeedbackBased2021", "title": "Validation Feedback Based Image Transfer Network for Data Augmentation", "abstract": "Modern image classifiers are often suffering over-fitting problems because of the insufficient number of images in the dataset. Data augmentation is a strategy to increase the number of training samples. However, recent data augmentation methods are designed manually and cannot generate real-like images. Some neural network-based image generation methods such as GAN and VAE can also be used for data augmentation, but they are usually applied to unbalanced datasets. Since the generated images cannot be guaranteed to be from the same label, using them to extend a balanced dataset may lead to decreasing the accuracy of the classifier. In this paper, we propose an image transfer network to produce images that automatically adapt to a specific dataset and classifier. The image transfer network will search for the output images which can maximize the validation accuracy and help the classifier to overcome the over-fitting problems. Through the experiments, our method achieves high accuracy on CIFAR-10 and CIFAR-100 datasets. Moreover, since it could combine with other data augmentation methods, we show that using our method can push the state-of-the-art results furthermore.", "venue": "Proceedings of the 2020 2nd International Conference on Video, Signal and Image Processing", "keywords": []}
{"id": "cherianSemGANSemanticallyConsistentImagetoImage2018", "title": "Sem-GAN: Semantically-Consistent Image-to-Image Translation", "abstract": "Unpaired image-to-image translation is the problem of mapping an image in the source domain to one in the target domain, without requiring corresponding image pairs. To ensure the translated images are realistically plausible, recent works, such as Cycle-GAN, demands this mapping to be invertible. While, this requirement demonstrates promising results when the domains are unimodal, its performance is unpredictable in a multi-modal scenario such as in an image segmentation task. This is because, invertibility does not necessarily enforce semantic correctness. To this end, we present a semantically-consistent GAN framework, dubbed Sem-GAN, in which the semantics are defined by the class identities of image segments in the source domain as produced by a semantic segmentation algorithm. Our proposed framework includes consistency constraints on the translation task that, together with the GAN loss and the cycle-constraints, enforces that the images when translated will inherit the appearances of the target domain, while (approximately) maintaining their identities from the source domain. We present experiments on several image-to-image translation tasks and demonstrate that Sem-GAN improves the quality of the translated images significantly, sometimes by more than 20% on the FCN score. Further, we show that semantic segmentation models, trained with synthetic images translated via Sem-GAN, leads to significantly better segmentation results than other variants.", "venue": "arXiv", "keywords": ["consistency training", "gans", "image-to-image"]}
{"id": "cheungAdaAugLearningClass2021", "title": "AdaAug: Learning Class- and Instance-adaptive Data Augmentation Policies", "abstract": "Data augmentation is an effective way to improve the generalization capability of modern deep learning models. However, the underlying augmentation methods mostly rely on handcrafted operations. Moreover, an augmentation policy useful to one dataset may not transfer well to other datasets. Therefore, Automated Data Augmentation (AutoDA) methods, like \\ and -based Augmentation\\, have been proposed recently to automate the process of searching for optimal augmentation policies. However, the augmentation policies found are not adaptive to the dataset used, hindering the effectiveness of these AutoDA methods. In this paper, we propose a novel AutoDA method called \\ to efficiently learn adaptive augmentation policies in a class-dependent and potentially instance-dependent manner. Our experiments show that the adaptive augmentation policies learned by our method transfer well to unseen datasets such as the Oxford Flowers, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars datasets when compared with other AutoDA baselines. In addition, our method also achieves state-of-the-art performance on the CIFAR-10, CIFAR-100, and SVHN datasets.", "venue": "International Conference on Learning Representations", "keywords": ["auto-augmentation policies", "reinforcement learning"]}
{"id": "chiangStylizing3DScene2022", "title": "Stylizing 3D Scene via Implicit Representation and HyperNetwork", "abstract": "In this work, we aim to address the 3D scene stylization problem - generating stylized images of the scene at arbitrary novel view angles. A straightforward solution is to combine existing novel view synthesis and image/video style transfer approaches, which often leads to blurry results or inconsistent appearance. Inspired by the high-quality results of the neural radiance fields (NeRF) method, we propose a joint framework to directly render novel views with the desired style. Our framework consists of two components: an implicit representation of the 3D scene with the neural radiance fields model, and a hypernetwork to transfer the style information into the scene representation. In particular, our implicit representation model disentangles the scene into the geometry and appearance branches, and the hypernetwork learns to predict the parameters of the appearance branch from the reference style image. To alleviate the training difficulties and memory burden, we propose a two-stage training procedure and a patch sub-sampling approach to optimize the style and content losses with the neural radiance fields model. After optimization, our model is able to render consistent novel views at arbitrary view angles with arbitrary style. Both quantitative evaluation and human subject study have demonstrated that the proposed method generates faithful stylization results with consistent appearance across different views.", "venue": "arXiv", "keywords": ["3d scenes", "3d style transfer", "neural radiance fields"]}
{"id": "chiuBetterMayNot2023", "title": "Better May Not Be Fairer: A Study on Subgroup Discrepancy in Image Classification", "abstract": "In this paper, we provide 20,000 non-trivial human annotations on popular datasets as a first step to bridge gap to studying how natural semantic spurious features affect image classification, as prior works often study datasets mixing low-level features due to limitations in accessing realistic datasets. We investigate how natural background colors play a role as spurious features by annotating the test sets of CIFAR10 and CIFAR100 into subgroups based on the background color of each image. We name our datasets 10-B\\ and 100-B\\ and integrate them with CIFAR-Cs. We find that overall human-level accuracy does not guarantee consistent subgroup performances, and the phenomenon remains even on models pre-trained on ImageNet or after data augmentation (DA). To alleviate this issue, we propose \\, a \\ DA that leverages decoupled semantic representations captured by a pre-trained generative flow. Experimental results show that FlowAug achieves more consistent subgroup results than other types of DA methods on CIFAR10/100 and on CIFAR10/100-C. Additionally, it shows better generalization performance. Furthermore, we propose a generic metric, \\, for studying model robustness to spurious correlations, where we take a macro average on the weighted standard deviations across different classes. We show \\ being more predictive of better performances; per our metric, FlowAug demonstrates improvements on subgroup discrepancy. Although this metric is proposed to study our curated datasets, it applies to all datasets that have subgroups or subclasses. Lastly, we also show superior out-of-distribution results on CIFAR10.1.", "venue": "arXiv", "keywords": ["bias sources", "domain generalization", "subpopulation shift"]}
{"id": "chiuLineSearchBasedFeature2022", "title": "Line Search-Based Feature Transformation for Fast, Stable, and Tunable Content-Style Control in Photorealistic Style Transfer", "abstract": "Photorealistic style transfer is the task of synthesizing a realistic-looking image when adapting the content from one image to appear in the style of another image. Modern models commonly embed a transformation that fuses features describing the content image and style image and then decodes the resulting feature into a stylized image. We introduce a general-purpose transformation that enables controlling the balance between how much content is preserved and the strength of the infused style. We offer the first experiments that demonstrate the performance of existing transformations across different style transfer models and demonstrate how our transformation performs better in its ability to simultaneously run fast, produce consistently reasonable results, and control the balance between content and style in different models. To support reproducing our method and models, we share the code at https://github.com/chiutaiyin/LS-FT.", "venue": "arXiv", "keywords": ["augmentation stability", "feature-level augmentation", "style transfer"]}
{"id": "chiuPCABasedKnowledgeDistillation2022", "title": "PCA-Based Knowledge Distillation Towards Lightweight and Content-Style Balanced Photorealistic Style Transfer Models", "abstract": "Photorealistic style transfer entails transferring the style of a reference image to another image so the result seems like a plausible photo. Our work is inspired by the observation that existing models are slow due to their large sizes. We introduce PCA-based knowledge distillation to distill lightweight models and show it is motivated by theory. To our knowledge, this is the first knowledge distillation method for photorealistic style transfer. Our experiments demonstrate its versatility for use with different backbone architectures, VGG and MobileNet, across six image resolutions. Compared to existing models, our top-performing model runs at speeds 5-20x faster using at most 1 % of the parameters. Additionally, our distilled models achieve a better balance between stylization strength and content preservation than existing models. To support reproducing our method and models, we share the code at ://github.com/chiutaiyin/PCA-Knowledge-Distillation\\.", "venue": "arXiv", "keywords": ["feature engineering", "knowledge distillation", "style transfer"]}
{"id": "chiuUnderstandingGeneralizedWhitening2019", "title": "Understanding Generalized Whitening and Coloring Transform for Universal Style Transfer", "abstract": "", "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "keywords": ["feature whitening", "feature-level augmentation", "style transfer"]}
{"id": "choDualPrototypeAttention2023", "title": "Dual Prototype Attention for Unsupervised Video Object Segmentation", "abstract": "Unsupervised video object segmentation (VOS) aims to detect and segment the most salient object in videos. The primary techniques used in unsupervised VOS are 1) the collaboration of appearance and motion information and 2) temporal fusion between different frames. This paper proposes two novel prototype-based attention mechanisms, inter-modality attention (IMA) and inter-frame attention (IFA), to incorporate these techniques via dense propagation across different modalities and frames. IMA densely integrates context information from different modalities based on a mutual refinement. IFA injects global context of a video to the query frame, enabling a full utilization of useful properties from multiple frames. Experimental results on public benchmark datasets demonstrate that our proposed approach outperforms all existing methods by a substantial margin. The proposed two components are also thoroughly validated via ablative study. Code and models are available at https://github.com/Hydragon516/DPA.", "venue": "arXiv", "keywords": ["unsupervised learning"]}
{"id": "choiRobustNetImprovingDomain2021", "title": "RobustNet: Improving Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening", "abstract": "Enhancing the generalization capability of deep neural networks to unseen domains is crucial for safety-critical applications in the real world such as autonomous driving. To address this issue, this paper proposes a novel instance selective whitening loss to improve the robustness of the segmentation networks for unseen domains. Our approach disentangles the domain-specific style and domain-invariant content encoded in higher-order statistics (i.e., feature covariance) of the feature representations and selectively removes only the style information causing domain shift. As shown in Fig. 1, our method provides reasonable predictions for (a) low-illuminated, (b) rainy, and (c) unseen structures. These types of images are not included in the training dataset, where the baseline shows a significant performance drop, contrary to ours. Being simple yet effective, our approach improves the robustness of various backbone networks without additional computational cost. We conduct extensive experiments in urban-scene segmentation and show the superiority of our approach to existing work. Our code is available at https://github.com/shachoi/RobustNet.", "venue": "arXiv", "keywords": ["domain generalization", "feature whitening", "feature-level augmentation", "instance normalization", "semantic segmentation"]}
{"id": "choiStarGANV2Diverse2020", "title": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "abstract": "A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at https://github.com/clovaai/stargan-v2.", "venue": "arXiv", "keywords": ["gans", "image synthesis", "image-to-image"]}
{"id": "choMaskbasedStyleControlledImage2021", "title": "Mask-Based Style-Controlled Image Synthesis Using a Mask Style Encoder", "abstract": "In recent years, the advances in Generative Adversarial Networks (GANs) have shown impressive results for image generation and translation tasks. In particular, the image-to-image translation is a method of learning mapping from a source domain to a target domain and synthesizing an image. Image-to-image translation can be applied to a variety of tasks, making it possible to quickly and easily synthesize realistic images from semantic segmentation masks. However, in the existing image-to-image translation method, there is a limitation on controlling the style of the translated image, and it is not easy to synthesize an image by controlling the style of each mask element in detail. Therefore, we propose an image synthesis method that controls the style of each element by improving the existing image-to-image translation method. In the proposed method, we implement a mask style encoder that extracts style features for each mask element. The extracted style features are concatenated to the semantic mask in the normalization layer, and used the style-controlled image synthesis of each mask element. In the experiments, we performed style-controlled images synthesis using the datasets consisting of semantic segmentation masks and real images. The results show that the proposed method has excellent performance for style-controlled images synthesis for each element.", "venue": "2020 25th International Conference on Pattern Recognition (ICPR)", "keywords": ["image synthesis", "style transfer"]}
{"id": "chuangMeasuringGeneralizationOptimal2021", "title": "Measuring Generalization with Optimal Transport", "abstract": "Understanding the generalization of deep neural networks is one of the most important tasks in deep learning. Although much progress has been made, theoretical error bounds still often behave disparately from empirical observations. In this work, we develop margin-based generalization bounds, where the margins are normalized with optimal transport costs between independent random subsets sampled from the training distribution. In particular, the optimal transport cost can be interpreted as a generalization of variance which captures the structural properties of the learned feature space. Our bounds robustly predict the generalization error, given training data and network parameters, on large scale datasets. Theoretically, we demonstrate that the concentration and separation of features play crucial roles in generalization, supporting empirical results in the literature.", "venue": "Advances in Neural Information Processing Systems", "keywords": []}
{"id": "chugunovNeuralSplineFields2023", "title": "Neural Spline Fields for Burst Image Fusion and Layer Separation", "abstract": "Each photo in an image burst can be considered a sample of a complex 3D scene: the product of parallax, diffuse and specular materials, scene motion, and illuminant variation. While decomposing all of these effects from a stack of misaligned images is a highly ill-conditioned task, the conventional align-and-merge burst pipeline takes the other extreme: blending them into a single image. In this work, we propose a versatile intermediate representation: a twolayer alpha-composited image plus flow model constructed with neural spline fields -- networks trained to map input coordinates to spline control points. Our method is able to, during test-time optimization, jointly fuse a burst image capture into one high-resolution reconstruction and decompose it into transmission and obstruction layers. Then, by discarding the obstruction layer, we can perform a range of tasks including seeing through occlusions, reflection suppression, and shadow removal. Validated on complex synthetic and in-the-wild captures we find that, with no postprocessing steps or learned priors, our generalizable model is able to outperform existing dedicated single-image and multi-view obstruction removal approaches.", "venue": "arXiv", "keywords": ["generative augmentation", "image-to-image", "neural fields", "neural splines"]}
{"id": "ClearPathsTraining", "title": "Beyond Clear Paths: Training with Neural Style Transfer and Auto-Augmentation for Domain-Generalized Segmentation of Soiled Images - ProQuest", "abstract": "Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.", "venue": "", "keywords": []}
{"id": "corbiereConfidenceEstimationAuxiliary2021", "title": "Confidence Estimation via Auxiliary Models", "abstract": "Reliably quantifying the confidence of deep neural classifiers is a challenging yet fundamental requirement for deploying such models in safety-critical applications. In this paper, we introduce a novel target criterion for model confidence, namely the true class probability (TCP). We show that TCP offers better properties for confidence estimation than standard maximum class probability (MCP). Since the true class is by essence unknown at test time, we propose to learn TCP criterion from data with an auxiliary model, introducing a specific learning scheme adapted to this context. We evaluate our approach on the task of failure prediction and of self-training with pseudo-labels for domain adaptation, which both necessitate effective confidence estimates. Extensive experiments are conducted for validating the relevance of the proposed approach in each task. We study various network architectures and experiment with small and large datasets for image classification and semantic segmentation. In every tested benchmark, our approach outperforms strong baselines.", "venue": "arXiv", "keywords": ["ood detection"]}
{"id": "cordtsCityscapesDatasetSemantic2016", "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding", "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.", "venue": "arXiv", "keywords": []}
{"id": "corinziaVariationalFederatedMultiTask2021", "title": "Variational Federated Multi-Task Learning", "abstract": "In federated learning, a central server coordinates the training of a single model on a massively distributed network of devices. This setting can be naturally extended to a multi-task learning framework, to handle real-world federated datasets that typically show strong statistical heterogeneity among devices. Despite federated multi-task learning being shown to be an effective paradigm for real-world datasets, it has been applied only on convex models. In this work, we introduce VIRTUAL, an algorithm for federated multi-task learning for general non-convex models. In VIRTUAL the federated network of the server and the clients is treated as a star-shaped Bayesian network, and learning is performed on the network using approximated variational inference. We show that this method is effective on real-world federated datasets, outperforming the current state-of-the-art for federated learning, and concurrently allowing sparser gradient updates.", "venue": "arXiv", "keywords": ["federated learning", "multi-task learning"]}
{"id": "corneanuComputingTestingError2020", "title": "Computing the Testing Error without a Testing Set", "abstract": "Deep Neural Networks (DNNs) have revolutionized computer vision. We now have DNNs that achieve top (performance) results in many problems, including object recognition, facial expression analysis, and semantic segmentation, to name but a few. The design of the DNNs that achieve top results is, however, non-trivial and mostly done by trail-and-error. That is, typically, researchers will derive many DNN architectures (i.e., topologies) and then test them on multiple datasets. However, there are no guarantees that the selected DNN will perform well in the real world. One can use a testing set to estimate the performance gap between the training and testing sets, but avoiding overfitting-to-the-testing-data is almost impossible. Using a sequestered testing dataset may address this problem, but this requires a constant update of the dataset, a very expensive venture. Here, we derive an algorithm to estimate the performance gap between training and testing that does not require any testing dataset. Specifically, we derive a number of persistent topology measures that identify when a DNN is learning to generalize to unseen samples. This allows us to compute the DNN's testing error on unseen samples, even when we do not have access to them. We provide extensive experimental validation on multiple networks and datasets to demonstrate the feasibility of the proposed approach.", "venue": "arXiv", "keywords": []}
{"id": "corneanuWhatDoesIt2019", "title": "What Does It Mean to Learn in Deep Networks? And, How Does One Detect Adversarial Attacks?", "abstract": "The flexibility and high-accuracy of Deep Neural Networks (DNNs) has transformed computer vision. But, the fact that we do not know when a specific DNN will work and when it will fail has resulted in a lack of trust. A clear example is self-driving cars; people are uncomfortable sitting in a car driven by algorithms that may fail under some unknown, unpredictable conditions. Interpretability and explainability approaches attempt to address this by uncovering what a DNN models, i.e., what each node (cell) in the network represents and what images are most likely to activate it. This can be used to generate, for example, adversarial attacks. But these approaches do not generally allow us to determine where a DNN will succeed or fail and why. i.e., does this learned representation generalize to unseen samples? Here, we derive a novel approach to define what it means to learn in deep networks, and how to use this knowledge to detect adversarial attacks. We show how this defines the ability of a network to generalize to unseen testing samples and, most importantly, why this is the case.", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["topological data analysis", "topological ml"]}
{"id": "crawshawMultiTaskLearningDeep2020", "title": "Multi-Task Learning with Deep Neural Networks: A Survey", "abstract": "Multi-task learning (MTL) is a subfield of machine learning in which multiple tasks are simultaneously learned by a shared model. Such approaches offer advantages like improved data efficiency, reduced overfitting through shared representations, and fast learning by leveraging auxiliary information. However, the simultaneous learning of multiple tasks presents new design and optimization challenges, and choosing which tasks should be learned jointly is in itself a non-trivial problem. In this survey, we give an overview of multi-task learning methods for deep neural networks, with the aim of summarizing both the well-established and most recent directions within the field. Our discussion is structured according to a partition of the existing deep MTL techniques into three groups: architectures, optimization methods, and task relationship learning. We also provide a summary of common multi-task benchmarks.", "venue": "arXiv", "keywords": []}
{"id": "csurkaUnsupervisedDomainAdaptation2021", "title": "Unsupervised Domain Adaptation for Semantic Image Segmentation: A Comprehensive Survey", "abstract": "Semantic segmentation plays a fundamental role in a broad variety of computer vision applications, providing key information for the global understanding of an image. Yet, the state-of-the-art models rely on large amount of annotated samples, which are more expensive to obtain than in tasks such as image classification. Since unlabelled data is instead significantly cheaper to obtain, it is not surprising that Unsupervised Domain Adaptation reached a broad success within the semantic segmentation community.", "venue": "arXiv", "keywords": ["semantic segmentation", "surveys", "transformers", "unsupervised da"]}
{"id": "cubukAutoAugmentLearningAugmentation2019", "title": "AutoAugment: Learning Augmentation Policies from Data", "abstract": "Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error rate of 1.5%, which is 0.6% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "kornia native", "pytorch native"]}
{"id": "cubukRandAugmentPracticalAutomated2019", "title": "RandAugment: Practical Automated Data Augmentation with a Reduced Search Space", "abstract": "Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0% accuracy, a 0.6% increase over the previous state-of-the-art and 1.0% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3% improvement over baseline augmentation, and is within 0.3% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "pytorch native"]}
{"id": "cuturiSinkhornDistancesLightspeed2013", "title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances", "abstract": "Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.", "venue": "arXiv", "keywords": ["entropic optimization", "highly-analytical", "optimal transport", "topological analysis"]}
{"id": "daiFlare7KMixingSynthetic2023", "title": "Flare7K++: Mixing Synthetic and Real Datasets for Nighttime Flare Removal and Beyond", "abstract": "Artificial lights commonly leave strong lens flare artifacts on the images captured at night, degrading both the visual quality and performance of vision algorithms. Existing flare removal approaches mainly focus on removing daytime flares and fail in nighttime cases. Nighttime flare removal is challenging due to the unique luminance and spectrum of artificial lights, as well as the diverse patterns and image degradation of the flares. The scarcity of the nighttime flare removal dataset constraints the research on this crucial task. In this paper, we introduce Flare7K++, the first comprehensive nighttime flare removal dataset, consisting of 962 real-captured flare images (Flare-R) and 7,000 synthetic flares (Flare7K). Compared to Flare7K, Flare7K++ is particularly effective in eliminating complicated degradation around the light source, which is intractable by using synthetic flares alone. Besides, the previous flare removal pipeline relies on the manual threshold and blur kernel settings to extract light sources, which may fail when the light sources are tiny or not overexposed. To address this issue, we additionally provide the annotations of light sources in Flare7K++ and propose a new end-to-end pipeline to preserve the light source while removing lens flares. Our dataset and pipeline offer a valuable foundation and benchmark for future investigations into nighttime flare removal studies. Extensive experiments demonstrate that Flare7K++ supplements the diversity of existing flare datasets and pushes the frontier of nighttime flare removal towards real-world scenarios.", "venue": "arXiv", "keywords": ["automotive occlusion", "dataset debut"]}
{"id": "damicantonioAutomatedCameraCalibration", "title": "Automated Camera Calibration via Homography Estimation With GNNs", "abstract": "Over the past few decades, a significant rise of camerabased applications for traffic monitoring has occurred. Governments and local administrations are increasingly relying on the data collected from these cameras to enhance road safety and optimize traffic conditions. However, for effective data utilization, it is imperative to ensure accurate and automated calibration of the involved cameras. This paper proposes a novel approach to address this challenge by leveraging the topological structure of intersections.", "venue": "", "keywords": []}
{"id": "danielrosehillHowYouTubeCompression2021", "title": "How Is YouTube's Compression Better than What Most Software Editors Can Produce?", "abstract": "", "venue": "r/VideoEditing", "keywords": []}
{"id": "dasCDCGenCrossDomainConditional2021", "title": "CDCGen: Cross-Domain Conditional Generation via Normalizing Flows and Adversarial Training", "abstract": "How to generate conditional synthetic data for a domain without utilizing information about its labels/attributes? Our work presents a solution to the above question. We propose a transfer learning-based framework utilizing normalizing flows, coupled with both maximum-likelihood and adversarial training. We model a source domain (labels available) and a target domain (labels unavailable) with individual normalizing flows, and perform domain alignment to a common latent space using adversarial discriminators. Due to the invertible property of flow models, the mapping has exact cycle consistency. We also learn the joint distribution of the data samples and attributes in the source domain by employing an encoder to map attributes to the latent space via adversarial training. During the synthesis phase, given any combination of attributes, our method can generate synthetic samples conditioned on them in the target domain. Empirical studies confirm the effectiveness of our method on benchmarked datasets. We envision our method to be particularly useful for synthetic data generation in label-scarce systems by generating non-trivial augmentations via attribute transformations. These synthetic samples will introduce more entropy into the label-scarce domain than their geometric and photometric transformation counterparts, helpful for robust downstream tasks.", "venue": "arXiv", "keywords": ["gans", "generative models", "image synthesis", "normalizing flows"]}
{"id": "dasSoildNetSoilingDegradation", "title": "SoildNet: Soiling Degradation Detection in Autonomous Driving", "abstract": "In the field of autonomous driving, camera sensors are extremely prone to soiling because they are located outside of the car and interact with environmental sources of soiling such as rain drops, snow, dust, sand, mud and so on. This can lead to either partial or complete vision degradation. Hence detecting such decay in vision is very important for safety and overall to preserve the functionality of the ``autonomous'' components in autonomous driving. The contribution of this work involves: 1) Designing a Deep Convolutional Neural Network (DCNN) based baseline network, 2) Exploiting several network remodelling techniques such as employing static and dynamic group convolution, channel reordering to compress the baseline architecture and make it suitable for low power embedded systems with 1 TOPS, 3) Comparing various result metrics of all interim networks dedicated for soiling degradation detection at tile level of size 64 64 on input resolution 1280 768. The compressed network, is called SoildNet (Sand, snOw, raIn/dIrt, oiL, Dust/muD) that uses only 9.72% trainable parameters of the base network and reduces the model size by more than 7 times with no loss in accuracy.", "venue": "", "keywords": ["automotive occlusion"]}
{"id": "dasSoildNetSoilingDegradation2019", "title": "SoildNet: Soiling Degradation Detection in Autonomous Driving", "abstract": "In the field of autonomous driving, camera sensors are extremely prone to soiling because they are located outside of the car and interact with environmental sources of soiling such as rain drops, snow, dust, sand, mud and so on. This can lead to either partial or complete vision degradation. Hence detecting such decay in vision is very important for safety and overall to preserve the functionality of the \"autonomous\" components in autonomous driving. The contribution of this work involves: 1) Designing a Deep Convolutional Neural Network (DCNN) based baseline network, 2) Exploiting several network remodelling techniques such as employing static and dynamic group convolution, channel reordering to compress the baseline architecture and make it suitable for low power embedded systems with nearly 1 TOPS, 3) Comparing various result metrics of all interim networks dedicated for soiling degradation detection at tile level of size 64 x 64 on input resolution 1280 x 768. The compressed network, is called SoildNet (Sand, snOw, raIn/dIrt, oiL, Dust/muD) that uses only 9.72% trainable parameters of the base network and reduces the model size by more than 7 times with no loss in accuracy", "venue": "arXiv", "keywords": []}
{"id": "dasTiledSoilingNetTilelevelSoiling2020", "title": "TiledSoilingNet: Tile-level Soiling Detection on Automotive Surround-view Cameras Using Coverage Metric", "abstract": "Automotive cameras, particularly surround-view cameras, tend to get soiled by mud, water, snow, etc. For higher levels of autonomous driving, it is necessary to have a soiling detection algorithm which will trigger an automatic cleaning system. Localized detection of soiling in an image is necessary to control the cleaning system. It is also necessary to enable partial functionality in unsoiled areas while reducing confidence in soiled areas. Although this can be solved using a semantic segmentation task, we explore a more efficient solution targeting deployment in low power embedded system. We propose a novel method to regress the area of each soiling type within a tile directly. We refer to this as coverage. The proposed approach is better than learning the dominant class in a tile as multiple soiling types occur within a tile commonly. It also has the advantage of dealing with coarse polygon annotation, which will cause the segmentation task. The proposed soiling coverage decoder is an order of magnitude faster than an equivalent segmentation decoder. We also integrated it into an object detection and semantic segmentation multi-task model using an asynchronous back-propagation algorithm. A portion of the dataset used will be released publicly as part of our WoodScape dataset to encourage further research.", "venue": "arXiv", "keywords": ["automotive occlusion"]}
{"id": "dasTransAdaptTransformativeFramework2023", "title": "TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation", "abstract": "Test-time adaptive (TTA) semantic segmentation adapts a source pre-trained image semantic segmentation model to unlabeled batches of target domain test images, different from real-world, where samples arrive one-by-one in an online fashion. To tackle online settings, we propose TransAdapt, a framework that uses transformer and input transformations to improve segmentation performance. Specifically, we pre-train a transformer-based module on a segmentation network that transforms unsupervised segmentation output to a more reliable supervised output, without requiring test-time online training. To also facilitate test-time adaptation, we propose an unsupervised loss based on the transformed input that enforces the model to be invariant and equivariant to photometric and geometric perturbations, respectively. Overall, our framework produces higher quality segmentation masks with up to 17.6% and 2.8% mIOU improvement over no-adaptation and competitive baselines, respectively.", "venue": "arXiv", "keywords": ["domain adaptation", "test-time da", "traditional augmentation", "transformers"]}
{"id": "dayalMADGMarginbasedAdversarial2023", "title": "MADG: Margin-based Adversarial Learning for Domain Generalization", "abstract": "", "venue": "Advances in Neural Information Processing Systems", "keywords": ["adversarial learning", "domain generalization", "generalization quantification"]}
{"id": "deanLargeScaleDistributed2012", "title": "Large Scale Distributed Deep Networks", "abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.", "venue": "Advances in Neural Information Processing Systems", "keywords": []}
{"id": "deboerTutorialCrossEntropyMethod2005", "title": "A Tutorial on the Cross-Entropy Method", "abstract": "The cross-entropy (CE) method is a new generic approach to combinatorial and multi-extremal optimization and rare event simulation. The purpose of this tutorial is to give a gentle introduction to the CE method. We present the CE methodology, the basic algorithm and its modifications, and discuss applications in combinatorial optimization and machine learning.", "venue": "Annals of Operations Research", "keywords": []}
{"id": "DeepImageMatting", "title": "Deep Image Matting", "abstract": "", "venue": "", "keywords": []}
{"id": "dehghaniDistributedMachineDistributed2023", "title": "From Distributed Machine to Distributed Deep Learning: A Comprehensive Survey", "abstract": "Artificial intelligence has made remarkable progress in handling complex tasks, thanks to advances in hardware acceleration and machine learning algorithms. However, to acquire more accurate outcomes and solve more complex issues, algorithms should be trained with more data. Processing this huge amount of data could be time-consuming and require a great deal of computation. To address these issues, distributed machine learning has been proposed, which involves distributing the data and algorithm across several machines. There has been considerable effort put into developing distributed machine learning algorithms, and different methods have been proposed so far. We divide these algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has gained more attention in recent years and most of the studies have focused on this approach. Therefore, we mostly concentrate on this category. Based on the investigation of the mentioned algorithms, we highlighted the limitations that should be addressed in future research.", "venue": "Journal of Big Data", "keywords": []}
{"id": "dehghaniScalingVisionTransformers2023", "title": "Scaling Vision Transformers to 22 Billion Parameters", "abstract": "The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for ``LLM-like'' scaling in vision, and provides key steps towards getting there.", "venue": "arXiv", "keywords": ["bias sources", "transformers"]}
{"id": "dengArbitraryStyleTransfer2020", "title": "Arbitrary Style Transfer via Multi-Adaptation Network", "abstract": "Arbitrary style transfer is a significant topic with research value and application prospect. A desired style transfer, given a content image and referenced style painting, would render the content image with the color tone and vivid stroke patterns of the style painting while synchronously maintaining the detailed content structure information. Style transfer approaches would initially learn content and style representations of the content and style references and then generate the stylized images guided by these representations. In this paper, we propose the multi-adaptation network which involves two self-adaptation (SA) modules and one co-adaptation (CA) module: the SA modules adaptively disentangle the content and style representations, i.e., content SA module uses position-wise self-attention to enhance content representation and style SA module uses channel-wise self-attention to enhance style representation; the CA module rearranges the distribution of style representation based on content representation distribution by calculating the local similarity between the disentangled content and style features in a non-local fashion. Moreover, a new disentanglement loss function enables our network to extract main style patterns and exact content structures to adapt to various input images, respectively. Various qualitative and quantitative experiments demonstrate that the proposed multi-adaptation network leads to better results than the state-of-the-art style transfer methods.", "venue": "arXiv", "keywords": ["style transfer"]}
{"id": "dengArbitraryVideoStyle2021", "title": "Arbitrary Video Style Transfer via Multi-Channel Correlation", "abstract": "Video style transfer is getting more attention in AI community for its numerous applications such as augmented reality and animation productions. Compared with traditional image style transfer, performing this task on video presents new challenges: how to effectively generate satisfactory stylized results for any specified style, and maintain temporal coherence across frames at the same time. Towards this end, we propose Multi-Channel Correction network (MCCNet), which can be trained to fuse the exemplar style features and input content features for efficient style transfer while naturally maintaining the coherence of input videos. Specifically, MCCNet works directly on the feature space of style and content domain where it learns to rearrange and fuse style features based on their similarity with content features. The outputs generated by MCC are features containing the desired style patterns which can further be decoded into images with vivid style textures. Moreover, MCCNet is also designed to explicitly align the features to input which ensures the output maintains the content structures as well as the temporal continuity. To further improve the performance of MCCNet under complex light conditions, we also introduce the illumination loss during training. Qualitative and quantitative evaluations demonstrate that MCCNet performs well in both arbitrary video and image style transfer tasks.", "venue": "arXiv", "keywords": ["style transfer", "temporal consistency"]}
{"id": "dengHardnessRobustnessTransfer2023", "title": "On the Hardness of Robustness Transfer: A Perspective from Rademacher Complexity over Symmetric Difference Hypothesis Space", "abstract": "Recent studies demonstrated that the adversarially robust learning under \\ attack is harder to generalize to different domains than standard domain adaptation. How to transfer robustness across different domains has been a key question in domain adaptation field. To investigate the fundamental difficulty behind adversarially robust domain adaptation (or robustness transfer), we propose to analyze a key complexity measure that controls the cross-domain generalization: the adversarial Rademacher complexity over \\ symmetric difference hypothesis space\\ \\ . For linear models, we show that adversarial version of this complexity is always greater than the non-adversarial one, which reveals the intrinsic hardness of adversarially robust domain adaptation. We also establish upper bounds on this complexity measure. Then we extend them to the ReLU neural network class by upper bounding the adversarial Rademacher complexity in the binary classification setting. Finally, even though the robust domain adaptation is provably harder, we do find positive relation between robust learning and standard domain adaptation. We explain adversarial training helps domain adaptation in terms of standard risk\\. We believe our results initiate the study of the generalization theory of adversarially robust domain adaptation, and could shed lights on distributed adversarially robust learning from heterogeneous sources, e.g., federated learning scenario.", "venue": "arXiv", "keywords": ["adversarial robustness", "generalization quantification", "pac learning", "robustness analysis"]}
{"id": "dengStructuralTeacherStudentNormality2024", "title": "Structural Teacher-Student Normality Learning for Multi-Class Anomaly Detection and Localization", "abstract": "Visual anomaly detection is a challenging open-set task aimed at identifying unknown anomalous patterns while modeling normal data. The knowledge distillation paradigm has shown remarkable performance in one-class anomaly detection by leveraging teacher-student network feature comparisons. However, extending this paradigm to multi-class anomaly detection introduces novel scalability challenges. In this study, we address the significant performance degradation observed in previous teacher-student models when applied to multi-class anomaly detection, which we identify as resulting from cross-class interference. To tackle this issue, we introduce a novel approach known as Structural Teacher-Student Normality Learning (SNL): (1) We propose spatial-channel distillation and intra-&inter-affinity distillation techniques to measure structural distance between the teacher and student networks. (2) We introduce a central residual aggregation module (CRAM) to encapsulate the normal representation space of the student network. We evaluate our proposed approach on two anomaly detection datasets, MVTecAD and VisA. Our method surpasses the state-of-the-art distillation-based algorithms by a significant margin of 3.9% and 1.5% on MVTecAD and 1.2% and 2.5% on VisA in the multi-class anomaly detection and localization tasks, respectively. Furthermore, our algorithm outperforms the current state-of-the-art unified models on both MVTecAD and VisA.", "venue": "arXiv", "keywords": ["anomaly detection", "knowledge distillation"]}
{"id": "dengStyTr2ImageStyle2022", "title": "StyTr2: Image Style Transfer with Transformers", "abstract": "The goal of image style transfer is to render an image with artistic features guided by a style reference while maintaining the original content. Owing to the locality in convolutional neural networks (CNNs), extracting and maintaining the global information of input images is difficult. Therefore, traditional neural style transfer methods face biased content representation. To address this critical issue, we take long-range dependencies of input images into account for image style transfer by proposing a transformer-based approach called StyTr\\ . In contrast with visual transformers for other vision tasks, StyTr\\ contains two different transformer encoders to generate domain-specific sequences for content and style, respectively. Following the encoders, a multi-layer transformer decoder is adopted to stylize the content sequence according to the style sequence. We also analyze the deficiency of existing positional encoding methods and propose the content-aware positional encoding (CAPE), which is scale-invariant and more suitable for image style transfer tasks. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed StyTr\\ compared with state-of-the-art CNN-based and flow-based approaches. Code and models are available at https://github.com/diyiiyiii/StyTR-2.", "venue": "arXiv", "keywords": ["promising", "style ind learning", "style transfer", "transformers"]}
{"id": "deoliEvaluatingRobustnessOffRoad2024", "title": "Evaluating the Robustness of Off-Road Autonomous Driving Segmentation against Adversarial Attacks: A Dataset-Centric Analysis", "abstract": "This study investigates the vulnerability of semantic segmentation models to adversarial input perturbations, in the domain of off-road autonomous driving. Despite good performance in generic conditions, the state-of-the-art classifiers are often susceptible to (even) small perturbations, ultimately resulting in inaccurate predictions with high confidence. Prior research has directed their focus on making models more robust by modifying the architecture and training with noisy input images, but has not explored the influence of datasets in adversarial attacks. Our study aims to address this gap by examining the impact of non-robust features in off-road datasets and comparing the effects of adversarial attacks on different segmentation network architectures. To enable this, a robust dataset is created consisting of only robust features and training the networks on this robustified dataset. We present both qualitative and quantitative analysis of our findings, which have important implications on improving the robustness of machine learning models in off-road autonomous driving applications. Additionally, this work contributes to the safe navigation of autonomous robot Unimog U5023 in rough off-road unstructured environments by evaluating the robustness of segmentation outputs. The code is publicly available at https://github.com/rohtkumar/adversarial_attacks_ on_segmentation", "venue": "arXiv", "keywords": ["adversarial learning", "adversarial robustness", "robustness analysis", "semantic segmentation"]}
{"id": "DetailedOverviewPopular2023", "title": "A Detailed Overview Of Popular Video Compression Techniques", "abstract": "We demystify the jargon and guide you through the most effective video compression methods for your streaming requirements.", "venue": "ImageKit.io Blog", "keywords": []}
{"id": "devlinBERTPretrainingDeep2019", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.", "venue": "arXiv", "keywords": []}
{"id": "devriesImprovedRegularizationConvolutional2017", "title": "Improved Regularization of Convolutional Neural Networks with Cutout", "abstract": "Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56%, 15.20%, and 1.30% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout", "venue": "arXiv", "keywords": ["mixture augmentations"]}
{"id": "diamondDirtyPixelsEndtoEnd2017", "title": "Dirty Pixels: Towards End-to-End Image Processing and Perception", "abstract": "Real-world imaging systems acquire measurements that are degraded by noise, optical aberrations, and other imperfections that make image processing for human viewing and higher-level perception tasks challenging. Conventional cameras address this problem by compartmentalizing imaging from high-level task processing. As such, conventional imaging involves processing the RAW sensor measurements in a sequential pipeline of steps, such as demosaicking, denoising, deblurring, tone-mapping and compression. This pipeline is optimized to obtain a visually pleasing image. High-level processing, on the other hand, involves steps such as feature extraction, classification, tracking, and fusion. While this siloed design approach allows for efficient development, it also dictates compartmentalized performance metrics, without knowledge of the higher-level task of the camera system. For example, today's demosaicking and denoising algorithms are designed using perceptual image quality metrics but not with domain-specific tasks such as object detection in mind. We propose an end-to-end differentiable architecture that jointly performs demosaicking, denoising, deblurring, tone-mapping, and classification. The architecture learns processing pipelines whose outputs differ from those of existing ISPs optimized for perceptual quality, preserving fine detail at the cost of increased noise and artifacts. We demonstrate on captured and simulated data that our model substantially improves perception in low light and other challenging conditions, which is imperative for real-world applications. Finally, we found that the proposed model also achieves state-of-the-art accuracy when optimized for image reconstruction in low-light conditions, validating the architecture itself as a potentially useful drop-in network for reconstruction and analysis tasks beyond the applications demonstrated in this work.", "venue": "arXiv", "keywords": ["noise injection"]}
{"id": "dinhDensityEstimationUsing2017", "title": "Density Estimation Using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "venue": "arXiv", "keywords": ["distribution estimation", "foundational", "normalizing flows", "representation learning", "vaes", "variational inference"]}
{"id": "dinhNICENonlinearIndependent2015", "title": "NICE: Non-linear Independent Components Estimation", "abstract": "We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.", "venue": "arXiv", "keywords": ["distribution estimation", "feature engineering", "foundational", "manifold learning", "normalizing flows"]}
{"id": "DiscoveringStatesTransformations", "title": "Discovering States and Transformations in Image Collections", "abstract": "", "venue": "", "keywords": []}
{"id": "doerschUnsupervisedVisualRepresentation2016", "title": "Unsupervised Visual Representation Learning by Context Prediction", "abstract": "This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the RCNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-theart performance among algorithms which use only Pascalprovided training set annotations.", "venue": "arXiv", "keywords": ["representation learning", "unsupervised learning"]}
{"id": "dolezalUncertaintyinformedDeepLearning2022", "title": "Uncertainty-Informed Deep Learning Models Enable High-Confidence Predictions for Digital Histopathology", "abstract": "Abstract A model's ability to express its own predictive uncertainty is an essential attribute for maintaining clinical user confidence as computational biomarkers are deployed into real-world medical settings. In the domain of cancer digital histopathology, we describe a clinically-oriented approach to uncertainty quantification for whole-slide images, estimating uncertainty using dropout and calculating thresholds on training data to establish cutoffs for low- and high-confidence predictions. We train models to identify lung adenocarcinoma vs. squamous cell carcinoma and show that high-confidence predictions outperform predictions without uncertainty, in both cross-validation and testing on two large external datasets spanning multiple institutions. Our testing strategy closely approximates real-world application, with predictions generated on unsupervised, unannotated slides using predetermined thresholds. Furthermore, we show that uncertainty thresholding remains reliable in the setting of domain shift, with accurate high-confidence predictions of adenocarcinoma vs. squamous cell carcinoma for out-of-distribution, non-lung cancer cohorts.", "venue": "Nature Communications", "keywords": []}
{"id": "douDomainGeneralizationModelAgnostic2019", "title": "Domain Generalization via Model-Agnostic Learning of Semantic Features", "abstract": "Generalization capability to unseen domains is crucial for machine learning models when deploying to real-world conditions. We investigate the challenging problem of domain generalization, i.e., training a model on multi-domain source data such that it can directly generalize to target domains with unknown statistics. We adopt a model-agnostic learning paradigm with gradient-based meta-train and meta-test procedures to expose the optimization to domain shift. Further, we introduce two complementary losses which explicitly regularize the semantic structure of the feature space. Globally, we align a derived soft confusion matrix to preserve general knowledge about inter-class relationships. Locally, we promote domain-independent class-specific cohesion and separation of sample features with a metric-learning component. The effectiveness of our method is demonstrated with new state-of-the-art results on two common object recognition benchmarks. Our method also shows consistent improvement on a medical image segmentation task.", "venue": "arXiv", "keywords": ["class balancing", "domain generalization", "meta-learning", "semantic segmentation"]}
{"id": "draxlerCharacterizingRoleSingle2021", "title": "Characterizing the Role of a Single Coupling Layer in Affine Normalizing Flows", "abstract": "Deep Affine Normalizing Flows are efficient and powerful models for high-dimensional density estimation and sample generation. Yet little is known about how they succeed in approximating complex distributions, given the seemingly limited expressiveness of individual affine layers. In this work, we take a first step towards theoretical understanding by analyzing the behaviour of a single affine coupling layer under maximum likelihood loss. We show that such a layer estimates and normalizes conditional moments of the data distribution, and derive a tight lower bound on the loss depending on the orthogonal transformation of the data before the affine coupling. This bound can be used to identify the optimal orthogonal transform, yielding a layer-wise training algorithm for deep affine flows. Toy examples confirm our findings and stimulate further research by highlighting the remaining gap between layer-wise and end-to-end training of deep affine flows.", "venue": "Pattern Recognition", "keywords": ["normalizing flows"]}
{"id": "draxlerCharacterizingRoleSingle2021a", "title": "Characterizing the Role of a Single Coupling Layer in Affine Normalizing Flows", "abstract": "Deep Affine Normalizing Flows are efficient and powerful models for high-dimensional density estimation and sample generation. Yet little is known about how they succeed in approximating complex distributions, given the seemingly limited expressiveness of individual affine layers. In this work, we take a first step towards theoretical understanding by analyzing the behaviour of a single affine coupling layer under maximum likelihood loss. We show that such a layer estimates and normalizes conditional moments of the data distribution, and derive a tight lower bound on the loss depending on the orthogonal transformation of the data before the affine coupling. This bound can be used to identify the optimal orthogonal transform, yielding a layer-wise training algorithm for deep affine flows. Toy examples confirm our findings and stimulate further research by highlighting the remaining gap between layer-wise and end-to-end training of deep affine flows.", "venue": "Pattern Recognition", "keywords": ["highly-analytical", "normalizing flows"]}
{"id": "draxlerUniversalityCouplingbasedNormalizing2024", "title": "On the Universality of Coupling-based Normalizing Flows", "abstract": "We present a novel theoretical framework for understanding the expressive power of coupling-based normalizing flows such as RealNVP. Despite their prevalence in scientific applications, a comprehensive understanding of coupling flows remains elusive due to their restricted architectures. Existing theorems fall short as they require the use of arbitrarily ill-conditioned neural networks, limiting practical applicability. Additionally, we demonstrate that these constructions inherently lead to volume-preserving flows, a property which we show to be a fundamental constraint for expressivity. We propose a new distributional universality theorem for coupling-based normalizing flows, which overcomes several limitations of prior work. Our results support the general wisdom that the coupling architecture is expressive and provide a nuanced view for choosing the expressivity of coupling functions, bridging a gap between empirical results and theoretical understanding.", "venue": "arXiv", "keywords": ["highly-analytical", "normalizing flows"]}
{"id": "drineasFastApproximationMatrix2012", "title": "Fast Approximation of Matrix Coherence and Statistical Leverage", "abstract": "The statistical leverage scores of a matrix \\ are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystr \" -based low-rank matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary \\ matrix \\ , with \\ , and that returns as output relative-error approximations to all \\ of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of \\ and \\ ) in \\ time, as opposed to the \\ time required by the na \" algorithm that involves computing an orthogonal basis for the range of \\ . Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with \\ , and the extension to streaming environments.", "venue": "arXiv", "keywords": ["anomaly detection", "ood detection"]}
{"id": "ducotterdImprovingLipschitzConstrainedNeural2023", "title": "Improving Lipschitz-Constrained Neural Networks by Learning Activation Functions", "abstract": "Lipschitz-constrained neural networks have several advantages over unconstrained ones and can be applied to a variety of problems, making them a topic of attention in the deep learning community. Unfortunately, it has been shown both theoretically and empirically that they perform poorly when equipped with ReLU activation functions. By contrast, neural networks with learnable 1-Lipschitz linear splines are known to be more expressive. In this paper, we show that such networks correspond to global optima of a constrained functional optimization problem that consists of the training of a neural network composed of 1-Lipschitz linear layers and 1-Lipschitz freeform activation functions with second-order total-variation regularization. Further, we propose an efficient method to train these neural networks. Our numerical experiments show that our trained networks compare favorably with existing 1-Lipschitz neural architectures.", "venue": "arXiv", "keywords": ["lipschitz-constraints"]}
{"id": "duHowMuchDeep2020", "title": "How Much Deep Learning Does Neural Style Transfer Really Need? An Ablation Study", "abstract": "Neural style transfer has been a ``killer app'' for deep learning, drawing attention from and advertising the effectiveness to both the academic and the general public. However, we have found by ablative experiments that optimizing an image in the way neural style transfer does, while the objective functions (or more precisely, the functions to transform raw images to corresponding feature maps being compared) are constructed without pretrained weights or biases, worked almost as well. We can even factor out the deepness (multiple layers of alternating linear and nonlinear transformations) alltogether and have neural style transfer working to a certain extent. This raises the question how much of the the current success of deep learning in computer vision should be attributed to training, structure or simply spatially aggregating the image.", "venue": "2020 IEEE Winter Conference on Applications of Computer Vision (WACV)", "keywords": ["style transfer", "surveys"]}
{"id": "duongEvaluatingEffectData2023", "title": "Evaluating the Effect of Data Augmentation and BALD Heuristics on Distillation of Semantic-KITTI Dataset", "abstract": "Active Learning (AL) has remained relatively unexplored for LiDAR perception tasks in autonomous driving datasets. In this study we evaluate Bayesian active learning methods applied to the task of dataset distillation or core subset selection (subset with near equivalent performance as full dataset). We also study the effect of application of data augmentation (DA) within Bayesian AL based dataset distillation. We perform these experiments on the full Semantic-KITTI dataset. We extend our study over our existing work (Duong. et al., 2022) only on 1/4th of the same dataset. Addition of DA and BALD have a negative impact over the labeling efficiency and thus the capacity to distill datasets. We demonstrate key issues in designing a functional AL framework and finally conclude with a review of challenges in real world active learning.", "venue": "arXiv", "keywords": ["knowledge distillation", "semantic segmentation"]}
{"id": "durkanNeuralSplineFlows2019", "title": "Neural Spline Flows", "abstract": "A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.", "venue": "arXiv", "keywords": ["augmentation stability", "neural splines", "normalizing flows"]}
{"id": "dwiyantoroDistanceIoULossImprovement2020", "title": "Distance-IoU Loss : An Improvement of IoU-based Loss for Object Detection Bounding Box Regression", "abstract": "In object detection task, we try to guide the computer to predict the objects and their location in a given image data. To accomplish this", "venue": "Nodeflux", "keywords": []}
{"id": "E11BioRoadmap", "title": "E11 Bio Roadmap", "abstract": "", "venue": "E11 Bio", "keywords": []}
{"id": "eastwoodProbableDomainGeneralization2022", "title": "Probable Domain Generalization via Quantile Risk Minimization", "abstract": "", "venue": "Advances in Neural Information Processing Systems", "keywords": ["domain generalization", "generalization certification", "highly-analytical", "risk optimization"]}
{"id": "edelsbrunnerPersistentHomologySurvey2008", "title": "Persistent Homology---a Survey", "abstract": "Persistent homology is an algebraic tool for measuring topological features of shapes and functions. It casts the multi-scale organization we frequently observe in nature into a mathematical formalism. Here we give a record of the short history of persistent homology and present its basic concepts. Besides the mathematics we focus on algorithms and mention the various connections to applications, including to biomolecules, biological networks, data analysis, and geometric modeling.", "venue": "Contemporary Mathematics", "keywords": ["persistent homology", "surveys", "topological data analysis"]}
{"id": "edelsbrunnerTopologicalDataAnalysis2019", "title": "Topological Data Analysis in Information Space", "abstract": "Various kinds of data are routinely represented as discrete probability distributions. Examples include text documents summarized by histograms of word occurrences and images represented as histograms of oriented gradients. Viewing a discrete probability distribution as a point in the standard simplex of the appropriate dimension, we can understand collections of such objects in geometric and topological terms. Importantly, instead of using the standard Euclidean distance, we look into dissimilarity measures with information-theoretic justification, and we develop the theory needed for applying topological data analysis in this setting. In doing so, we emphasize constructions that enable usage of existing computational topology software in this context.", "venue": "arXiv", "keywords": []}
{"id": "efrosImageQuiltingTexture", "title": "Image Quilting for Texture Synthesis and Transfer", "abstract": "We present a simple image-based method of generating novel visual appearance in which a new image is synthesized by stitching together small patches of existing images. We call this process image quilting. First, we use quilting as a fast and very simple texture synthesis algorithm which produces surprisingly good results for a wide range of textures. Second, we extend the algorithm to perform texture transfer -- rendering an object with a texture taken from a different object. More generally, we demonstrate how an image can be re-rendered in the style of a different image. The method works directly on the images and does not require 3D information.", "venue": "", "keywords": ["texture transfer"]}
{"id": "eichenseerDataSetProviding2022", "title": "A Data Set Providing Synthetic and Real-World Fisheye Video Sequences", "abstract": "In video surveillance as well as automotive applications, so-called fisheye cameras are often employed to capture a very wide angle of view. As such cameras depend on projections quite different from the classical perspective projection, the resulting fisheye image and video data correspondingly exhibits non-rectilinear image characteristics. Typical image and video processing algorithms, however, are not designed for these fisheye characteristics. To be able to develop and evaluate algorithms specifically adapted to fisheye images and videos, a corresponding test data set is therefore introduced in this paper. The first of those sequences were generated during the authors' own work on motion estimation for fish-eye videos and further sequences have gradually been added to create a more extensive collection. The data set now comprises synthetically generated fisheye sequences, ranging from simple patterns to more complex scenes, as well as fisheye video sequences captured with an actual fisheye camera. For the synthetic sequences, exact information on the lens employed is available, thus facilitating both verification and evaluation of any adapted algorithms. For the real-world sequences, we provide calibration data as well as the settings used during acquisition. The sequences are freely available via www.lms.lnt.de/fisheyedataset/.", "venue": "arXiv", "keywords": ["dataset debut", "fisheye distortion"]}
{"id": "eigenPredictingDepthSurface2015", "title": "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture", "abstract": "In this paper we address three different computer vision tasks using a single multiscale convolutional network architecture: depth prediction, surface normal estimation, and semantic labeling. The network that we develop is able to adapt naturally to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.", "venue": "arXiv", "keywords": ["depth estimation", "feature pyramids"]}
{"id": "elfleinMasterThesisOutofdistribution2023", "title": "Master's Thesis: Out-of-distribution Detection with Energy-based Models", "abstract": "Today, deep learning is increasingly applied in security-critical situations such as autonomous driving and medical diagnosis. Despite its success, the behavior and robustness of deep networks are not fully understood yet, posing a significant risk. In particular, researchers recently found that neural networks are overly confident in their predictions, even on data they have never seen before. To tackle this issue, one can differentiate two approaches in the literature. One accounts for uncertainty in the predictions, while the second estimates the underlying density of the training data to decide whether a given input is close to the training data, and thus the network is able to perform as expected.In this thesis, we investigate the capabilities of EBMs at the task of fitting the training data distribution to perform detection of out-of-distribution (OOD) inputs. We find that on most datasets, EBMs do not inherently outperform other density estimators at detecting OOD data despite their flexibility. Thus, we additionally investigate the effects of supervision, dimensionality reduction, and architectural modifications on the performance of EBMs. Further, we propose Energy-Prior Network (EPN) which enables estimation of various uncertainties within an EBM for classification, bridging the gap between two approaches for tackling the OOD detection problem. We identify a connection between the concentration parameters of the Dirichlet distribution and the joint energy in an EBM. Additionally, this allows optimization without a held-out OOD dataset, which might not be available or costly to collect in some applications. Finally, we empirically demonstrate that Energy-Prior Network (EPN) is able to detect OOD inputs, datasets shifts, and adversarial examples. Theoretically, EPN offers favorable properties for the asymptotic case when inputs are far from the training data.", "venue": "arXiv", "keywords": ["dissertations", "distribution estimation", "domain generalization"]}
{"id": "ericNormalizingFlowsTutorial", "title": "Normalizing Flows Tutorial, Part 2: Modern Normalizing Flows", "abstract": "This tutorial will show you how to use normalizing flows like MAF, IAF, and Real-NVP to deform an isotropic 2D Gaussian into a complex cl...", "venue": "", "keywords": ["normalizing flows"]}
{"id": "ericNormalizingFlowsTutoriala", "title": "Normalizing Flows Tutorial, Part 1: Distributions and Determinants", "abstract": "I'm looking for help translate these posts into different languages! Please email me at myfirstname mylastname 2004 at gmail.com if you ar...", "venue": "", "keywords": ["normalizing flows"]}
{"id": "erkentSemanticGridEstimation2018", "title": "Semantic Grid Estimation with Occupancy Grids and Semantic Segmentation Networks", "abstract": "We propose a method to estimate the semantic grid for an autonomous vehicle. The semantic grid is a 2D bird's eye view map where the grid cells contain semantic characteristics such as road, car, pedestrian, signage, etc. We obtain the semantic grid by fusing the semantic segmentation information and an occupancy grid computed by using a Bayesian filter technique. To compute the semantic information from a monocular RGB image, we integrate segmentation deep neural networks into our model. We use a deep neural network to learn the relation between the semantic information and the occupancy grid which can be trained end-to-end extending our previous work on semantic grids. Furthermore, we investigate the effect of using a conditional random field to refine the results. Finally, we test our method on two datasets and compare different architecture types for semantic segmentation. We perform the experiments on KITTI dataset and InriaChroma dataset.", "venue": "2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)", "keywords": []}
{"id": "erkentSemanticSegmentationUnsupervised2020", "title": "Semantic Segmentation With Unsupervised Domain Adaptation Under Varying Weather Conditions for Autonomous Vehicles", "abstract": "Semantic information provides a valuable source for scene understanding around autonomous vehicles in order to plan their actions and make decisions; however, varying weather conditions reduce the accuracy of the semantic segmentation. We propose a method to adapt to varying weather conditions without supervision, namely without labeled data. We update the parameters of a deep neural network (DNN) model that is pre-trained on the known weather condition (source domain) to adapt it to the new weather conditions (target domain) without forgetting the segmentation in the known weather condition. Furthermore, we don't require the labels from the source domain during adaptation training. The parameters of the DNN are optimized to reduce the distance between the distribution of the features from the images of old and new weather conditions. To measure this distance, we propose three alternatives: W-GAN, GAN and maximum-mean discrepancy (MMD). We evaluate our method on various datasets with varying weather conditions. The results show that the accuracy of the semantic segmentation is improved for varying conditions after adaptation with the proposed method.", "venue": "IEEE Robotics and Automation Letters", "keywords": ["adverse weather", "semantic segmentation", "unsupervised da"]}
{"id": "espeholtIMPALAScalableDistributed2018", "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures", "abstract": "In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.", "venue": "arXiv", "keywords": ["reinforcement learning"]}
{"id": "eyubogluDominoDiscoveringSystematic2022", "title": "Domino: Discovering Systematic Errors with Cross-Modal Embeddings", "abstract": "Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices on which a model performs poorly. To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent (i.e. united by a human-understandable concept). However, no quantitative evaluation framework currently exists for rigorously assessing SDMs with respect to these criteria. Additionally, prior qualitative evaluations have shown that SDMs often identify slices that are incoherent. In this work, we address these challenges by first designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data). Then, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices. We find that Domino accurately identifies 36% of the 1,235 slices in our framework - a 12 percentage point improvement over prior methods. Further, Domino is the first SDM that can provide natural language descriptions of identified slices, correctly generating the exact name of the slice in 35% of settings.", "venue": "arXiv", "keywords": []}
{"id": "fabbrizziStudyingBiasVisual2024", "title": "Studying Bias in Visual Features through the Lens of Optimal Transport", "abstract": "Computer vision systems are employed in a variety of high-impact applications. However, making them trustworthy requires methods for the detection of potential biases in their training data, before~models learn to harm already disadvantaged groups in downstream applications.~Image data are typically represented via extracted features, which can be hand-crafted or pre-trained neural network embeddings. In this work, we introduce a framework for bias discovery given such features that is based on optimal transport theory; it uses the (quadratic) Wasserstein distance to quantify disparity between the feature distributions of two demographic groups (e.g., women vs men). In this context, we show that the Kantorovich potentials of the images, which are a byproduct of computing the Wasserstein distance and act as ``transportation prices\", can serve as bias scores by indicating which images might exhibit distinct biased characteristics. We thus introduce a visual dataset exploration pipeline that helps auditors identify common characteristics across high- or low-scored images as potential sources of bias. We conduct a case study to identify prospective gender biases and demonstrate theoretically-derived properties with experiments on the CelebA and Biased MNIST datasets.", "venue": "Data Mining and Knowledge Discovery", "keywords": ["bias sources", "feature engineering", "optimal transport"]}
{"id": "FairUseYouTube", "title": "Fair Use on YouTube - YouTube Help", "abstract": "", "venue": "", "keywords": []}
{"id": "fanAbuttingGratingIllusion2022", "title": "Abutting Grating Illusion: Cognitive Challenge to Neural Network Models", "abstract": "Even the state-of-the-art deep learning models lack fundamental abilities compared to humans. Multiple comparison paradigms have been proposed to explore the distinctions between humans and deep learning. While most comparisons rely on corruptions inspired by mathematical transformations, very few have bases on human cognitive phenomena. In this study, we propose a novel corruption method based on the abutting grating illusion, which is a visual phenomenon widely discovered in both human and a wide range of animal species. The corruption method destroys the gradient-defined boundaries and generates the perception of illusory contours using line gratings abutting each other. We applied the method on MNIST, high resolution MNIST, and silhouette object images. Various deep learning models are tested on the corruption, including models trained from scratch and 109 models pretrained with ImageNet or various data augmentation techniques. Our results show that abutting grating corruption is challenging even for state-of-the-art deep learning models because most models are randomly guessing. We also discovered that the DeepAugment technique can greatly improve robustness against abutting grating illusion. Visualisation of early layers indicates that better performing models exhibit stronger end-stopping property, which is consistent with neuroscience discoveries. To validate the corruption method, 24 human subjects are involved to classify samples of corrupted datasets.", "venue": "arXiv", "keywords": ["adversarial robustness", "noise injection", "texture transfer"]}
{"id": "fanAbuttingGratingIllusion2022a", "title": "Abutting Grating Illusion: Cognitive Challenge to Neural Network Models", "abstract": "Even the state-of-the-art deep learning models lack fundamental abilities compared to humans. Multiple comparison paradigms have been proposed to explore the distinctions between humans and deep learning. While most comparisons rely on corruptions inspired by mathematical transformations, very few have bases on human cognitive phenomena. In this study, we propose a novel corruption method based on the abutting grating illusion, which is a visual phenomenon widely discovered in both human and a wide range of animal species. The corruption method destroys the gradient-defined boundaries and generates the perception of illusory contours using line gratings abutting each other. We applied the method on MNIST, high resolution MNIST, and silhouette object images. Various deep learning models are tested on the corruption, including models trained from scratch and 109 models pretrained with ImageNet or various data augmentation techniques. Our results show that abutting grating corruption is challenging even for state-of-the-art deep learning models because most models are randomly guessing. We also discovered that the DeepAugment technique can greatly improve robustness against abutting grating illusion. Visualisation of early layers indicates that better performing models exhibit stronger end-stopping property, which is consistent with neuroscience discoveries. To validate the corruption method, 24 human subjects are involved to classify samples of corrupted datasets.", "venue": "arXiv", "keywords": []}
{"id": "fanBidirectionallyLearningDense2022", "title": "Bidirectionally Learning Dense Spatio-temporal Feature Propagation Network for Unsupervised Video Object Segmentation", "abstract": "", "venue": "Proceedings of the 30th ACM International Conference on Multimedia", "keywords": ["bidirectional learning", "temporal consistency", "unsupervised learning"]}
{"id": "fanStyleFlowContentFixedImage2022", "title": "StyleFlow For Content-Fixed Image to Image Translation", "abstract": "Image-to-image (I2I) translation is a challenging topic in computer vision. We divide this problem into three tasks: strongly constrained translation, normally constrained translation, and weakly constrained translation. The constraint here indicates the extent to which the content or semantic information in the original image is preserved. Although previous approaches have achieved good performance in weakly constrained tasks, they failed to fully preserve the content in both strongly and normally constrained tasks, including photo-realism synthesis, style transfer, and colorization, etc. To achieve content-preserving transfer in strongly constrained and normally constrained tasks, we propose StyleFlow, a new I2I translation model that consists of normalizing flows and a novel Style-Aware Normalization (SAN) module. With the invertible network structure, StyleFlow first projects input images into deep feature space in the forward pass, while the backward pass utilizes the SAN module to perform content-fixed feature transformation and then projects back to image space. Our model supports both image-guided translation and multi-modal synthesis. We evaluate our model in several I2I translation benchmarks, and the results show that the proposed model has advantages over previous methods in both strongly constrained and normally constrained tasks.", "venue": "arXiv", "keywords": ["image-to-image", "normalizing flows", "style transfer"]}
{"id": "farynaAutomaticDataAugmentation2024", "title": "Automatic Data Augmentation to Improve Generalization of Deep Learning in H&E Stained Histopathology", "abstract": "In histopathology practice, scanners, tissue processing, staining, and image acquisition protocols vary from center to center, resulting in subtle variations in images. Vanilla convolutional neural networks are sensitive to such domain shifts. Data augmentation is a popular way to improve domain generalization. Currently, state-of-the-art domain generalization in computational pathology is achieved using a manually curated set of augmentation transforms. However, manual tuning of augmentation parameters is time-consuming and can lead to sub-optimal generalization performance. Meta-learning frameworks can provide efficient ways to find optimal training hyper-parameters, including data augmentation. In this study, we hypothesize that an automated search of augmentation hyper-parameters can provide superior generalization performance and reduce experimental optimization time. We select four state-of-the-art automatic augmentation methods from general computer vision and investigate their capacity to improve domain generalization in histopathology. We analyze their performance on data from 25 centers across two different tasks: tumor metastasis detection in lymph nodes and breast cancer tissue type classification. On tumor metastasis detection, most automatic augmentation methods achieve comparable performance to state-of-the-art manual augmentation. On breast cancer tissue type classification, the leading automatic augmentation method significantly outperforms state-of-the-art manual data augmentation.", "venue": "Computers in Biology and Medicine", "keywords": ["auto-augmentation policies", "surveys"]}
{"id": "fengNeuralAutoCurricula2021", "title": "Neural Auto-Curricula", "abstract": "When solving two-player zero-sum games, multi-agent reinforcement learning (MARL) algorithms often create populations of agents where, at each iteration, a new agent is discovered as the best response to a mixture over the opponent population. Within such a process, the update rules of \"who to compete with\" (i.e., the opponent mixture) and \"how to beat them\" (i.e., finding best responses) are underpinned by manually developed game theoretical principles such as fictitious play and Double Oracle. In this paper, we introduce a novel framework -- Neural Auto-Curricula (NAC) -- that leverages meta-gradient descent to automate the discovery of the learning update rule without explicit human design. Specifically, we parameterise the opponent selection module by neural networks and the best-response module by optimisation subroutines, and update their parameters solely via interaction with the game engine, where both players aim to minimise their exploitability. Surprisingly, even without human design, the discovered MARL algorithms achieve competitive or even better performance with the state-of-the-art population-based game solvers (e.g., PSRO) on Games of Skill, differentiable Lotto, non-transitive Mixture Games, Iterated Matching Pennies, and Kuhn Poker. Additionally, we show that NAC is able to generalise from small games to large games, for example training on Kuhn Poker and outperforming PSRO on Leduc Poker. Our work inspires a promising future direction to discover general MARL algorithms solely from data.", "venue": "arXiv", "keywords": ["auto-ml", "meta-learning"]}
{"id": "ferradansRegularizedDiscreteOptimal2013", "title": "Regularized Discrete Optimal Transport", "abstract": "This article introduces a generalization of the discrete optimal transport, with applications to color image manipulations. This new formulation includes a relaxation of the mass conservation constraint and a regularization term. These two features are crucial for image processing tasks, which necessitate to take into account families of multimodal histograms, with large mass variation across modes. The corresponding relaxed and regularized transportation problem is the solution of a convex optimization problem. Depending on the regularization used, this minimization can be solved using standard linear programming methods or first order proximal splitting schemes. The resulting transportation plan can be used as a color transfer map, which is robust to mass variation across images color palettes. Furthermore, the regularization of the transport plan helps to remove colorization artifacts due to noise amplification. We also extend this framework to the computation of barycenters of distributions. The barycenter is the solution of an optimization problem, which is separately convex with respect to the barycenter and the transportation plans, but not jointly convex. A block coordinate descent scheme converges to a stationary point of the energy. We show that the resulting algorithm can be used for color normalization across several images. The relaxed and regularized barycenter defines a common color palette for those images. Applying color transfer toward this average palette performs a color normalization of the input images.", "venue": "arXiv", "keywords": ["optimal transport", "promising"]}
{"id": "fingscheidtDeepNeuralNetworks2022", "title": "Deep Neural Networks and Data for Automated Driving: Robustness, Uncertainty Quantification, and Insights Towards Safety", "abstract": "This open access book brings together the latest developments from industry and research on automated driving and artificial intelligence. Environment perception for highly automated driving heavily employs deep neural networks, facing many challenges. How much data do we need for training and testing? How to use synthetic data to save labeling costs for training? How do we increase robustness and decrease memory usage? For inevitably poor conditions: How do we know that the network is uncertain about its decisions? Can we understand a bit more about what actually happens inside neural networks? This leads to a very practical problem particularly for DNNs employed in automated driving: What are useful validation techniques and how about safety? This book unites the views from both academia and industry, where computer vision and machine learning meet environment perception for highly automated driving. Naturally, aspects of data, robustness, uncertainty quantification, and, last but not least, safety are at the core of it. This book is unique: In its first part, an extended survey of all the relevant aspects is provided. The second part contains the detailed technical elaboration of the various questions mentioned above.", "venue": "Springer Nature", "keywords": []}
{"id": "Flare7KDataset", "title": "Flare 7K Dataset", "abstract": "", "venue": "", "keywords": ["automotive occlusion", "dataset debut"]}
{"id": "foxUnifyingInformationtheoreticPerspective2025", "title": "A Unifying Information-theoretic Perspective on Evaluating Generative Models", "abstract": "Considering the difficulty of interpreting generative model output, there is significant current research focused on determining meaningful evaluation metrics. Several recent approaches utilize \"precision\" and \"recall,\" borrowed from the classification domain, to individually quantify the output fidelity (realism) and output diversity (representation of the real data variation), respectively. With the increase in metric proposals, there is a need for a unifying perspective, allowing for easier comparison and clearer explanation of their benefits and drawbacks. To this end, we unify a class of kth-nearest-neighbors (kNN)-based metrics under an information-theoretic lens using approaches from kNN density estimation. Additionally, we propose a tri-dimensional metric composed of Precision Cross-Entropy (PCE), Recall Cross-Entropy (RCE), and Recall Entropy (RE), which separately measure fidelity and two distinct aspects of diversity, inter- and intra-class. Our domain-agnostic metric, derived from the information-theoretic concepts of entropy and cross-entropy, can be dissected for both sample- and mode-level analysis. Our detailed experimental results demonstrate the sensitivity of our metric components to their respective qualities and reveal undesirable behaviors of other metrics.", "venue": "arXiv", "keywords": []}
{"id": "FundamentalMatrixComputer2023", "title": "Fundamental Matrix (Computer Vision)", "abstract": "In computer vision, the fundamental matrix F \\ \\ \\ is a 3 3 matrix which relates corresponding points in stereo images. In epipolar geometry, with homogeneous image coordinates, x and x , of corresponding points in a stereo image pair, Fx describes a line (an epipolar line) on which the corresponding point x on the other image must lie. That means, for all pairs of corresponding points holds x F x = 0. \\ \\ ' \\ \\ \\ =0.\\ Being of rank two and determined only up to scale, the fundamental matrix can be estimated given at least seven point correspondences. Its seven parameters represent the only geometric information about cameras that can be obtained through point correspondences alone. The term \"fundamental matrix\" was coined by QT Luong in his influential PhD thesis. It is sometimes also referred to as the \"bifocal tensor\". As a tensor it is a two-point tensor in that it is a bilinear form relating points in distinct coordinate systems. The above relation which defines the fundamental matrix was published in 1992 by both Olivier Faugeras and Richard Hartley. Although H. Christopher Longuet-Higgins' essential matrix satisfies a similar relationship, the essential matrix is a metric object pertaining to calibrated cameras, while the fundamental matrix describes the correspondence in more general and fundamental terms of projective geometry. This is captured mathematically by the relationship between a fundamental matrix F \\ \\ \\ and its corresponding essential matrix E \\ \\ \\ , which is E = ( K ) F K \\ \\ =(\\ \\ '\\) \\ \\ ; \\ ; \\ \\ K \\ \\ \\ and K \\ \\ '\\ being the intrinsic calibration matrices of the two images involved.", "venue": "Wikipedia", "keywords": []}
{"id": "galakatosDistributedMachineLearning2017", "title": "Distributed Machine Learning", "abstract": "", "venue": "Encyclopedia of Database Systems", "keywords": []}
{"id": "galantiROLENEURALCOLLAPSE2022", "title": "ON THE ROLE OF NEURAL COLLAPSE IN TRANSFER LEARNING", "abstract": "We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. In this paper, we provide an explanation for this behavior based on the recently observed phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse. We demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and -- more importantly -- to new classes as well, allowing foundation models to provide feature maps that work well in transfer learning and, specifically, in the few-shot setting.", "venue": "", "keywords": ["transfer learning"]}
{"id": "ganinDomainAdversarialTrainingNeural2016", "title": "Domain-Adversarial Training of Neural Networks", "abstract": "We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.", "venue": "arXiv", "keywords": ["adversarial learning", "domain randomization", "foundational"]}
{"id": "gastalDomainTransformEdgeaware2011", "title": "Domain Transform for Edge-Aware Image and Video Processing", "abstract": "We present a new approach for performing high-quality edge-preserving filtering of images and videos in real time. Our solution is based on a transform that defines an isometry between curves on the 2D image manifold in 5D and the real line. This transform preserves the geodesic distance between points on these curves, adaptively warping the input signal so that 1D edge-preserving filtering can be efficiently performed in linear time. We demonstrate three realizations of 1D edge-preserving filters, show how to produce high-quality 2D edge-preserving filters by iterating 1D-filtering operations, and empirically analyze the convergence of this process. Our approach has several desirable features: the use of 1D operations leads to considerable speedups over existing techniques and potential memory savings; its computational cost is not affected by the choice of the filter parameters; and it is the first edge-preserving filter to work on color images at arbitrary scales in real time, without resorting to subsampling or quantization. We demonstrate the versatility of our domain transform and edge-preserving filters on several real-time image and video processing tasks including edge-preserving filtering, depth-of-field effects, stylization, recoloring, colorization, detail enhancement, and tone mapping.", "venue": "ACM Transactions on Graphics", "keywords": ["filter augmentations"]}
{"id": "gastalDomainTransformEdgeAware2011a", "title": "Domain Transform for Edge-Aware Image and Video Processing", "abstract": "We present a new approach for performing high-quality edge-preserving filtering of images and videos in real time. Our solution is based on a transform that defines an isometry between curves on the 2D image manifold in 5D and the real line. This transform preserves the geodesic distance between points on these curves, adaptively warping the input signal so that 1D edge-preserving filtering can be efficiently performed in linear time. We demonstrate three realizations of 1D edge-preserving filters, show how to produce high-quality 2D edge-preserving filters by iterating 1D-filtering operations, and empirically analyze the convergence of this process. Our approach has several desirable features: the use of 1D operations leads to considerable speedups over existing techniques and potential memory savings; its computational cost is not affected by the choice of the filter parameters; and it is the first edge-preserving filter to work on color images at arbitrary scales in real time, without resorting to subsampling or quantization. We demonstrate the versatility of our domain transform and edge-preserving filters on several real-time image and video processing tasks including edge-preserving filtering, depth-of-field effects, stylization, recoloring, colorization, detail enhancement, and tone mapping.", "venue": "ACM Trans. Graph.", "keywords": []}
{"id": "gatysControllingPerceptualFactors2017", "title": "Controlling Perceptual Factors in Neural Style Transfer", "abstract": "Neural Style Transfer has shown very exciting results enabling new forms of image manipulation. Here we extend the existing method to introduce control over spatial location, colour information and across spatial scale. We demonstrate how this enhances the method by allowing high-resolution controlled stylisation and helps to alleviate common failure cases such as applying ground textures to sky regions. Furthermore, by decomposing style into these perceptual factors we enable the combination of style information from multiple sources to generate new, perceptually appealing styles from existing ones. We also describe how these methods can be used to more efficiently produce large size, high-quality stylisation. Finally we show how the introduced control measures can be applied in recent methods for Fast Neural Style Transfer.", "venue": "arXiv", "keywords": ["style transfer", "texture transfer"]}
{"id": "gatysImageStyleTransfer2016", "title": "Image Style Transfer Using Convolutional Neural Networks", "abstract": "", "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "keywords": ["cnns", "style transfer"]}
{"id": "gatysNeuralAlgorithmArtistic2015", "title": "A Neural Algorithm of Artistic Style", "abstract": "In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.", "venue": "arXiv", "keywords": ["foundational", "style transfer"]}
{"id": "gatysPreservingColorNeural2016", "title": "Preserving Color in Neural Artistic Style Transfer", "abstract": "This note presents an extension to the neural artistic style transfer algorithm (Gatys et al.). The original algorithm transforms an image to have the style of another given image. For example, a photograph can be transformed to have the style of a famous painting. Here we address a potential shortcoming of the original method: the algorithm transfers the colors of the original painting, which can alter the appearance of the scene in undesirable ways. We describe simple linear methods for transferring style while preserving colors.", "venue": "arXiv", "keywords": ["style transfer", "texture transfer"]}
{"id": "gatysTextureSynthesisUsing2015", "title": "Texture Synthesis Using Convolutional Neural Networks", "abstract": "Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.", "venue": "arXiv.org", "keywords": ["cnns", "texture transfer"]}
{"id": "gazdievaUnbalancedLightOptimal2024", "title": "Unbalanced and Light Optimal Transport", "abstract": "While the field of continuous Entropic Optimal Transport (EOT) has been actively developing in recent years, it became evident that the classic EOT problem is prone to different issues like the sensitivity to outliers and imbalance of classes in the source and target measures. This fact inspired the development of solvers which deal with the unbalanced EOT (UEOT) problem - the generalization of EOT allowing for mitigating the mentioned issues by relaxing the marginal constraints. Surprisingly, it turns out that the existing solvers are either based on heuristic principles or heavyweighted with complex optimization objectives involving several neural networks. We address this challenge and propose a novel theoreticallyjustified and lightweight unbalanced EOT solver. Our advancement consists in developing a novel view on the optimization of the UEOT problem yielding tractable and non-minimax optimization objective. We show that combined with a light parametrization recently proposed in the field our objective leads to fast, simple and effective solver. It allows solving the continuous UEOT problem in minutes on CPU. We provide illustrative examples of the performance of our solver.", "venue": "arXiv", "keywords": ["entropic optimization", "highly-analytical", "optimal transport"]}
{"id": "geImprovingTransferabilityAdversarial2023", "title": "Improving the Transferability of Adversarial Examples with Arbitrary Style Transfer", "abstract": "Deep neural networks are vulnerable to adversarial examples crafted by applying human-imperceptible perturbations on clean inputs. Although many attack methods can achieve high success rates in the white-box setting, they also exhibit weak transferability in the black-box setting. Recently, various methods have been proposed to improve adversarial transferability, in which the input transformation is one of the most effective methods. In this work, we notice that existing input transformation-based works mainly adopt the transformed data in the same domain for augmentation. Inspired by domain generalization, we aim to further improve the transferability using the data augmented from different domains. Specifically, a style transfer network can alter the distribution of low-level visual features in an image while preserving semantic content for humans. Hence, we propose a novel attack method named Style Transfer Method (STM) that utilizes a proposed arbitrary style transfer network to transform the images into different domains. To avoid inconsistent semantic information of stylized images for the classification network, we fine-tune the style transfer network and mix up the generated images added by random noise with the original images to maintain semantic consistency and boost input diversity. Extensive experimental results on the ImageNet-compatible dataset show that our proposed method can significantly improve the adversarial transferability on either normally trained models or adversarially trained models than stateof-the-art input transformation-based attacks. Code is available at: https://github.com/Zhijin-Ge/STM.", "venue": "Proceedings of the 31st ACM International Conference on Multimedia", "keywords": ["style transfer"]}
{"id": "geirhosImageNettrainedCNNsAre2022", "title": "ImageNet-trained CNNs Are Biased towards Texture; Increasing Shape Bias Improves Accuracy and Robustness", "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.", "venue": "arXiv", "keywords": ["bias sources", "cnns"]}
{"id": "geMaximumLikelihoodEstimation2023", "title": "Maximum Likelihood Estimation Is All You Need for Well-Specified Covariate Shift", "abstract": "A key challenge of modern machine learning systems is to achieve Out-of-Distribution (OOD) generalization -- generalizing to target data whose distribution differs from that of source data. Despite its significant importance, the fundamental question of ``what are the most effective algorithms for OOD generalization'' remains open even under the standard setting of covariate shift. This paper addresses this fundamental question by proving that, surprisingly, classical Maximum Likelihood Estimation (MLE) purely using source data (without any modification) achieves the minimax optimality for covariate shift under the well-specified setting. That is, no algorithm performs better than MLE in this setting (up to a constant factor), justifying MLE is all you need. Our result holds for a very rich class of parametric models, and does not require any boundedness condition on the density ratio. We illustrate the wide applicability of our framework by instantiating it to three concrete examples -- linear regression, logistic regression, and phase retrieval. This paper further complement the study by proving that, under the misspecified setting, MLE is no longer the optimal choice, whereas Maximum Weighted Likelihood Estimator (MWLE) emerges as minimax optimal in certain scenarios.", "venue": "arXiv", "keywords": []}
{"id": "gengLegalTransformerModels2021", "title": "Legal Transformer Models May Not Always Help", "abstract": "Deep learning-based Natural Language Processing methods, especially transformers, have achieved impressive performance in the last few years. Applying those state-of-the-art NLP methods to legal activities to automate or simplify some simple work is of great value. This work investigates the value of domain adaptive pre-training and language adapters in legal NLP tasks. By comparing the performance of language models with domain adaptive pre-training on different tasks and different dataset splits, we show that domain adaptive pre-training is only helpful with lowresource downstream tasks, thus far from being a panacea. We also benchmark the performance of adapters in a typical legal NLP task and show that they can yield similar performance to full model tuning with much smaller training costs. As an additional result, we release LegalRoBERTa, a RoBERTa model further pre-trained on legal corpora.", "venue": "arXiv", "keywords": []}
{"id": "germainMADEMaskedAutoencoder2015", "title": "MADE: Masked Autoencoder for Distribution Estimation", "abstract": "There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.", "venue": "arXiv", "keywords": ["autoregressive flows", "distribution estimation"]}
{"id": "getreuerBLADEFilterLearning2017", "title": "BLADE: Filter Learning for General Purpose Computational Photography", "abstract": "The Rapid and Accurate Image Super Resolution (RAISR) method of Romano, Isidoro, and Milanfar is a computationally efficient image upscaling method using a trained set of filters. We describe a generalization of RAISR, which we name Best Linear Adaptive Enhancement (BLADE). This approach is a trainable edge-adaptive filtering framework that is general, simple, computationally efficient, and useful for a wide range of image processing problems. We show applications to denoising, compression artifact removal, demosaicing, and approximation of anisotropic diffusion equations.", "venue": "arXiv", "keywords": ["filter augmentations", "kernel methods"]}
{"id": "ghabussiWassersteinAutoencodersMixture2021", "title": "Wasserstein Autoencoders with Mixture of Gaussian Priors for Stylized Text Generation", "abstract": "Probabilistic autoencoders are effective for text generation. However, they are unable to control the style of generated text, despite the training samples explicitly labeled with different styles. We present a Wasserstein autoencoder with a Gaussian mixture prior for style-aware sentence generation. Our model is trained on a multi-class dataset and generates sentences in the style of the desired class. It is also capable of interpolating multiple classes. Moreover, we can train our model on relatively small datasets. While a regular WAE or VAE cannot generate diverse sentences with few training samples, our approach generates diverse sentences and preserves the style of the desired classes.", "venue": "Text, Speech, and Dialogue", "keywords": []}
{"id": "ghiasiExploringStructureRealtime2017", "title": "Exploring the Structure of a Real-Time, Arbitrary Neural Artistic Stylization Network", "abstract": "In this paper, we present a method which combines the flexibility of the neural algorithm of artistic style with the speed of fast style transfer networks to allow real-time stylization using any content/style image pair. We build upon recent work leveraging conditional instance normalization for multi-style transfer networks by learning to predict the conditional instance normalization parameters directly from a style image. The model is successfully trained on a corpus of roughly 80,000 paintings and is able to generalize to paintings previously unobserved. We demonstrate that the learned embedding space is smooth and contains a rich structure and organizes semantic information associated with paintings in an entirely unsupervised manner.", "venue": "arXiv", "keywords": ["style transfer"]}
{"id": "gholizadehShortSurveyTopological2018", "title": "A Short Survey of Topological Data Analysis in Time Series and Systems Analysis", "abstract": "Topological Data Analysis (TDA) is the collection of mathematical tools that capture the structure of shapes in data. Despite computational topology and computational geometry, the utilization of TDA in time series and signal processing is relatively new. In some recent contributions, TDA has been utilized as an alternative to the conventional signal processing methods. Specifically, TDA is been considered to deal with noisy signals and time series. In these applications, TDA is used to find the shapes in data as the main properties, while the other properties are assumed much less informative. In this paper, we will review recent developments and contributions where topological data analysis especially persistent homology has been applied to time series analysis, dynamical systems and signal processing. We will cover problem statements such as stability determination, risk analysis, systems behaviour, and predicting critical transitions in financial markets.", "venue": "arXiv", "keywords": []}
{"id": "giambagliMachineLearningSpectral2021", "title": "Machine Learning in Spectral Domain", "abstract": "Deep neural networks are usually trained in the space of the nodes, by adjusting the weights of existing links via suitable optimization protocols. We here propose a radically new approach which anchors the learning process to reciprocal space. Specifically, the training acts on the spectral domain and seeks to modify the eigenvalues and eigenvectors of transfer operators in direct space. The proposed method is ductile and can be tailored to return either linear or non-linear classifiers. Adjusting the eigenvalues, when freezing the eigenvectors entries, yields performances that are superior to those attained with standard methods restricted to operate with an identical number of free parameters. To recover a feed-forward architecture in direct space, we have postulated a nested indentation of the eigenvectors. Different non-orthogonal basis could be employed to export the spectral learning to other frameworks, as e.g. reservoir computing.", "venue": "Nature Communications", "keywords": ["spectral methods"]}
{"id": "gindeleBayesianOccupancyGrid2009", "title": "Bayesian Occupancy Grid Filter for Dynamic Environments Using Prior Map Knowledge", "abstract": "", "venue": "2009 IEEE Intelligent Vehicles Symposium", "keywords": []}
{"id": "gokhaleGeneralizedNotRobust2022", "title": "Generalized but Not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness", "abstract": "Data modification, either via additional training datasets, data augmentation, debiasing, and dataset filtering, has been proposed as an effective solution for generalizing to outof-domain (OOD) inputs, in both natural language processing and computer vision literature. However, the effect of data modification on adversarial robustness remains unclear. In this work, we conduct a comprehensive study of common data modification strategies and evaluate not only their in-domain and OOD performance, but also their adversarial robustness (AR). We also present results on a twodimensional synthetic dataset to visualize the effect of each method on the training distribution. This work serves as an empirical study towards understanding the relationship between generalizing to unseen domains and defending against adversarial perturbations. Our findings suggest that more data (either via additional datasets or data augmentation) benefits both OOD accuracy and AR. However, data filtering (previously shown to improve OOD accuracy on natural language inference) hurts OOD accuracy on other tasks such as question answering and image classification. We provide insights from our experiments to inform future work in this direction.", "venue": "arXiv", "keywords": ["domain generalization", "promising", "robustness analysis", "surveys"]}
{"id": "golatkarTimeMattersRegularizing2019", "title": "Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence", "abstract": "Regularization is typically understood as improving generalization by altering the landscape of local extrema to which the model eventually converges. Deep neural networks (DNNs), however, challenge this view: We show that removing regularization after an initial transient period has little effect on generalization, even if the final loss landscape is the same as if there had been no regularization. In some cases, generalization even improves after interrupting regularization. Conversely, if regularization is applied only after the initial transient, it has no effect on the final solution, whose generalization gap is as bad as if regularization never happened. This suggests that what matters for training deep networks is not just whether or how, but when to regularize. The phenomena we observe are manifest in different datasets (CIFAR-10, CIFAR-100), different architectures (ResNet-18, All-CNN), different regularization methods (weight decay, data augmentation), different learning rate schedules (exponential, piece-wise constant). They collectively suggest that there is a ``critical period'' for regularizing deep networks that is decisive of the final performance. More analysis should, therefore, focus on the transient rather than asymptotic behavior of learning.", "venue": "arXiv", "keywords": []}
{"id": "gonfalonieriWhyMachineLearning2019", "title": "Why Machine Learning Models Degrade In Production", "abstract": "After several failed ML projects due to unexpected ML degradation, I wanted to share my experience in ML models degradation. Indeed, there", "venue": "Medium", "keywords": []}
{"id": "gongCrossdomainFewshotLearning2023", "title": "Cross-Domain Few-Shot Learning Based on Pseudo-Siamese Neural Network", "abstract": "Abstract Cross-domain few-shot learning is one of the research highlights in machine learning. The difficulty lies in the accuracy drop of cross-domain network learning on a single domain due to the differences between the domains. To alleviate the problem, according to the idea of contour cognition and the process of human recognition, we propose a few-shot learning method based on pseudo-Siamese convolution neural network. The original image and the sketch map are respectively sent to the branch network in the pre-training and meta-learning process. While maintaining the original image features, the contour features are separately extracted as branch for training at the same time to improve the accuracy and generalization of learning. We conduct cross-domain few-shot learning experiments and good results have been achieved using mini-ImageNet as source domain, EuroSAT and ChestX as the target domains. Also, the results are qualitatively analyzed using a heatmap to verify the feasibility of our method.", "venue": "Scientific Reports", "keywords": ["cross-domain augmentation", "few-shot learning", "siamese networks"]}
{"id": "gongDLOWDomainFlow2019", "title": "DLOW: Domain Flow for Adaptation and Generalization", "abstract": "", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": ["gans", "instance normalization", "loss functions", "normalizing flows", "style transfer"]}
{"id": "gongGeodesicFlowKernel2012", "title": "Geodesic Flow Kernel for Unsupervised Domain Adaptation", "abstract": "In real-world applications of visual recognition, many factors - such as pose, illumination, or image quality - can cause a significant mismatch between the source domain on which classifiers are trained and the target domain to which those classifiers are applied. As such, the classifiers often perform poorly on the target domain. Domain adaptation techniques aim to correct the mismatch. Existing approaches have concentrated on learning feature representations that are invariant across domains, and they often do not directly exploit low-dimensional structures that are intrinsic to many vision datasets. In this paper, we propose a new kernel-based method that takes advantage of such structures. Our geodesic flow kernel models domain shift by integrating an infinite number of subspaces that characterize changes in geometric and statistical properties from the source to the target domain. Our approach is computationally advantageous, automatically inferring important algorithmic parameters without requiring extensive cross-validation or labeled data from either domain. We also introduce a metric that reliably measures the adaptability between a pair of source and target domains. For a given target domain and several source domains, the metric can be used to automatically select the optimal source domain to adapt and avoid less desirable ones. Empirical studies on standard datasets demonstrate the advantages of our approach over competing methods.", "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition", "keywords": ["domain adaptation", "feature engineering", "highly-analytical", "kernel methods", "manifold learning", "representation learning", "unsupervised da"]}
{"id": "gongPromptingDiffusionRepresentations2023", "title": "Prompting Diffusion Representations for Cross-Domain Semantic Segmentation", "abstract": "While originally designed for image generation, diffusion models have recently shown to provide excellent pretrained feature representations for semantic segmentation. Intrigued by this result, we set out to explore how well diffusion-pretrained representations generalize to new domains, a crucial ability for any representation. We find that diffusion-pretraining achieves extraordinary domain generalization results for semantic segmentation, outperforming both supervised and self-supervised backbone networks. Motivated by this, we investigate how to utilize the model's unique ability of taking an input prompt, in order to further enhance its crossdomain performance. We introduce a scene prompt and a prompt randomization strategy to help further disentangle the domain-invariant information when training the segmentation head. Moreover, we propose a simple but highly effective approach for test-time domain adaptation, based on learning a scene prompt on the target domain in an unsupervised manner. Extensive experiments conducted on four synthetic-to-real and clear-to-adverse weather benchmarks demonstrate the effectiveness of our approaches. Without resorting to any complex techniques, such as image translation, augmentation, or rare-class sampling, we set a new state-of-the-art on all benchmarks. Our implementation will be publicly available at https://github.com/ETHRuiGong/PTDiffSeg.", "venue": "arXiv", "keywords": ["diffusion models", "domain randomization", "promising", "representation learning", "semantic segmentation", "test-time da"]}
{"id": "goodfellowExplainingHarnessingAdversarial2015", "title": "Explaining and Harnessing Adversarial Examples", "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.", "venue": "arXiv", "keywords": ["adversarial robustness", "foundational", "robustness analysis"]}
{"id": "gormondInfluencePhotometricData2021", "title": "Influence of Photometric Data Augmentation", "abstract": "This research paper aims to explore influence of Photometric Data Augmentation within a complete Data Augmentation pipeline in context of Advanced Driver-Assistance Systems. We trained several object detection convolutional neural networks (CNN) with color augmentation techniques, measure their performances on the validation dataset and compare these strategies through a sensitivity analysis. Experimental results indicate that photometric augmentation increase CNN detection performances. Article written under the supervision of Ian Benderitter", "venue": "", "keywords": ["cnns", "photometric augmentations", "surveys"]}
{"id": "goukDISTANCEBASEDREGULARISATIONDEEP2021", "title": "DISTANCE-BASED REGULARISATION OF DEEP NETWORKS FOR FINE-TUNING", "abstract": "We investigate approaches to regularisation during fine-tuning of deep neural networks. First we provide a neural network generalisation bound based on Rademacher complexity that uses the distance the weights have moved from their initial values. This bound has no direct dependence on the number of weights and compares favourably to other bounds when applied to convolutional networks. Our bound is highly relevant for fine-tuning, because providing a network with a good initialisation based on transfer learning means that learning can modify the weights less, and hence achieve tighter generalisation. Inspired by this, we develop a simple yet effective fine-tuning algorithm that constrains the hypothesis class to a small sphere centred on the initial pre-trained weights, thus obtaining provably better generalisation performance than conventional transfer learning. Empirical evaluation shows that our algorithm works well, corroborating our theoretical results. It outperforms both state of the art fine-tuning competitors, and penalty-based alternatives that we show do not directly constrain the radius of the search space.", "venue": "", "keywords": []}
{"id": "goukDistanceBasedRegularisationDeep2021a", "title": "Distance-Based Regularisation of Deep Networks for Fine-Tuning", "abstract": "We investigate approaches to regularisation during fine-tuning of deep neural networks. First we provide a neural network generalisation bound based on Rademacher complexity that uses the distance the weights have moved from their initial values. This bound has no direct dependence on the number of weights and compares favourably to other bounds when applied to convolutional networks. Our bound is highly relevant for fine-tuning, because providing a network with a good initialisation based on transfer learning means that learning can modify the weights less, and hence achieve tighter generalisation. Inspired by this, we develop a simple yet effective fine-tuning algorithm that constrains the hypothesis class to a small sphere centred on the initial pre-trained weights, thus obtaining provably better generalisation performance than conventional transfer learning. Empirical evaluation shows that our algorithm works well, corroborating our theoretical results. It outperforms both state of the art fine-tuning competitors, and penalty-based alternatives that we show do not directly constrain the radius of the search space.", "venue": "arXiv", "keywords": ["domain generalization", "transfer learning"]}
{"id": "gowdridgeTopologicalDataAnalysis2022", "title": "On Topological Data Analysis for Structural Dynamics: An Introduction to Persistent Homology", "abstract": "Topological methods can provide a way of proposing new metrics and methods of scrutinising data, that otherwise may be overlooked. In this work, a method of quantifying the shape of data, via a topic called topological data analysis will be introduced. The main tool within topological data analysis (TDA) is persistent homology. Persistent homology is a method of quantifying the shape of data over a range of length scales. The required background and a method of computing persistent homology is briefly discussed in this work. Ideas from topological data analysis are then used for nonlinear dynamics to analyse some common attractors, by calculating their embedding dimension, and then to assess their general topologies. A method will also be proposed, that uses topological data analysis to determine the optimal delay for a time-delay embedding. TDA will also be applied to a Z24 Bridge case study in structural health monitoring, where it will be used to scrutinise different data partitions, classified by the conditions at which the data were collected. A metric, from topological data analysis, is used to compare data between the partitions. The results presented demonstrate that the presence of damage alters the manifold shape more significantly than the effects present from temperature.", "venue": "arXiv", "keywords": ["topological analysis"]}
{"id": "grandvaletSemisupervisedLearningEntropy", "title": "Semi-Supervised Learning by Entropy Minimization", "abstract": "We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data. In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning. Our approach includes other approaches to the semi-supervised problem as particular or limiting cases. A series of experiments illustrates that the proposed solution benefits from unlabeled data. The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model. The performances are definitely in favor of minimum entropy regularization when generative models are misspecified, and the weighting of unlabeled data provides robustness to the violation of the ``cluster assumption''. Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces.", "venue": "", "keywords": ["entropic optimization", "semi-supervised learning"]}
{"id": "grillBootstrapYourOwn2020", "title": "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning", "abstract": "We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \\ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \\ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.", "venue": "arXiv", "keywords": ["representation learning", "self-supervised learning", "siamese networks"]}
{"id": "groverALIGNFLOWLEARNINGMULTIPLE2019", "title": "ALIGNFLOW: LEARNING FROM MULTIPLE DOMAINS VIA NORMALIZING FLOWS", "abstract": "The goal of unpaired cross-domain translation is to learn useful mappings between two domains, given only unpaired sets of datapoints from these domains. While this formulation is highly underconstrained, recent work has shown that it is possible to learn mappings useful for downstream tasks by encouraging approximate cycle consistency in the mappings between the two domains (Zhu et al., 2017a). In this work, we propose AlignFlow, a framework for unpaired cross-domain translation that ensures exact cycle consistency in the learned mappings. Our framework uses a normalizing flow model to specify a single invertible mapping between the two domains. In contrast to prior works in cycle-consistent translations, we can learn AlignFlow via adversarial training, maximum likelihood estimation, or a hybrid of the two methods. Theoretically, we derive consistency results for AlignFlow which guarantee recovery of desirable mappings under suitable assumptions. Empirically, AlignFlow demonstrates significant improvements over relevant baselines on imageto-image translation and unsupervised domain adaptation tasks on benchmark datasets.", "venue": "", "keywords": ["gans", "image-to-image", "normalizing flows"]}
{"id": "gSoilClassificationUsing2023", "title": "Soil Classification Using Active Contour Model for Efficient Texture Feature Extraction", "abstract": "Precision farming is a systematic approach in agriculture that aims in improving economic and environment status of the farmers. It is achieved by having prior knowledge on soil texture, nutrient, pH and other climatic conditions. Hence this paper proposes a soil classification for crop prediction approach that uses an active contour algorithm for band estimation in Fourier domain for efficient texture feature extraction. This approach initially segments the soil sample and extracts into the color and texture features. The approach proposes a texture feature extraction where the image is initially transformed to Fourier domain of a 2D-discrete Fourier transform. The image in the Fourier domain is classified into high and low-frequency bands. The cut off frequency is decided by final contour of active contour method, where initial circular contour is used for estimating final contours on Fourier coefficients. This leads to the estimation of an irregular-shaped cut off frequency along with the 2D Fourier coefficients, instead of using a circular-shaped cut off frequency. A local binary pattern (LBP) from the high-frequency band image extracts texture feature. The extracted texture and color features are trained using a fully connected network. Active contour-based proposed model was evaluated by metrics F1-score, accuracy, specificity, sensitivity, and precision on soil datasets of Kaggle and IRSID. The accuracy, F1-score, specificity, precision, and sensitivity of proposed approach active contour-based were estimated as 97.89%, 97.87%, 99.46%, 98.11 and 97.94% respectively when evaluated in the Kaggle dataset. The evaluation results of proposed active contour model based soil classification outperform other traditional approaches.", "venue": "International Journal of Information Technology", "keywords": ["active contouring", "texture transfer"]}
{"id": "guArbitraryStyleTransfer2018", "title": "Arbitrary Style Transfer with Deep Feature Reshuffle", "abstract": "This paper introduces a novel method by reshuffling deep features (i.e., permuting the spacial locations of a feature map) of the style image for arbitrary style transfer. We theoretically prove that our new style loss based on reshuffle connects both global and local style losses respectively used by most parametric and non-parametric neural style transfer methods. This simple idea can effectively address the challenging issues in existing style transfer methods. On one hand, it can avoid distortions in local style patterns, and allow semantic-level transfer, compared with neural parametric methods. On the other hand, it can preserve globally similar appearance to the style image, and avoid wash-out artifacts, compared with neural non-parametric methods. Based on the proposed loss, we also present a progressive feature-domain optimization approach. The experiments show that our method is widely applicable to various styles, and produces better quality than existing methods.", "venue": "arXiv", "keywords": ["feature-level augmentation", "style transfer"]}
{"id": "guarinoMachineLearningbasedApproach2021", "title": "A Machine Learning-Based Approach to Identify Unlawful Practices in Online Terms of Service: Analysis, Implementation and Evaluation", "abstract": "Terms of Service (ToS) are fundamental factors in the creation of physical as well as online legally relevant relationships. They not only define mutual rights and obligations but also inform users about contract key issues that, in online settings, span from liability limitations to data management and processing conditions. Despite their crucial role, however, ToS are often neglected by users that frequently accept without even reading what they agree upon, representing a critical issue when there exist potentially unfair clauses. To enhance users' awareness and uphold legal safeguards, we first propose a definition of ToS unfairness based on a novel unfairness measure computed counting the unfair clauses contained in a ToS, and therefore, weighted according to their direct impact on the customers concrete interests. Secondly, we introduce a novel machine learning-based approach to classify ToS clauses, represented by using sentence embedding, in different categories classes and fairness levels. Results of a test involving well-known machine learning models show that Support Vector Machine is able to classify clauses into categories with a F1-score of 86% outperforming state-of-the-art methods, while Random Forest is able to classify clauses into fairness levels with a F1-score of 81%. With the final goal of making terms of service more readable and understandable, we embedded this approach into ToSware, a prototype of a Google Chrome extension. An evaluation study was performed to measure ToSware effectiveness, efficiency, and the overall users' satisfaction when interacting with it.", "venue": "Neural Computing and Applications", "keywords": []}
{"id": "guerraouiFeGANScalingDistributed2020", "title": "FeGAN: Scaling Distributed GANs", "abstract": "Existing approaches to distribute Generative Adversarial Networks (GANs) either (i) fail to scale for they typically put the two components of a GAN (the generator and the discriminator) on different machines, inducing significant communication overhead, or (ii) they face GAN training specific issues, exacerbated by distribution. We propose FeGAN, the first middleware for distributing GANs over hundreds of devices addressing the issues of mode collapse and vanishing gradients. Essentially, we revisit the idea of Federated Learning, co-locating a generator with a discriminator on each device (addressing the scaling problem) and having a server aggregate the devices' models using balanced sampling and Kullback-Leibler (KL) weighting, mitigating training issues and boosting convergence. Through extensive experiments, we show that FeGAN generates high-quality dataset samples in a scalable and devices' heterogeneity tolerant manner. In particular, FeGAN achieves up to 5 throughput gain with 1.5 less bandwidth compared to the state-of-the-art GAN distributed approach (named MD-GAN), while scaling to at least one order of magnitude more devices. We demonstrate that FeGAN boosts training by 2.6 w.r.t. a baseline application of Federated Learning to GANs, while preventing training issues.", "venue": "ACM/IFIP Middleware 2020 - Annual ACM/IFIP Middleware Conference", "keywords": []}
{"id": "guImageQualityAssessment", "title": "Image Quality Assessment for Perceptual Image Restoration: A New Dataset, Benchmark and Metric", "abstract": "", "venue": "", "keywords": []}
{"id": "guiSPTFineTuningTransformerbased2023", "title": "SPT: Fine-Tuning Transformer-based Language Models Efficiently with Sparsification", "abstract": "Transformer-based large language models (e.g., BERT and GPT) achieve great success, and fine-tuning, which tunes a pre-trained model on a task-specific dataset, is the standard practice to utilize these models for downstream tasks. However, Transformer fine-tuning has long running time and high memory consumption due to the large size of the models. We propose the SPT system to fine-tune Transformer-based models efficiently by introducing sparsity. We observe that the memory consumption of Transformer mainly comes from storing attention weights for multi-head attention (MHA), and the majority of running time is spent on feed-forward network (FFN). Thus, we design the sparse MHA module, which computes and stores only large attention weights to reduce memory consumption, and the routed FFN module, which dynamically activates a subset of model parameters for each token to reduce computation cost. We implement SPT on PyTorch and customize CUDA kernels to run sparse MHA and routed FFN efficiently. Specifically, we use product quantization to identify the large attention weights and compute attention via sparse matrix multiplication for sparse MHA. For routed FFN, we batch the tokens according to their activated model parameters for efficient computation. We conduct extensive experiments to evaluate SPT on various model configurations. The results show that SPT consistently outperforms well-optimized baselines, reducing the peak memory consumption by up to 50% and accelerating fine-tuning by up to 2.2x.", "venue": "arXiv", "keywords": []}
{"id": "guiziliniGeometricUnsupervisedDomain2021", "title": "Geometric Unsupervised Domain Adaptation for Semantic Segmentation", "abstract": "Simulators can efficiently generate large amounts of labeled synthetic data with perfect supervision for hard-tolabel tasks like semantic segmentation. However, they introduce a domain gap that severely hurts real-world performance. We propose to use self-supervised monocular depth estimation as a proxy task to bridge this gap and improve sim-to-real unsupervised domain adaptation (UDA). Our Geometric Unsupervised Domain Adaptation method (GUDA)1 learns a domain-invariant representation via a multi-task objective combining synthetic semantic supervision with real-world geometric constraints on videos. GUDA establishes a new state of the art in UDA for semantic segmentation on three benchmarks, outperforming methods that use domain adversarial learning, self-training, or other self-supervised proxy tasks. Furthermore, we show that our method scales well with the quality and quantity of synthetic data while also improving depth prediction.", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "keywords": ["depth estimation", "domain adaptation", "self-supervised learning", "semantic segmentation", "sim-to-real", "unsupervised da"]}
{"id": "gulrajaniImprovedTrainingWasserstein2017", "title": "Improved Training of Wasserstein GANs", "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.", "venue": "arXiv", "keywords": ["lipschitz-constraints"]}
{"id": "gulrajaniSearchLostDomain2020", "title": "In Search of Lost Domain Generalization", "abstract": "The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions -- datasets, architectures, and model selection criteria -- render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DomainBed, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DomainBed and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DomainBed, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization.", "venue": "arXiv", "keywords": ["critical citation", "domain generalization", "promising", "risk optimization", "surveys"]}
{"id": "guoCrossEntropyLabel2024", "title": "Cross Entropy versus Label Smoothing: A Neural Collapse Perspective", "abstract": "Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empirical evidence and theoretical results, not only provides nuanced insights into the differences between label smoothing and cross-entropy losses, but also serves as an example of how the powerful neural collapse framework can be used to improve our understanding of DNNs.", "venue": "arXiv", "keywords": []}
{"id": "guoKernelDensityEstimation2015", "title": "Kernel Density Estimation Method Basing on Color and Motion Features Frame for Moving Object Detection", "abstract": "Due to large calculations and complex background updating problems of kernel density estimation, this paper proposes a feature frame building method based on the combination of color feature and motion information, using this method to extract the number of samples N, it can not only reflect the global information of image but also reflect local information variations of image, besides it can effectively solve the inaccurate problem of the sample numbers, thereby enhancing the instantaneity of kernel density estimation algorithm.", "venue": "Proceedings of the 2nd International Conference on Intelligent Computing and Cognitive Informatics", "keywords": ["kernel methods", "temporal consistency"]}
{"id": "guptaCharacterizingImprovingStability2017", "title": "Characterizing and Improving Stability in Neural Style Transfer", "abstract": "Recent progress in style transfer on images has focused on improving the quality of stylized images and speed of methods. However, real-time methods are highly unstable resulting in visible flickering when applied to videos. In this work we characterize the instability of these methods by examining the solution set of the style transfer objective. We show that the trace of the Gram matrix representing style is inversely related to the stability of the method. Then, we present a recurrent convolutional network for real-time video style transfer which incorporates a temporal consistency loss and overcomes the instability of prior methods. Our networks can be applied at any resolution, do not require optical flow at test time, and produce high quality, temporally consistent stylized videos in real-time.", "venue": "2017 IEEE International Conference on Computer Vision (ICCV)", "keywords": ["augmentation stability", "optical flow", "style transfer", "temporal consistency"]}
{"id": "guptaFirstPrinciplesTheoryNeuralNetwork", "title": "A First-Principles Theory of Neural br Network Generalization", "abstract": "The BAIR Blog", "venue": "The Berkeley Artificial Intelligence Research Blog", "keywords": []}
{"id": "gutierrez-fandinoPersistentHomologyCaptures2021", "title": "Persistent Homology Captures the Generalization of Neural Networks Without A Validation Set", "abstract": "The training of neural networks is usually monitored with a validation (holdout) set to estimate the generalization of the model. This is done instead of measuring intrinsic properties of the model to determine whether it is learning appropriately. In this work, we suggest studying the training of neural networks with Algebraic Topology, specifically Persistent Homology (PH). Using simplicial complex representations of neural networks, we study the PH diagram distance evolution on the neural network learning process with different architectures and several datasets. Results show that the PH diagram distance between consecutive neural network states correlates with the validation accuracy, implying that the generalization error of a neural network could be intrinsically estimated without any holdout set.", "venue": "arXiv", "keywords": ["domain generalization", "persistent homology", "promising", "topological data analysis"]}
{"id": "haarnojaPolicyMaximumEntropy", "title": "Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.", "venue": "", "keywords": ["entropic optimization", "reinforcement learning"]}
{"id": "haarnojaSoftActorCriticOffPolicy2018", "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.", "venue": "arXiv", "keywords": ["foundational", "reinforcement learning"]}
{"id": "hadaStyleTransferRigid2020", "title": "Style Transfer by Rigid Alignment in Neural Net Feature Space", "abstract": "Arbitrary style transfer is an important problem in computer vision that aims to transfer style patterns from an arbitrary style image to a given content image. However, current methods either rely on slow iterative optimization or fast pre-determined feature transformation, but at the cost of compromised visual quality of the styled image; especially, distorted content structure. In this work, we present an effective and efficient approach for arbitrary style transfer that seamlessly transfers style patterns as well as keep content structure intact in the styled image. We achieve this by aligning style features to content features using rigid alignment; thus modifying style features, unlike the existing methods that do the opposite. We demonstrate the effectiveness of the proposed approach by generating high-quality stylized images and compare the results with the current state-of-the-art techniques for arbitrary style transfer.", "venue": "arXiv", "keywords": ["feature-level augmentation", "style transfer"]}
{"id": "haddouchePACBayesianLinkGeneralisation2024", "title": "A PAC-Bayesian Link Between Generalisation and Flat Minima", "abstract": "Modern machine learning usually involves predictors in the overparametrised setting (number of trained parameters greater than dataset size), and their training yield not only good performances on training data, but also good generalisation capacity. This phenomenon challenges many theoretical results, and remains an open problem. To reach a better understanding, we provide novel generalisation bounds involving gradient terms. To do so, we combine the PAC-Bayes toolbox with Poincar 'e and Log-Sobolev inequalities, avoiding an explicit dependency on dimension of the predictor space. Our results highlight the positive influence of minima\\ (being minima with a neighbourhood nearly minimising the learning problem as well) on generalisation performances, involving directly the benefits of the optimisation phase.", "venue": "arXiv", "keywords": ["generalization certification", "generalization quantification", "loss functions", "pac learning"]}
{"id": "haidarCILDAContrastiveData2022", "title": "CILDA: Contrastive Data Augmentation Using Intermediate Layer Knowledge Distillation", "abstract": "Knowledge distillation (KD) is an efficient framework for compressing large-scale pretrained language models. Recent years have seen a surge of research aiming to improve KD by leveraging Contrastive Learning, Intermediate Layer Distillation, Data Augmentation, and Adversarial Training. In this work, we propose a learning based data augmentation technique tailored for knowledge distillation, called CILDA. To the best of our knowledge, this is the first time that intermediate layer representations of the main task are used in improving the quality of augmented samples. More precisely, we introduce an augmentation technique for KD based on intermediate layer matching using contrastive loss to improve masked adversarial data augmentation. CILDA outperforms existing state-of-theart KD approaches on the GLUE benchmark, as well as in an out-of-domain evaluation.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "contrastive learning", "knowledge distillation"]}
{"id": "hajduSimulatingWeatherConditions", "title": "Simulating Weather Conditions on Digital Images", "abstract": "", "venue": "", "keywords": ["synthetic weather"]}
{"id": "hamriHierarchicalOptimalTransport2021", "title": "Hierarchical Optimal Transport for Unsupervised Domain Adaptation", "abstract": "In this paper, we propose a novel approach for unsupervised domain adaptation, that relates notions of optimal transport, learning probability measures and unsupervised learning. The proposed approach, HOT-DA, is based on a hierarchical formulation of optimal transport, that leverages beyond the geometrical information captured by the ground metric, richer structural information in the source and target domains. The additional information in the labeled source domain is formed instinctively by grouping samples into structures according to their class labels. While exploring hidden structures in the unlabeled target domain is reduced to the problem of learning probability measures through Wasserstein barycenter, which we prove to be equivalent to spectral clustering. Experiments on a toy dataset with controllable complexity and two challenging visual adaptation datasets show the superiority of the proposed approach over the state-of-the-art.", "venue": "arXiv", "keywords": ["optimal transport", "unsupervised da"]}
{"id": "hamriHierarchicalOptimalTransport2021a", "title": "Hierarchical Optimal Transport for Unsupervised Domain Adaptation", "abstract": "In this paper, we propose a novel approach for unsupervised domain adaptation, that relates notions of optimal transport, learning probability measures and unsupervised learning. The proposed approach, HOT-DA, is based on a hierarchical formulation of optimal transport, that leverages beyond the geometrical information captured by the ground metric, richer structural information in the source and target domains. The additional information in the labeled source domain is formed instinctively by grouping samples into structures according to their class labels. While exploring hidden structures in the unlabeled target domain is reduced to the problem of learning probability measures through Wasserstein barycenter, which we prove to be equivalent to spectral clustering. Experiments on a toy dataset with controllable complexity and two challenging visual adaptation datasets show the superiority of the proposed approach over the state-of-the-art.", "venue": "arXiv", "keywords": ["generalization quantification", "optimal transport", "unsupervised da"]}
{"id": "hamriTheoreticalGuaranteesDomain2022", "title": "Theoretical Guarantees for Domain Adaptation with Hierarchical Optimal Transport", "abstract": "Domain adaptation arises as an important problem in statistical learning theory when the data-generating processes differ between training and test samples, respectively called source and target domains. Recent theoretical advances show that the success of domain adaptation algorithms heavily relies on their ability to minimize the divergence between the probability distributions of the source and target domains. However, minimizing this divergence cannot be done independently of the minimization of other key ingredients such as the source risk or the combined error of the ideal joint hypothesis. The trade-off between these terms is often ensured by algorithmic solutions that remain implicit and not directly reflected by the theoretical guarantees. To get to the bottom of this issue, we propose in this paper a new theoretical framework for domain adaptation through hierarchical optimal transport. This framework provides more explicit generalization bounds and allows us to consider the natural hierarchical organization of samples in both domains into classes or clusters. Additionally, we provide a new divergence measure between the source and target domains called Hierarchical Wasserstein distance that indicates under mild assumptions, which structures have to be aligned to lead to a successful adaptation.", "venue": "arXiv", "keywords": ["generalization certification", "generalization quantification", "highly-analytical", "optimal transport"]}
{"id": "hararyUnsupervisedDomainGeneralization2022", "title": "Unsupervised Domain Generalization by Learning a Bridge Across Domains", "abstract": "The ability to generalize learned representations across significantly different visual domains, such as between real photos, clipart, paintings, and sketches, is a fundamental capacity of the human visual system. In this paper, different from most cross-domain works that utilize some (or full) source domain supervision, we approach a relatively new and very practical Unsupervised Domain Generalization (UDG) setup of having no training supervision in neither source nor target domains. Our approach is based on selfsupervised learning of a Bridge Across Domains (BrAD) an auxiliary bridge domain accompanied by a set of semantics preserving visual (image-to-image) mappings to BrAD from each of the training domains. The BrAD and mappings to it are learned jointly (end-to-end) with a contrastive selfsupervised representation model that semantically aligns each of the domains to its BrAD-projection, and hence implicitly drives all the domains (seen or unseen) to semantically align to each other. In this work, we show how using an edge-regularized BrAD our approach achieves significant gains across multiple benchmarks and a range of tasks, including UDG, Few-shot UDA, and unsupervised generalization across multi-domain datasets (including generalization to unseen domains and classes).", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": []}
{"id": "hararyUnsupervisedDomainGeneralization2022a", "title": "Unsupervised Domain Generalization by Learning a Bridge Across Domains", "abstract": "The ability to generalize learned representations across significantly different visual domains, such as between real photos, clipart, paintings, and sketches, is a fundamental capacity of the human visual system. In this paper, different from most cross-domain works that utilize some (or full) source domain supervision, we approach a relatively new and very practical Unsupervised Domain Generalization (UDG) setup of having no training supervision in neither source nor target domains. Our approach is based on self-supervised learning of a Bridge Across Domains (BrAD) - an auxiliary bridge domain accompanied by a set of semantics preserving visual (image-to-image) mappings to BrAD from each of the training domains. The BrAD and mappings to it are learned jointly (end-to-end) with a contrastive self-supervised representation model that semantically aligns each of the domains to its BrAD-projection, and hence implicitly drives all the domains (seen or unseen) to semantically align to each other. In this work, we show how using an edge-regularized BrAD our approach achieves significant gains across multiple benchmarks and a range of tasks, including UDG, Few-shot UDA, and unsupervised generalization across multi-domain datasets (including generalization to unseen domains and classes).", "venue": "arXiv", "keywords": ["contrastive learning", "domain generalization", "domain projection", "momentum encoders", "self-supervised learning"]}
{"id": "harleyParticleVideoRevisited2022", "title": "Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories", "abstract": "Tracking pixels in videos is typically studied as an optical flow estimation problem, where every pixel is described with a displacement vector that locates it in the next frame. Even though wider temporal context is freely available, prior efforts to take this into account have yielded only small gains over 2-frame methods. In this paper, we revisit Sand and Teller's ``particle video'' approach, and study pixel tracking as a long-range motion estimation problem, where every pixel is described with a trajectory that locates it in multiple future frames. We re-build this classic approach using components that drive the current state-ofthe-art in flow and object tracking, such as dense cost maps, iterative optimization, and learned appearance updates. We train our models using long-range amodal point trajectories mined from existing optical flow data that we synthetically augment with multi-frame occlusions. We test our approach in trajectory estimation benchmarks and in keypoint label propagation tasks, and compare favorably against state-of-the-art optical flow and feature tracking methods.", "venue": "arXiv", "keywords": ["automotive occlusion", "optical flow"]}
{"id": "hatayaFasterAutoAugmentLearning2019", "title": "Faster AutoAugment: Learning Augmentation Strategies Using Backpropagation", "abstract": "Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as the differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented data and the original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior work without a performance drop.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "haubergDreamingMoreData2016", "title": "Dreaming More Data: Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation", "abstract": "Data augmentation is a key element in training high-dimensional models. In this approach, one synthesizes new observations by applying pre-specified transformations to the original training data; e.g. images are formed by rotating old ones. Current augmentation schemes, however, rely on manual specification of the applied transformations, making data augmentation an implicit form of feature engineering. With an eye towards true end-to-end learning, we suggest learning the applied transformations on a per-class basis. Particularly, we align image pairs within each class under the assumption that the spatial transformation between images belongs to a large class of diffeomorphisms. We then learn a class-specific probabilistic generative models of the transformations in a Riemannian submanifold of the Lie group of diffeomorphisms. We demonstrate significant performance improvements in training deep neural nets over manually-specified augmentation schemes. Our code and augmented datasets are available online.", "venue": "arXiv", "keywords": ["geometric transformations", "highly-analytical", "representation learning"]}
{"id": "heFastMattingUsing2010", "title": "Fast Matting Using Large Kernel Matting Laplacian Matrices", "abstract": "Image matting is of great importance in both computer vision and graphics applications. Most existing state-of-the-art techniques rely on large sparse matrices such as the matting Laplacian . However, solving these linear systems is often time-consuming, which is unfavored for the user interaction. In this paper, we propose a fast method for high quality matting. We first derive an efficient algorithm to solve a large kernel matting Laplacian. A large kernel propagates information more quickly and may improve the matte quality. To further reduce running time, we also use adaptive kernel sizes by a KD-tree trimap segmentation technique. A variety of experiments show that our algorithm provides high quality results and is 5 to 20 times faster than previous methods.", "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "keywords": []}
{"id": "heGuidedImageFiltering2013", "title": "Guided Image Filtering", "abstract": "In this paper, we propose a novel explicit image filter called guided filter. Derived from a local linear model, the guided filter computes the filtering output by considering the content of a guidance image, which can be the input image itself or another different image. The guided filter can be used as an edge-preserving smoothing operator like the popular bilateral filter , but it has better behaviors near edges. The guided filter is also a more generic concept beyond smoothing: It can transfer the structures of the guidance image to the filtering output, enabling new filtering applications like dehazing and guided feathering. Moreover, the guided filter naturally has a fast and nonapproximate linear time algorithm, regardless of the kernel size and the intensity range. Currently, it is one of the fastest edge-preserving filters. Experiments show that the guided filter is both effective and efficient in a great variety of computer vision and computer graphics applications, including edge-aware smoothing, detail enhancement, HDR compression, image matting/feathering, dehazing, joint upsampling, etc.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "keywords": ["filter augmentations", "foundational", "guided filters"]}
{"id": "heGuidedImageFiltering2013a", "title": "Guided Image Filtering", "abstract": "In this paper, we propose a novel explicit image filter called guided filter. Derived from a local linear model, the guided filter computes the filtering output by considering the content of a guidance image, which can be the input image itself or another different image. The guided filter can be used as an edge-preserving smoothing operator like the popular bilateral filter , but it has better behaviors near edges. The guided filter is also a more generic concept beyond smoothing: It can transfer the structures of the guidance image to the filtering output, enabling new filtering applications like dehazing and guided feathering. Moreover, the guided filter naturally has a fast and nonapproximate linear time algorithm, regardless of the kernel size and the intensity range. Currently, it is one of the fastest edge-preserving filters. Experiments show that the guided filter is both effective and efficient in a great variety of computer vision and computer graphics applications, including edge-aware smoothing, detail enhancement, HDR compression, image matting/feathering, dehazing, joint upsampling, etc.", "venue": "IEEE transactions on pattern analysis and machine intelligence", "keywords": ["filter augmentations"]}
{"id": "heinze-demlConditionalVariancePenalties2017", "title": "Conditional Variance Penalties and Domain Shift Robustness", "abstract": "When training a deep neural network for image classification, one can broadly distinguish between two types of latent features of images that will drive the classification. We can divide latent features into (i) \"core\" or \"conditionally invariant\" features \\ whose distribution \\ , conditional on the class \\ , does not change substantially across domains and (ii) \"style\" features \\ whose distribution \\ can change substantially across domains. Examples for style features include position, rotation, image quality or brightness but also more complex ones like hair color, image quality or posture for images of persons. Our goal is to minimize a loss that is robust under changes in the distribution of these style features. In contrast to previous work, we assume that the domain itself is not observed and hence a latent variable. We do assume that we can sometimes observe a typically discrete identifier or \"\\ variable\". In some applications we know, for example, that two images show the same person, and \\ then refers to the identity of the person. The proposed method requires only a small fraction of images to have \\ information. We group observations if they share the same class and identifier \\ and penalize the conditional variance of the prediction or the loss if we condition on \\ . Using a causal framework, this conditional variance regularization (CoRe) is shown to protect asymptotically against shifts in the distribution of the style variables. Empirically, we show that the CoRe penalty improves predictive accuracy substantially in settings where domain changes occur in terms of image quality, brightness and color while we also look at more complex changes such as changes in movement and posture.", "venue": "arXiv.org", "keywords": ["distribution estimation", "domain generalization", "generalization certification"]}
{"id": "hellstromGeneralizationBoundsPerspectives2024", "title": "Generalization Bounds: Perspectives from Information Theory and PAC-Bayes", "abstract": "A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of PAC-Bayesian and information-theoretic generalization bounds. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demonstrate how many proofs in the area share a modular structure, through which the underlying ideas can be intuited. We pay special attention to the conditional mutual information (CMI) framework; analytical studies of the information complexity of learning algorithms; and the application of the proposed methods to deep learning. This monograph is intended to provide a comprehensive introduction to information-theoretic generalization bounds and their connection to PAC-Bayes, serving as a foundation from which the most recent developments are accessible. It is aimed broadly towards researchers with an interest in generalization and theoretical machine learning.", "venue": "arXiv", "keywords": ["generalization certification", "generalization quantification", "mutual information measures", "pac learning", "surveys"]}
{"id": "helmingerGenericImageRestoration2021", "title": "Generic Image Restoration With Flow Based Priors", "abstract": "", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": ["image restoration", "normalizing flows"]}
{"id": "hematiCrossDomainGenerative2023", "title": "Cross Domain Generative Augmentation: Domain Generalization with Latent Diffusion Models", "abstract": "Despite the huge effort in developing novel regularizers for Domain Generalization (DG), adding simple data augmentation to the vanilla ERM which is a practical implementation of the Vicinal Risk Minimization principle (VRM) 2000vicinal\\ outperforms or stays competitive with many of the proposed regularizers. The VRM reduces the estimation error in ERM by replacing the point-wise kernel estimates with a more precise estimation of true data distribution that reduces the gap between data points each domain\\. However, in the DG setting, the estimation error of true data distribution by ERM is mainly caused by the distribution shift domains\\ which cannot be fully addressed by simple data augmentation techniques within each domain. Inspired by this limitation of VRM, we propose a novel data augmentation named Cross Domain Generative Augmentation (CDGA) that replaces the pointwise kernel estimates in ERM with new density estimates in the of domain pairs\\ so that the gap between domains is further reduced. To this end, CDGA, which is built upon latent diffusion models (LDM), generates synthetic images to fill the gap between all domains and as a result, reduces the non-iidness. We show that CDGA outperforms SOTA DG methods under the Domainbed benchmark. To explain the effectiveness of CDGA, we generate more than 5 Million synthetic images and perform extensive ablation studies including data scaling laws, distribution visualization, domain shift quantification, adversarial robustness, and loss landscape analysis.", "venue": "arXiv", "keywords": ["cross-domain augmentation", "diffusion models", "generative augmentation", "visualizations"]}
{"id": "heMomentumContrastUnsupervised2020", "title": "Momentum Contrast for Unsupervised Visual Representation Learning", "abstract": "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.", "venue": "arXiv", "keywords": ["contrastive learning", "momentum encoders", "representation learning", "unsupervised learning"]}
{"id": "hendrycksAugMixSimpleData2020", "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty", "abstract": "Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.", "venue": "arXiv", "keywords": ["mixture augmentations", "pytorch native"]}
{"id": "hendrycksBenchmarkingNeuralNetwork2019", "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations", "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.", "venue": "arXiv", "keywords": ["generalization quantification", "robustness analysis"]}
{"id": "hendrycksDeepAnomalyDetection2019", "title": "Deep Anomaly Detection with Outlier Exposure", "abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.", "venue": "arXiv", "keywords": ["anomaly detection", "domain generalization"]}
{"id": "hendrycksManyFacesRobustness2021", "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization", "abstract": "We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000 times more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "dataset debut", "surveys"]}
{"id": "hendrycksMeasuringMathematicalProblem2021", "title": "Measuring Mathematical Problem Solving With the MATH Dataset", "abstract": "Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.", "venue": "arXiv", "keywords": []}
{"id": "hendrycksPixMixDreamlikePictures2022", "title": "PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures", "abstract": "In real-world applications of machine learning, reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out-of-distribution (OOD) robustness, prediction consistency, resilience to adversaries, calibrated uncertainty estimates, and the ability to detect anomalous inputs. However, improving performance towards these goals is often a balancing act that today's methods cannot achieve without sacrificing performance on other safety axes. For instance, adversarial training improves adversarial robustness but sharply degrades other classifier performance metrics. Similarly, strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection, raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge, we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals, which outperforms numerous baselines, is near Pareto-optimal, and roundly improves safety measures.", "venue": "arXiv", "keywords": ["mixture augmentations", "style transfer"]}
{"id": "henriquesHighSpeedTrackingKernelized2014", "title": "High-Speed Tracking with Kernelized Correlation Filters", "abstract": "The core component of most modern trackers is a discriminative classifier, tasked with distinguishing between the target and the surrounding environment. To cope with natural image changes, this classifier is typically trained with translated and scaled sample patches. Such sets of samples are riddled with redundancies -- any overlapping pixels are constrained to be the same. Based on this simple observation, we propose an analytic model for datasets of thousands of translated patches. By showing that the resulting data matrix is circulant, we can diagonalize it with the Discrete Fourier Transform, reducing both storage and computation by several orders of magnitude. Interestingly, for linear regression our formulation is equivalent to a correlation filter, used by some of the fastest competitive trackers. For kernel regression, however, we derive a new Kernelized Correlation Filter (KCF), that unlike other kernel algorithms has the exact same complexity as its linear counterpart. Building on it, we also propose a fast multi-channel extension of linear correlation filters, via a linear kernel, which we call Dual Correlation Filter (DCF). Both KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite running at hundreds of frames-per-second, and being implemented in a few lines of code (Algorithm 1). To encourage further developments, our tracking framework was made open-source.", "venue": "arXiv", "keywords": []}
{"id": "henselSurveyTopologicalMachine2021", "title": "A Survey of Topological Machine Learning Methods", "abstract": "The last decade saw an enormous boost in the field of computational topology: methods and concepts from algebraic and differential topology, formerly confined to the realm of pure mathematics, have demonstrated their utility in numerous areas such as computational biology personalised medicine, and time-dependent data analysis, to name a few. The newly-emerging domain comprising topology-based techniques is often referred to as topological data analysis (TDA). Next to their applications in the aforementioned areas, TDA methods have also proven to be effective in supporting, enhancing, and augmenting both classical machine learning and deep learning models. In this paper, we review the state of the art of a nascent field we refer to as 'topological machine learning', i.e. the successful symbiosis of topology-based methods and machine learning algorithms, such as deep neural networks. We identify common threads, current applications, and future challenges.", "venue": "Frontiers in Artificial Intelligence", "keywords": ["surveys", "topological data analysis", "topological ml"]}
{"id": "heProgressiveNormalizingFlow2024", "title": "Progressive Normalizing Flow with Learnable Spectrum Transform for Style Transfer", "abstract": "Most current style transfer models are designed as encoder--decoder structures. Some encoding operations, such as downsampling and pooling, cause a loss of image details. If the encoder and decoder are not compatible, it can also introduce distortion. Reversible neural networks have demonstrated their superior power in lossless projection. However, since the inputs and outputs of neural flows are holistic features, merely the high-level features can be utilized for image generation through reverse inference. These high-level features emphasize the image style more, leading to the generated results easily losing content details and producing abstract colors. To address the above issues, we propose LSTFlow, the first progressive reversible neural network capable of feature decomposition. First, LSTFlow incorporates our proposed reversible Learnable Spectrum Transform (LST), which can dynamically decompose the feature into feature spectrum and recover them losslessly. LSTFlow can retain more details by enabling multi-level features to be fused in backward inference. Second, we propose a Progressive Flow Stylization Strategy (PFSS) to balance the model's emphasis between content and style and enhance the color perception. Forward inference based PFSS is carried out progressively, while the backward inference focuses on progressive generation. To demonstrate the effectiveness of our proposed method, we conducted comparative experiments with seven other state-of-the-art algorithms. The stylized effects are evaluated in terms of visual effects and quantitative indicators. The experiments show that the lightest LSTFlow performs the best in SSIM, Color Entropy, Color Uniformity and FID indicators and outperforms state-of-the-art methods.", "venue": "Knowledge-Based Systems", "keywords": ["favorite", "normalizing flows", "spectral methods", "style transfer"]}
{"id": "hermannOriginsPrevalenceTexture2020", "title": "The Origins and Prevalence of Texture Bias in Convolutional Neural Networks", "abstract": "Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to classify images by texture rather than by shape. How pervasive is this bias, and where does it come from? We find that, when trained on datasets of images with conflicting shape and texture, CNNs learn to classify by shape at least as easily as by texture. What factors, then, produce the texture bias in CNNs trained on ImageNet? Different unsupervised training objectives and different architectures have small but significant and largely independent effects on the level of texture bias. However, all objectives and architectures still lead to models that make texture-based classification decisions a majority of the time, even if shape information is decodable from their hidden representations. The effect of data augmentation is much larger. By taking less aggressive random crops at training time and applying simple, naturalistic augmentation (color distortion, noise, and blur), we train models that classify ambiguous images by shape a majority of the time, and outperform baselines on out-of-distribution test sets. Our results indicate that apparent differences in the way humans and ImageNet-trained CNNs process images may arise not primarily from differences in their internal workings, but from differences in the data that they see.", "venue": "arXiv", "keywords": ["bias sources", "cnns", "feature engineering", "robustness analysis", "texture transfer"]}
{"id": "hermannOriginsPrevalenceTexture2020a", "title": "The Origins and Prevalence of Texture Bias in Convolutional Neural Networks", "abstract": "Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to classify images by texture rather than by shape. How pervasive is this bias, and where does it come from? We find that, when trained on datasets of images with conflicting shape and texture, CNNs learn to classify by shape at least as easily as by texture. What factors, then, produce the texture bias in CNNs trained on ImageNet? Different unsupervised training objectives and different architectures have small but significant and largely independent effects on the level of texture bias. However, all objectives and architectures still lead to models that make texture-based classification decisions a majority of the time, even if shape information is decodable from their hidden representations. The effect of data augmentation is much larger. By taking less aggressive random crops at training time and applying simple, naturalistic augmentation (color distortion, noise, and blur), we train models that classify ambiguous images by shape a majority of the time, and outperform baselines on out-of-distribution test sets. Our results indicate that apparent differences in the way humans and ImageNet-trained CNNs process images may arise not primarily from differences in their internal workings, but from differences in the data that they see.", "venue": "arXiv", "keywords": ["bias sources", "cnns"]}
{"id": "hernandez-cruzNeuralStyleTransfer2021", "title": "Neural Style Transfer as Data Augmentation for Improving COVID-19 Diagnosis Classification", "abstract": "Coronavirus disease 2019 (COVID-19) has accounted for millions of~causalities. While it affects not only individuals but also our collective healthcare and economic systems, testing is insufficient and costly hampering efforts to deal with the pandemic. Chest X-rays are routine radiographic imaging tests that are used for the diagnosis of respiratory conditions such as pneumonia and COVID-19. Convolutional neural networks have shown promise to be effective at classifying X-rays for assisting diagnosis of conditions; however, achieving robust performance demanded in most modern medical applications typically requires a large number of samples. While there exist datasets containing thousands of X-ray images of patients with healthy and pneumonia diagnoses, because COVID-19 is such a recent phenomenon, there are relatively few confirmed COVID-19 positive chest X-rays openly available to the research community. In this paper, we demonstrate the effectiveness of cycle-generative adversarial network, commonly used for neural style transfer, as a way to augment COVID-19 negative X-ray images to look like COVID-19 positive images for increasing the number of COVID-19 positive training samples. The statistical results show an increase in the mean macro f1-score over 21% on a one-tailed t score = 2.68 and p value = 0.01 to accept our alternative hypothesis for an \\ = 0.05\\ . We conclude that this approach, when used in conjunction with standard transfer learning techniques, is effective at improving the performance of COVID-19 classifiers for a variety of common convolutional neural networks.", "venue": "SN Computer Science", "keywords": ["adversarial learning", "cnns", "gans", "style transfer", "transfer learning"]}
{"id": "hesselMultitaskDeepReinforcement2018", "title": "Multi-Task Deep Reinforcement Learning with PopArt", "abstract": "The reinforcement learning community has made great strides in designing algorithms capable of exceeding human performance on specific tasks. These algorithms are mostly trained one task at the time, each new task requiring to train a brand new agent instance. This means the learning algorithm is general, but each solution is not; each agent can only solve the one task it was trained on. In this work, we study the problem of learning to master not one but multiple sequential-decision tasks at once. A general issue in multi-task learning is that a balance must be found between the needs of multiple tasks competing for the limited resources of a single learning system. Many learning algorithms can get distracted by certain tasks in the set of tasks to solve. Such tasks appear more salient to the learning process, for instance because of the density or magnitude of the in-task rewards. This causes the algorithm to focus on those salient tasks at the expense of generality. We propose to automatically adapt the contribution of each task to the agent's updates, so that all tasks have a similar impact on the learning dynamics. This resulted in state of the art performance on learning to play all games in a set of 57 diverse Atari games. Excitingly, our method learned a single trained policy - with a single set of weights - that exceeds median human performance. To our knowledge, this was the first time a single agent surpassed human-level performance on this multi-task domain. The same approach also demonstrated state of the art performance on a set of 30 tasks in the 3D reinforcement learning platform DeepMind Lab.", "venue": "arXiv", "keywords": ["multi-task learning", "reinforcement learning"]}
{"id": "heSurveyUncertaintyQuantification2024", "title": "A Survey on Uncertainty Quantification for Deep Learning: An Uncertainty Source Perspective", "abstract": "Deep neural networks (DNNs) have achieved tremendous success in making accurate predictions for computer vision, natural language processing, as well as science and engineering domains. However, it is also well-recognized that DNNs sometimes make unexpected, incorrect, but overconfident predictions. This can cause serious consequences in high-stake applications, such as autonomous driving, medical diagnosis, and disaster response. Uncertainty quantification (UQ) aims to estimate the confidence of DNN predictions beyond prediction accuracy. In recent years, many UQ methods have been developed for DNNs. It is of great practical value to systematically categorize these UQ methods and compare their advantages and disadvantages. However, existing surveys mostly focus on categorizing UQ methodologies from a neural network architecture perspective or a Bayesian perspective and ignore the source of uncertainty that each methodology can incorporate, making it difficult to select an appropriate UQ method in practice. To fill the gap, this paper presents a systematic taxonomy of UQ methods for DNNs based on the types of uncertainty sources (data uncertainty versus model uncertainty). We summarize the advantages and disadvantages of methods in each category. We show how our taxonomy of UQ methodologies can potentially help guide the choice of UQ method in different machine learning problems (e.g., active learning, robustness, and reinforcement learning). We also identify current research gaps and propose several future research directions.", "venue": "arXiv", "keywords": ["surveys", "uncertainty quantification"]}
{"id": "highamComputingNearestSymmetric1988", "title": "Computing a Nearest Symmetric Positive Semidefinite Matrix", "abstract": "The nearest symmetric positive semidefinite matrix in the Frobenius norm to an arbitrary real matrix A is shown to be (B + H)/2, where H is the symmetric polar factor of B=(A + AT)/2. In the 2-norm a nearest symmetric positive semidefinite matrix, and its distance 2(A) from A, are given by a computationally challenging formula due to Halmos. We show how the bisection method can be applied to this formula to compute upper and lower bounds for 2(A) differing by no more than a given amount. A key ingredient is a stable and efficient test for positive definiteness, based on an attempted Choleski decomposition. For accurate computation of 2(A) we formulate the problem as one of zero finding and apply a hybrid Newton-bisection algorithm. Some numerical difficulties are discussed and illustrated by example.", "venue": "Linear Algebra and its Applications", "keywords": ["feature engineering"]}
{"id": "hjelmLearningDeepRepresentations2019", "title": "Learning Deep Representations by Mutual Information Estimation and Maximization", "abstract": "In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.", "venue": "arXiv", "keywords": ["measure theory", "mutual information measures", "representation learning"]}
{"id": "hoermannDynamicOccupancyGrid2017", "title": "Dynamic Occupancy Grid Prediction for Urban Autonomous Driving: A Deep Learning Approach with Fully Automatic Labeling", "abstract": "Long-term situation prediction plays a crucial role in the development of intelligent vehicles. A major challenge still to overcome is the prediction of complex downtown scenarios with multiple road users, e.g., pedestrians, bikes, and motor vehicles, interacting with each other. This contribution tackles this challenge by combining a Bayesian filtering technique for environment representation, and machine learning as long-term predictor. More specifically, a dynamic occupancy grid map is utilized as input to a deep convolutional neural network. This yields the advantage of using spatially distributed velocity estimates from a single time step for prediction, rather than a raw data sequence, alleviating common problems dealing with input time series of multiple sensors. Furthermore, convolutional neural networks have the inherent characteristic of using context information, enabling the implicit modeling of road user interaction. Pixel-wise balancing is applied in the loss function counteracting the extreme imbalance between static and dynamic cells. One of the major advantages is the unsupervised learning character due to fully automatic label generation. The presented algorithm is trained and evaluated on multiple hours of recorded sensor data and compared to Monte-Carlo simulation.", "venue": "arXiv", "keywords": []}
{"id": "hoFlowImprovingFlowBased2019", "title": "Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design", "abstract": "Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. Despite their computational efficiency, flow-based models generally have much worse density modeling performance compared to state-of-the-art autoregressive models. In this paper, we investigate and improve upon three limiting design choices employed by flow-based models in prior work: the use of uniform noise for dequantization, the use of inexpressive affine flows, and the use of purely convolutional conditioning networks in coupling layers. Based on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant performance gap that has so far existed between autoregressive models and flow-based models. Our implementation is available at https://github.com/aravindsrinivas/flowpp", "venue": "arXiv", "keywords": ["generative models", "image synthesis", "normalizing flows"]}
{"id": "HomographyComputerVision2023", "title": "Homography (Computer Vision)", "abstract": "In the field of computer vision, any two images of the same planar surface in space are related by a homography (assuming a pinhole camera model). This has many practical applications, such as image rectification, image registration, or camera motion---rotation and translation---between two images. Once camera resectioning has been done from an estimated homography matrix, this information may be used for navigation, or to insert models of 3D objects into an image or video, so that they are rendered with the correct perspective and appear to have been part of the original scene (see Augmented reality).", "venue": "Wikipedia", "keywords": []}
{"id": "hongRobustnessNormalizingFlows2023", "title": "On the Robustness of Normalizing Flows for Inverse Problems in Imaging", "abstract": "", "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "keywords": ["normalizing flows", "robustness analysis"]}
{"id": "hongStyleMixSeparatingContent2021", "title": "StyleMix: Separating Content and Style for Enhanced Data Augmentation", "abstract": "In spite of the great success of deep neural networks for many challenging classification tasks, the learned networks are vulnerable to overfitting and adversarial attacks. Recently, mixup based augmentation methods have been actively studied as one practical remedy for these drawbacks. However, these approaches do not distinguish between the content and style features of the image, but mix or cut-andpaste the images. We propose StyleMix and StyleCutMix as the first mixup method that separately manipulates the content and style information of input image pairs. By carefully mixing up the content and style of images, we can create more abundant and robust samples, which eventually enhance the generalization of model training. We also develop an automatic scheme to decide the degree of style mixing according to the pair's class distance, to prevent messy mixed images from too differently styled pairs. Our experiments on CIFAR-10, CIFAR-100 and ImageNet datasets show that StyleMix achieves better or comparable performance to state of the art mixup methods and learns more robust classifiers to adversarial attacks.", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["mixture augmentations", "style transfer"]}
{"id": "hoPopulationBasedAugmentation2019", "title": "Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules", "abstract": "A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at https://github.com/arcelien/pba.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "houWhenLearnWhat2023", "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum", "abstract": "Data augmentation (DA) is widely used to improve the generalization of neural networks by enforcing the invariances and symmetries to pre-defined transformations applied to input data. However, a fixed augmentation policy may have different effects on each sample in different training stages but existing approaches cannot adjust the policy to be adaptive to each sample and the training model. In this paper, we propose ``Model-Adaptive Data Augmentation (MADAug)'' that jointly trains an augmentation policy network to teach the model ``when to learn what''. Unlike previous work, MADAug selects augmentation operators for each input image by a model-adaptive policy varying between training stages, producing a data augmentation curriculum optimized for better generalization. In MADAug, we train the policy through a bi-level optimization scheme, which aims to minimize a validationset loss of a model trained using the policy-produced data augmentations. We conduct an extensive evaluation of MADAug on multiple image classification tasks and network architectures with thorough comparisons to existing DA approaches. MADAug outperforms or is on par with other baselines and exhibits better fairness: it brings improvement to all classes and more to the difficult ones. Moreover, MADAug learned policy shows better performance when transferred to fine-grained datasets. In addition, the auto-optimized policy in MADAug gradually introduces increasing perturbations and naturally forms an easy-to-hard curriculum. Our code is available at https: //github.com/JackHck/MADAug.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "curriculum learning", "model adaptation"]}
{"id": "HowDownloadFiles", "title": "How to Download Files with cURL DigitalOcean", "abstract": "Download files from a remote server to your local system from the command-line using the curl command.", "venue": "", "keywords": []}
{"id": "hoyerDAFormerImprovingNetwork2022", "title": "DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation", "abstract": "As acquiring pixel-wise annotations of real-world images for semantic segmentation is a costly process, a model can instead be trained with more accessible synthetic data and adapted to real images without requiring their annotations. This process is studied in unsupervised domain adaptation (UDA). Even though a large number of methods propose new adaptation strategies, they are mostly based on outdated network architectures. As the influence of recent network architectures has not been systematically studied, we first benchmark different network architectures for UDA and newly reveal the potential of Transformers for UDA semantic segmentation. Based on the findings, we propose a novel UDA method, DAFormer. The network architecture of DAFormer consists of a Transformer encoder and a multilevel context-aware feature fusion decoder. It is enabled by three simple but crucial training strategies to stabilize the training and to avoid overfitting to the source domain: While (1) Rare Class Sampling on the source domain improves the quality of the pseudo-labels by mitigating the confirmation bias of self-training toward common classes, (2) a Thing-Class ImageNet Feature Distance and (3) a learning rate warmup promote feature transfer from ImageNet pretraining. DAFormer represents a major advance in UDA. It improves the state of the art by 10.8 mIoU for GTA Cityscapes and 5.4 mIoU for Synthia Cityscapes and enables learning even difficult classes such as train, bus, and truck well. The implementation is available at https://github.com/lhoyer/DAFormer.", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["promising", "segformer", "self-training", "semantic segmentation", "transformers", "unsupervised da"]}
{"id": "hoyerHRDAContextAwareHighResolution2022", "title": "HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation", "abstract": "Unsupervised domain adaptation (UDA) aims to adapt a model trained on the source domain (e.g. synthetic data) to the target domain (e.g. real-world data) without requiring further annotations on the target domain. This work focuses on UDA for semantic segmentation as real-world pixel-wise annotations are particularly expensive to acquire. As UDA methods for semantic segmentation are usually GPU memory intensive, most previous methods operate only on downscaled images. We question this design as low-resolution predictions often fail to preserve fine details. The alternative of training with random crops of high-resolution images alleviates this problem but falls short in capturing long-range, domain-robust context information. Therefore, we propose HRDA, a multi-resolution training approach for UDA, that combines the strengths of small high-resolution crops to preserve fine segmentation details and large low-resolution crops to capture long-range context dependencies with a learned scale attention, while maintaining a manageable GPU memory footprint. HRDA enables adapting small objects and preserving fine segmentation details. It significantly improves the state-of-the-art performance by 5.5 mIoU for GTA-to-Cityscapes and 4.9 mIoU for Synthia-to-Cityscapes, resulting in unprecedented 73.8 and 65.8 mIoU, respectively. The implementation is available at https://github.com/lhoyer/HRDA.", "venue": "arXiv", "keywords": ["domain adaptation", "segformer", "self-supervised learning", "self-training", "semantic segmentation"]}
{"id": "hoyerMICMaskedImage2023", "title": "MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation", "abstract": "In unsupervised domain adaptation (UDA), a model trained on source data (e.g. synthetic) is adapted to target data (e.g. real-world) without access to target annotation. Most previous UDA methods struggle with classes that have a similar visual appearance on the target domain as no ground truth is available to learn the slight appearance differences. To address this problem, we propose a Masked Image Consistency (MIC) module to enhance UDA by learning spatial context relations of the target domain as additional clues for robust visual recognition. MIC enforces the consistency between predictions of masked target images, where random patches are withheld, and pseudo-labels that are generated based on the complete image by an exponential moving average teacher. To minimize the consistency loss, the network has to learn to infer the predictions of the masked regions from their context. Due to its simple and universal concept, MIC can be integrated into various UDA methods across different visual recognition tasks such as image classification, semantic segmentation, and object detection. MIC significantly improves the state-of-the-art performance across the different recognition tasks for synthetic-to-real, day-to-nighttime, and clear-to-adverse-weather UDA. For instance, MIC achieves an unprecedented UDA performance of 75.9 mIoU and 92.8% on GTA-to-Cityscapes and VisDA-2017, respectively, which corresponds to an improvement of +2.1 and +3.0 percent points over the previous state of the art. The implementation is available at https://github.com/lhoyer/MIC.", "venue": "arXiv", "keywords": ["consistency training", "domain adaptation", "promising", "segformer", "unsupervised da"]}
{"id": "hsuGeneralizedODINDetecting2020", "title": "Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data", "abstract": "Deep neural networks have attained remarkable performance when applied to data that comes from the same distribution as that of the training set, but can significantly degrade otherwise. Therefore, detecting whether an example is out-of-distribution (OoD) is crucial to enable a system that can reject such samples or alert users. Recent works have made significant progress on OoD benchmarks consisting of small image datasets. However, many recent methods based on neural networks rely on training or tuning with both in-distribution and out-of-distribution data. The latter is generally hard to define a-priori, and its selection can easily bias the learning. We base our work on a popular method ODIN, proposing two strategies for freeing it from the needs of tuning with OoD data, while improving its OoD detection performance. We specifically propose to decompose confidence scoring as well as a modified input pre-processing method. We show that both of these significantly help in detection performance. Our further analysis on a larger scale image dataset shows that the two types of distribution shifts, specifically semantic shift and non-semantic shift, present a significant difference in the difficulty of the problem, providing an analysis of when ODIN-like strategies do or do not work.", "venue": "arXiv", "keywords": ["anomaly detection", "ood detection"]}
{"id": "HttpsWwwCvfoundation", "title": "https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_070.pdf", "abstract": "", "venue": "", "keywords": []}
{"id": "huaDistributedLearningPredictive2017", "title": "Distributed Learning of Predictive Structures From Multiple Tasks Over Networks", "abstract": "This paper is concerned with the problem of distributed multitask learning over networks, which aims to simultaneously infer multiple node-specific parameter vectors in a collaborative manner. Most of the existing works on the distributed multitask problem modeled the task relatedness by assuming some similarities of parameter vectors in an explicit way. In this paper, we implicitly model the similarity of parameter vectors by assuming that the parameter vectors share a common low-dimensional predictive structure on hypothesis spaces, which is learned using the available data in networks. A distributed structure learning algorithm for the in-network cooperative estimation problem is then derived based on the block coordinate descent method integrated with the inexact alternating direction method of multipliers technique. Simulations on both synthetic and real-world datasets are given to verify the effectiveness of the proposed algorithm. In the case that each node shares a common predictive subspace, it is demonstrated that the proposed multitask algorithm outperforms the noncooperative learning algorithm. Moreover, the use of the inexact approach can significantly reduce the communication bandwidth and still provide the same optimal solution as the corresponding centralized approach.", "venue": "IEEE Transactions on Industrial Electronics", "keywords": ["multi-task learning"]}
{"id": "huangArbitraryStyleTransfer2017", "title": "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization", "abstract": "Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.", "venue": "arXiv", "keywords": ["foundational", "instance normalization", "promising", "style transfer"]}
{"id": "huangAugmentedNormalizingFlows2020", "title": "Augmented Normalizing Flows: Bridging the Gap Between Generative Flows and Latent Variable Models", "abstract": "In this work, we propose a new family of generative flows on an augmented data space, with an aim to improve expressivity without drastically increasing the computational cost of sampling and evaluation of a lower bound on the likelihood. Theoretically, we prove the proposed flow can approximate a Hamiltonian ODE as a universal transport map. Empirically, we demonstrate stateof-the-art performance on standard benchmarks of flow-based generative modeling.", "venue": "arXiv", "keywords": ["generative models", "highly-analytical", "image synthesis", "normalizing flows"]}
{"id": "huangBatchingSoftIoU2020", "title": "Batching Soft IoU for Training Semantic Segmentation Networks", "abstract": "The majority of semantic segmentation networks generally employ cross-entropy as a loss function and intersectionover-union (IoU) as the evaluation metric for network performance. Employing IoU as a loss function can solve the mismatch issue between the loss function and the evaluation metric. We propose a Soft IoU training strategy based on mini-batch (mini-batch Soft IoU). Our work has two primary contributions: The first is to extend the IoU loss function to a multi-class segmentation network. The second is to collect various categories of the training samples in every mini-batch, which will ensure that the number of categories equals to the batch size at least. Our method breaks the randomness of the original mini-batch gradient descent (GD) strategy, advancing training samples in the mini-batch much more consistent with the distribution characteristics of the overall data. It solves the instability of IoU loss function. In addition, the experimental results on the PASCAL VOC2012 dataset reveal that our method effectively improves the segmentation accuracy of the network and attains significant improvements beyond state-of-the-art IoU loss function methods.", "venue": "IEEE Signal Processing Letters", "keywords": ["loss functions", "semantic segmentation"]}
{"id": "huangFSDRFrequencySpace2021", "title": "FSDR: Frequency Space Domain Randomization for Domain Generalization", "abstract": "Domain generalization aims to learn a generalizable model from a known source domain for various unknown target domains. It has been studied widely by domain randomization that transfers source images to different styles in spatial space for learning domain-agnostic features. However, most existing randomization uses GANs that often lack of controls and even alter semantic structures of images undesirably. Inspired by the idea of JPEG that converts spatial images into multiple frequency components (FCs), we propose Frequency Space Domain Randomization (FSDR) that randomizes images in frequency space by keeping domain-invariant FCs (DIFs) and randomizing domain-variant FCs (DVFs) only. FSDR has two unique features: 1) it decomposes images into DIFs and DVFs which allows explicit access and manipulation of them and more controllable randomization; 2) it has minimal effects on semantic structures of images and domain-invariant features. We examined domain variance and invariance property of FCs statistically and designed a network that can identify and fuse DIFs and DVFs dynamically through iterative learning. Extensive experiments over multiple domain generalizable segmentation tasks show that FSDR achieves superior segmentation and its performance is even on par with domain adaptation methods that access target data in training.", "venue": "arXiv", "keywords": ["bidirectional learning", "domain randomization", "feature-level augmentation", "promising", "spectral methods"]}
{"id": "huangIPMixLabelPreservingData2023", "title": "IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers", "abstract": "Data augmentation has been proven effective for training high-accuracy convolutional neural network classifiers by preventing overfitting. However, building deep neural networks in real-world scenarios requires not only high accuracy on clean data but also robustness when data distributions shift. While prior methods have proposed that there is a trade-off between accuracy and robustness, we propose IPMix, a simple data augmentation approach to improve robustness without hurting clean accuracy. IPMix integrates three levels of data augmentation (image-level, patch-level, and pixel-level) into a coherent and label-preserving technique to increase the diversity of training data with limited computational overhead. To further improve the robustness, IPMix introduces structural complexity at different levels to generate more diverse images and adopts the random mixing method for multi-scale information fusion. Experiments demonstrate that IPMix outperforms state-of-the-art corruption robustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also significantly improves the other safety measures, including robustness to adversarial perturbations, calibration, prediction consistency, and anomaly detection, achieving state-of-the-art or comparable results on several benchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.", "venue": "arXiv", "keywords": ["mixture augmentations"]}
{"id": "huangLearningAssociationsFeatures2021", "title": "Learning Associations between Features and Clusters: An Interpretable Deep Clustering Method", "abstract": "Clustering is a challenging problem when many features are irrelevant to separate clusters. Also, different clusters may relate to various feature subsets. This work proposes a deep clustering algorithm that localizes the search for instance clusters and their relevant features. The relevant features of each cluster are defined as those with high associations (dependency) within that cluster. Given the number of clusters K, we formulate the problem as K-parallel auto-reconstructive learning, where lowrank graph learning, rooted in graph Laplacian theory, is used to explore the unknown feature associations of each cluster. The model performs automatic feature weighting on residuals to minimize loss from the corresponding cluster. Through such design, different feature subsets can be learned to calculate the loss from different clusters. Subsequently, the associations between features and clusters can be acquired, and better clustering result can be achieved. Moreover, the associated features of each cluster can be used to interpret the clustering patterns.", "venue": "2021 International Joint Conference on Neural Networks (IJCNN)", "keywords": ["clustering", "manifold learning"]}
{"id": "huangMultimodalUnsupervisedImagetoImage2018", "title": "Multimodal Unsupervised Image-to-Image Translation", "abstract": "Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any examples of corresponding image pairs. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to state-of-the-art approaches further demonstrate the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT.", "venue": "arXiv", "keywords": ["bidirectional learning", "gans", "image-to-image", "unsupervised learning"]}
{"id": "huangOptimalImageTransport2023", "title": "Optimal Image Transport on Sparse Dictionaries", "abstract": "In this paper, we derive a novel optimal image transport algorithm over sparse dictionaries by taking advantage of Sparse Representation (SR) and Optimal Transport (OT). Concisely, we design a unified optimization framework in which the individual image features (color, textures, styles, etc.) are encoded using sparse representation compactly, and an optimal transport plan is then inferred between two learned dictionaries in accordance with the encoding process. This paradigm gives rise to a simple but effective way for simultaneous image representation and transformation, which is also empirically solvable because of the moderate size of sparse coding and optimal transport sub-problems. We demonstrate its versatility and many benefits to different image-to-image translation tasks, in particular image color transform and artistic style transfer, and show the plausible results for photo-realistic transferred effects.", "venue": "arXiv", "keywords": ["optimal transport"]}
{"id": "huangPartialDifferentialEquations2022", "title": "Partial Differential Equations Meet Deep Neural Networks: A Survey", "abstract": "Many problems in science and engineering can be represented by a set of partial differential equations (PDEs) through mathematical modeling. Mechanism-based computation following PDEs has long been an essential paradigm for studying topics such as computational fluid dynamics, multiphysics simulation, molecular dynamics, or even dynamical systems. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. At the same time, solving PDEs efficiently has been a long-standing challenge. Generally, except for a few differential equations for which analytical solutions are directly available, many more equations must rely on numerical approaches such as the finite difference method, finite element method, finite volume method, and boundary element method to be solved approximately. These numerical methods usually divide a continuous problem domain into discrete points and then concentrate on solving the system at each of those points. Though the effectiveness of these traditional numerical methods, the vast number of iterative operations accompanying each step forward significantly reduces the efficiency. Recently, another equally important paradigm, data-based computation represented by deep learning, has emerged as an effective means of solving PDEs. Surprisingly, a comprehensive review for this interesting subfield is still lacking. This survey aims to categorize and review the current progress on Deep Neural Networks (DNNs) for PDEs. We discuss the literature published in this subfield over the past decades and present them in a common taxonomy, followed by an overview and classification of applications of these related methods in scientific research and engineering scenarios. The origin, developing history, character, sort, as well as the future trends in each potential direction of this subfield are also introduced.", "venue": "arXiv", "keywords": []}
{"id": "huangResearchSemanticSegmentation2023", "title": "Research on Semantic Segmentation of Fish-Eye Images for Autonomous Driving", "abstract": "Abstract: Fisheye cameras, valued for their wide field of view, play a crucial role in perceiving the surrounding environment of vehicles. However, there is a lack of specific research addressing the processing of significant distortion features in segmenting fish-eye images. Additionally, fish-eye images for autonomous driving face the challenge of few datasets, potentially causing over fitting and hindering the model's generalization ability. Based on the semantic segmentation task, a method for transforming normal images into fish-eye images is proposed, which expands the fish-eye image dataset. By employing the Transformer network and the Across Feature Map Attention, the segmentation performance is further improved, achieving a 55.6% mIOU on Woodscape. Additionally, leveraging the concept of knowledge distillation, the network ensures a strong generalization based on dual-domain learning without compromising performance on Woodscape (54% mIOU).", "venue": "International Journal of Robotics and Automation Technology", "keywords": ["critical citation", "dual-domain learning", "fisheye distortion", "knowledge distillation", "promising", "segformer", "semantic segmentation"]}
{"id": "huangStyleDistributionFeatures2020", "title": "Style Is a Distribution of Features", "abstract": "Neural style transfer (NST) is a powerful image generation technique that uses a convolutional neural network (CNN) to merge the content of one image with the style of another. Contemporary methods of NST use first or second order statistics of the CNN's features to achieve transfers with relatively little computational cost. However, these methods cannot fully extract the style from the CNN's features. We present a new algorithm for style transfer that fully extracts the style from the features by redefining the style loss as the Wasserstein distance between the distribution of features. Thus, we set a new standard in style transfer quality. In addition, we state two important interpretations of NST. The first is a re-emphasis from Li et al. [2017a], which states that style is simply the distribution of features. The second states that NST is a type of generative adversarial network (GAN) problem.", "venue": "arXiv", "keywords": ["augmentation stability", "distribution matching", "style transfer", "surveys"]}
{"id": "huangStyleTransferBasedAugmentation2023", "title": "A Style Transfer-Based Augmentation Framework for~Improving Segmentation and~Classification Performance Across Different Sources in~Ultrasound Images", "abstract": "Ultrasound imaging can vary in style/appearance due to differences in scanning equipment and other factors, resulting in degraded segmentation and classification performance of deep learning models for ultrasound image analysis. Previous studies have attempted to solve this problem by using style transfer and augmentation techniques, but these methods usually require a large amount of data from multiple sources and source-specific discriminators, which are not feasible for medical datasets with limited samples. Moreover, finding suitable augmentation methods for ultrasound data can be difficult. To address these challenges, we propose a novel style transfer-based augmentation framework that consists of three components: mixed style augmentation (MixStyleAug), feature augmentation (FeatAug), and mask-based style augmentation (MaskAug). MixStyleAug uses a style transfer network to transform the style of a training image into various reference styles, which enriches the information from different sources for the network. FeatAug augments the styles at the feature level to compensate for possible style variations, especially for small-size datasets with limited styles. MaskAug leverages segmentation masks to highlight the key regions in the images, which enhances the model's generalizability. We evaluate our framework on five ultrasound datasets collected from different scanners and centers. Our framework outperforms previous methods on both segmentation and classification tasks, especially on small-size datasets. Our results suggest that our framework can effectively improve the performance of deep learning models across different ultrasound sources with limited data.", "venue": "Medical Image Computing and Computer Assisted Intervention -- MICCAI 2023", "keywords": ["feature-level augmentation", "masked style transfer", "mixture augmentations", "style transfer"]}
{"id": "huangTwoDimensionsWorstcase2022", "title": "The Two Dimensions of Worst-case Training and the Integrated Effect for Out-of-domain Generalization", "abstract": "Training with an emphasis on \"hard-to-learn\" components of the data has been proven as an effective method to improve the generalization of machine learning models, especially in the settings where robustness (e.g., generalization across distributions) is valued. Existing literature discussing this \"hard-to-learn\" concept are mainly expanded either along the dimension of the samples or the dimension of the features. In this paper, we aim to introduce a simple view merging these two dimensions, leading to a new, simple yet effective, heuristic to train machine learning models by emphasizing the worst-cases on both the sample and the feature dimensions. We name our method W2D following the concept of \"Worst-case along Two Dimensions\". We validate the idea and demonstrate its empirical strength over standard benchmarks.", "venue": "arXiv", "keywords": ["curriculum learning", "domain generalization", "generalization certification"]}
{"id": "huangUncertaintyEstimationNormalizedLogits2023", "title": "Uncertainty-Estimation with Normalized Logits for Out-of-Distribution Detection", "abstract": "Out-of-distribution (OOD) detection is critical for preventing deep learning models from making incorrect predictions to ensure the safety of artificial intelligence systems. Especially in safety-critical applications such as medical diagnosis and autonomous driving, the cost of incorrect decisions is usually unbearable. However, neural networks often suffer from the overconfidence issue, making high confidence for OOD data which are never seen during training process and may be irrelevant to training data, namely in-distribution (ID) data. Determining the reliability of the prediction is still a difficult and challenging task. In this work, we propose Uncertainty-Estimation with Normalized Logits (UE-NL), a robust learning method for OOD detection, which has three main benefits. (1) Neural networks with UE-NL treat every ID sample equally by predicting the uncertainty score of input data and the uncertainty is added into softmax function to adjust the learning strength of easy and hard samples during training phase, making the model learn robustly and accurately. (2) UE-NL enforces a constant vector norm on the logits to decouple the effect of the increasing output norm from optimization process, which causes the overconfidence issue to some extent. (3) UE-NL provides a new metric, the magnitude of uncertainty score, to detect OOD data. Experiments demonstrate that UE-NL achieves top performance on common OOD benchmarks and is more robust to noisy ID data that may be misjudged as OOD data by other methods.", "venue": "arXiv", "keywords": ["domain generalization", "loss functions", "uncertainty quantification"]}
{"id": "huComplexityMattersRethinking2023", "title": "Complexity Matters: Rethinking the Latent Space for Generative Modeling", "abstract": "In generative modeling, numerous successful approaches leverage a lowdimensional latent space, e.g., Stable Diffusion models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel \"distance\" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN and Diffusion Transformer , where our modifications yield significant improvements in sample quality with decreased model complexity.", "venue": "arXiv", "keywords": ["feature engineering", "image synthesis"]}
{"id": "huoManifoldAlignmentSemantically2021", "title": "Manifold Alignment for Semantically Aligned Style Transfer", "abstract": "Most existing style transfer methods follow the assumption that styles can be represented with global statistics (e.g., Gram matrices or covariance matrices), and thus address the problem by forcing the output and style images to have similar global statistics. An alternative is the assumption of local style patterns, where algorithms are designed to swap similar local features of content and style images. However, the limitation of these existing methods is that they neglect the semantic structure of the content image which may lead to corrupted content structure in the output. In this paper, we make a new assumption that image features from the same semantic region form a manifold and an image with multiple semantic regions follows a multi-manifold distribution. Based on this assumption, the style transfer problem is formulated as aligning two multi-manifold distributions and a Manifold Alignment based Style Transfer (MAST) framework is proposed. The proposed framework allows semantically similar regions between the output and the style image share similar style patterns. Moreover, the proposed manifold alignment method is flexible to allow user editing or using semantic segmentation maps as guidance for style transfer. To allow the method to be applicable to photorealistic style transfer, we propose a new adaptive weight skip connection network structure to preserve the content details. Extensive experiments verify the effectiveness of the proposed framework for both artistic and photorealistic style transfer. Code is available at https://github.com/NJUHuoJing/MAST.", "venue": "arXiv", "keywords": ["favorite", "manifold learning", "style transfer"]}
{"id": "hwangVideoInstanceSegmentation", "title": "Video Instance Segmentation Using Inter-Frame Communication Transformers", "abstract": "We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. Recently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. However, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality. In this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip. Specifically, we propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. We validate our method on the latest benchmark sets and achieved state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). Our method can also be applied to near-online inference for processing a video in real-time with only a small delay. The code is available at https://github.com/sukjunhwang/IFC.", "venue": "", "keywords": ["temporal consistency", "transformers"]}
{"id": "IKCEST", "title": "IKCEST", "abstract": "", "venue": "", "keywords": []}
{"id": "IntroductionZCAsphering", "title": "An Introduction to ZCA-sphering", "abstract": "", "venue": "", "keywords": ["feature whitening"]}
{"id": "ioannouDepthawareNeuralStyle2022", "title": "Depth-Aware Neural Style Transfer Using Instance Normalization", "abstract": "Neural Style Transfer (NST) is concerned with the artistic stylization of visual media. It can be described as the process of transferring the style of an artistic image onto an ordinary photograph. Recently, a number of studies have considered the enhancement of the depth-preserving capabilities of the NST algorithms to address the undesired effects that occur when the input content images include numerous objects at various depths. Our approach uses a deep residual convolutional network with instance normalization layers that utilizes an advanced depth prediction network to integrate depth preservation as an additional loss function to content and style. We demonstrate results that are effective in retaining the depth and global structure of content images. Three different evaluation processes show that our system is capable of preserving the structure of the stylized results while exhibiting style-capture capabilities and aesthetic qualities comparable or superior to state-of-the-art methods. Project page: https://ioannoue.github.io/depth-aware-nst-using-in.html.", "venue": "Computer Graphics and Visual Computing (CGVC)", "keywords": ["promising", "style transfer"]}
{"id": "ioffeBatchNormalizationAccelerating2015", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.", "venue": "arXiv", "keywords": ["foundational"]}
{"id": "irpinoNewWassersteinBased2006", "title": "A New Wasserstein Based Distance for the Hierarchical Clustering of Histogram Symbolic Data", "abstract": "", "venue": "Data Science and Classification", "keywords": []}
{"id": "islamRevealingHiddenPatterns2023", "title": "Revealing Hidden Patterns in Deep Neural Network Feature Space Continuum via Manifold Learning", "abstract": "Deep neural networks (DNNs) extract thousands to millions of task-specific features during model training for inference and decision-making. While visualizing these features is critical for comprehending the learning process and improving the performance of the DNNs, existing visualization techniques work only for classification tasks. For regressions, the feature points lie on a high dimensional continuum having an inherently complex shape, making a meaningful visualization of the features intractable. Given that the majority of deep learning applications are regression-oriented, developing a conceptual framework and computational method to reliably visualize the regression features is of great significance. Here, we introduce a manifold discovery and analysis (MDA) method for DNN feature visualization, which involves learning the manifold topology associated with the output and target labels of a DNN. MDA leverages the acquired topological information to preserve the local geometry of the feature space manifold and provides insightful visualizations of the DNN features, highlighting the appropriateness, generalizability, and adversarial robustness of a DNN. The performance and advantages of the MDA approach compared to the existing methods are demonstrated in different deep learning applications.", "venue": "Nature Communications", "keywords": ["clustering", "manifold learning", "topological analysis", "visualizations"]}
{"id": "isolaDiscoveringStatesTransformations2015", "title": "Discovering States and Transformations in Image Collections", "abstract": "Objects in visual scenes come in a rich variety of transformed states. A few classes of transformation have been heavily studied in computer vision: mostly simple, parametric changes in color and geometry. However, transformations in the physical world occur in many more flavors, and they come with semantic meaning: e.g., bending, folding, aging, etc. The transformations an object can undergo tell us about its physical and functional properties. In this paper, we introduce a dataset of objects, scenes, and materials, each of which is found in a variety of transformed states. Given a novel collection of images, we show how to explain the collection in terms of the states and transformations it depicts. Our system works by generalizing across object classes: states and transformations learned on one set of objects are used to interpret the image collection for an entirely new object class.", "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["meta-learning", "prompt-based"]}
{"id": "isolaImagetoImageTranslationConditional2018", "title": "Image-to-Image Translation with Conditional Adversarial Networks", "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.", "venue": "arXiv", "keywords": ["adversarial learning", "foundational", "gans", "image-to-image"]}
{"id": "izmailovAveragingWeightsLeads2019", "title": "Averaging Weights Leads to Wider Optima and Better Generalization", "abstract": "Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.", "venue": "arXiv", "keywords": ["domain generalization"]}
{"id": "jacksonStyleAugmentationData2019", "title": "Style Augmentation: Data Augmentation via Style Randomization", "abstract": "We introduce style augmentation, a new form of data augmentation based on random style transfer, for improving the robustness of Convolutional Neural Networks (CNN) over both classification and regression based tasks. During training, style augmentation randomizes texture, contrast and color, while preserving shape and semantic content. This is accomplished by adapting an arbitrary style transfer network to perform style randomization, by sampling target style embeddings from a multivariate normal distribution instead of computing them from a style image. In addition to standard classification experiments, we investigate the effect of style augmentation (and data augmentation generally) on domain transfer tasks. We find that data augmentation significantly improves robustness to domain shift, and can be used as a simple, domain agnostic alternative to domain adaptation. Comparing style augmentation against a mix of seven traditional augmentation techniques, we find that it can be readily combined with them to improve network performance. We validate the efficacy of our technique with domain transfer experiments in classification and monocular depth estimation illustrating superior performance over benchmark tasks.", "venue": "arXiv", "keywords": ["domain randomization", "style transfer"]}
{"id": "jaegermanSearchProbeableGeneralization2021", "title": "In Search of Probeable Generalization Measures", "abstract": "Understanding the generalization behaviour of deep neural networks is a topic of recent interest that has driven the production of many studies, notably the development and evaluation of generalization \"explainability\" measures that quantify model generalization ability. Generalization measures have also proven useful in the development of powerful layer-wise model tuning and optimization algorithms, though these algorithms require specific kinds of generalization measures which can probe individual layers. The purpose of this paper is to explore the neglected subtopic of probeable generalization measures; to establish firm ground for further investigations, and to inspire and guide the development of novel model tuning and optimization algorithms. We evaluate and compare measures, demonstrating effectiveness and robustness across model variations, dataset complexities, training hyperparameters, and training stages. We also introduce a new dataset of trained models and performance metrics, GenProb, for testing generalization measures, model tuning algorithms and optimization algorithms.", "venue": "arXiv", "keywords": ["generalization quantification", "measure theory", "neural probing"]}
{"id": "jagatapAdversariallyRobustLearning2022", "title": "Adversarially Robust Learning via Entropic Regularization", "abstract": "In this paper we propose a new family of algorithms for training adversarially robust deep neural networks. We formulate a new loss function that uses an entropic regularization. Our loss function considers the contribution of adversarial samples which are drawn from a specially designed distribution that assigns high probability to points with high loss from the immediate neighborhood of training samples. Our data entropy guided SGD approach is designed to search for adversarially robust valleys of the loss landscape. We observe that our approach generalizes better in terms of classification accuracy robustness as compared to state of art approaches based on projected gradient descent.", "venue": "Frontiers in Artificial Intelligence", "keywords": ["adversarial robustness", "entropic optimization", "robustness analysis"]}
{"id": "jangDaDADistortionawareDomain2022", "title": "DaDA: Distortion-aware Domain Adaptation for Unsupervised Semantic Segmentation", "abstract": "", "venue": "Advances in Neural Information Processing Systems", "keywords": ["homography estimation", "promising", "semantic segmentation", "unsupervised da"]}
{"id": "janouskovaSingleImageTestTime", "title": "Single Image Test-Time Adaptation for Segmentation", "abstract": "", "venue": "", "keywords": ["domain adaptation", "self-supervised learning", "surveys", "test-time da"]}
{"id": "jiaMultiScaleExposureFusion2022", "title": "Multi-Scale Exposure Fusion via Content Adaptive Edge-Preserving Smoothing Pyramids", "abstract": "Multi-scale exposure fusion (MEF) is an efficient way to fuse differently exposed low dynamic range (LDR) images of a high dynamic range (HDR) scene into an information enriched LDR image. In this paper, a new MEF algorithm is proposed to merge the differently exposed LDR images by introducing novel content adaptive edge-preserving smoothing (CAS) pyramids for the weight maps of all the LDR images. With the proposed CAS pyramids, details in the brightest and darkest regions of the HDR scene are preserved better than existing MEF algorithms on top of the Gaussian pyramids and edge-preserving smoothing pyramids. Comparisons experimentally demonstrate the effectiveness of the proposed algorithm to nine state-of-theart MEF algorithms from both subjective and objective points of view regardless the image sizes.", "venue": "IEEE Transactions on Consumer Electronics", "keywords": ["feature fusion", "feature pyramids"]}
{"id": "jiangCausalStructuralHypothesis2022", "title": "Causal Structural Hypothesis Testing and Data Generation Models", "abstract": "A vast amount of expert and domain knowledge is captured by causal structural priors, yet there has been little research on testing such priors for generalization and data synthesis purposes. We propose a novel model architecture, Causal Structural Hypothesis Testing, that can use nonparametric, structural causal knowledge and approximate a causal model's functional relationships using deep neural networks. We use these architectures for comparing structural priors, akin to hypothesis testing, using a deliberate (non-random) split of training and testing data. Extensive simulations demonstrate the effectiveness of out-of-distribution generalization error as a proxy for causal structural prior hypothesis testing and offers a statistical baseline for interpreting results. We show that the variational version of the architecture, Causal Structural Variational Hypothesis Testing can improve performance in low SNR regimes. Due to the simplicity and low parameter count of the models, practitioners can test and compare structural prior hypotheses on small dataset and use the priors with the best generalization capacity to synthesize much larger, causally-informed datasets. Finally, we validate our methods on a synthetic pendulum dataset, and show a use-case on a real-world trauma surgery ground-level falls dataset.", "venue": "arXiv", "keywords": []}
{"id": "jiangColorTextureDual2023", "title": "Color and Texture Dual Pipeline Lightweight Style Transfer", "abstract": "Style transfer methods typically generate a single stylized output of color and texture coupling for reference styles, and color transfer schemes may introduce distortion or artifacts when processing reference images with duplicate textures. To solve the problem, we propose a Color and Texture Dual Pipeline Lightweight Style Transfer CTDP method, which employs a dual pipeline method to simultaneously output the results of color and texture transfer. Furthermore, we designed a masked total variation loss to suppress artifacts and small texture representations in color transfer results without affecting the semantic part of the content. More importantly, we are able to add texture structures with controllable intensity to color transfer results for the first time. Finally, we conducted feature visualization analysis on the texture generation mechanism of the framework and found that smoothing the input image can almost completely eliminate this texture structure. In comparative experiments, the color and texture transfer results generated by CTDP both achieve state-of-the-art performance. Additionally, the weight of the color transfer branch model size is as low as 20k, which is 100-1500 times smaller than that of other state-of-the-art models.", "venue": "arXiv", "keywords": ["style transfer"]}
{"id": "jiangDomainGeneralizationBalancing2023", "title": "Domain Generalization via Balancing Training Difficulty and Model Capability", "abstract": "Domain generalization (DG) aims to learn domaingeneralizable models from one or multiple source domains that can perform well in unseen target domains. Despite its recent progress, most existing work suffers from the misalignment between the difficulty level of training samples and the capability of contemporarily trained models, leading to over-fitting or under-fitting in the trained generalization model. We design MoDify, a Momentum Difficulty framework that tackles the misalignment by balancing the seesaw between the model's capability and the samples' difficulties along the training process. MoDify consists of two novel designs that collaborate to fight against the misalignment while learning domain-generalizable models. The first is MoDify-based Data Augmentation which exploits an RGB Shuffle technique to generate difficulty-aware training samples on the fly. The second is MoDify-based Network Optimization which dynamically schedules the training samples for balanced and smooth learning with appropriate difficulty. Without bells and whistles, a simple implementation of MoDify achieves superior performance across multiple benchmarks. In addition, MoDify can complement existing methods as a plug-in, and it is generic and can work for different visual recognition tasks.", "venue": "arXiv", "keywords": ["curriculum learning", "domain generalization", "loss functions", "semantic segmentation"]}
{"id": "jiangDSADeformableSegmentation2023", "title": "DSA: Deformable Segmentation Attention for Multi-Scale Fisheye Image Segmentation", "abstract": "With a larger field of view (FOV) than ordinary images, fisheye images are becoming mainstream in the field of autonomous driving. However, the severe distortion problem of fisheye images also limits its application. The performance of neural networks designed for narrow FOV images degrades drastically for fisheye images, and the use of large composite models can improve the performance, but it brings huge time overhead and hardware costs. Therefore, we decided to balance real time and accuracy by designing the deformable segmentation attention(DSA) module, a generalpurpose architecture based on a deformable attention mechanism and a spatial pyramid architecture. The deformable mechanism serves to accurately extract feature information from fisheye images, together with attention to learn the global context and the spatial pyramid structure to balance multiscale feature information, thus improving the perception of fisheye images by traditional networks without increasing the amount of excessive computation. Lightweight networks such as SegNeXt equipped with the DSA module enable effective and rapid multi-scale segmentation of fisheye images in complex scenes. Our architecture achieves outstanding results on the WoodScape dataset, while our ablation experiments demonstrate the effectiveness of various parts of the architecture.", "venue": "Electronics", "keywords": ["feature pyramids", "fisheye distortion", "semantic segmentation"]}
{"id": "jiangDualPipelineStyle2023", "title": "Dual Pipeline Style Transfer with Input Distribution Differentiation", "abstract": "The color and texture dual pipeline architecture (CTDP) suppresses texture representation and artifacts through masked total variation loss (Mtv), and further experiments have shown that smooth input can almost completely eliminate texture representation. We have demonstrated through experiments that smooth input is not the key reason for removing texture representations, but rather the distribution differentiation of the training dataset. Based on this, we propose an input distribution differentiation training strategy (IDD), which forces the generation of textures to be completely dependent on the noise distribution, while the smooth distribution will not produce textures at all. Overall, our proposed distribution differentiation training strategy allows for two pre-defined input distributions to be responsible for two generation tasks, with noise distribution responsible for texture generation and smooth distribution responsible for color smooth transfer. Finally, we choose a smooth distribution as the input for the forward inference stage to completely eliminate texture representations and artifacts in color transfer tasks.", "venue": "arXiv", "keywords": ["filter augmentations", "masked style transfer", "promising", "siamese networks", "style ind learning", "style transfer", "texture transfer"]}
{"id": "jiangFantasticGeneralizationMeasures2019", "title": "Fantastic Generalization Measures and Where to Find Them", "abstract": "Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.", "venue": "arXiv", "keywords": ["generalization quantification", "highly-analytical", "measure theory", "surveys"]}
{"id": "jiangLightweightTextureTransfer2024", "title": "Lightweight Texture Transfer Based on Texture Feature Preset", "abstract": "In the task of texture transfer, reference texture images typically exhibit highly repetitive texture features, and the texture transfer results from different content images under the same style also share remarkably similar texture patterns. Encoding such highly similar texture features often requires deep layers and a large number of channels, making it is also the main source of the entire model's parameter count and computational load, and inference time. We propose a lightweight texture transfer based on texture feature preset (TFP). TFP takes full advantage of the high repetitiveness of texture features by providing preset universal texture feature maps for a given style. These preset feature maps can be fused and decoded directly with shallow color transfer feature maps of any content to generate texture transfer results, thereby avoiding redundant texture information from being encoded repeatedly. The texture feature map we preset is encoded through noise input images with consistent distribution (standard normal distribution). This consistent input distribution can completely avoid the problem of texture transfer differentiation, and by randomly sampling different noise inputs, we can obtain different texture features and texture transfer results under the same reference style. Compared to state-of-the-art techniques, our TFP not only produces visually superior results but also reduces the model size by 3.2-3538 times and speeds up the process by 1.8-5.6 times.", "venue": "arXiv", "keywords": ["texture transfer"]}
{"id": "jiangRELLIS3DDatasetData2021", "title": "RELLIS-3D Dataset: Data, Benchmarks and Analysis", "abstract": "", "venue": "2021 IEEE International Conference on Robotics and Automation (ICRA)", "keywords": ["dataset debut"]}
{"id": "jiangTSITSimpleVersatile2020", "title": "TSIT: A Simple and Versatile Framework for Image-to-Image Translation", "abstract": "We introduce a simple and versatile framework for image-to-image translation. We unearth the importance of normalization layers, and provide a carefully designed two-stream generative model with newly proposed feature transformations in a coarse-to-fine fashion. This allows multi-scale semantic structure information and style representation to be effectively captured and fused by the network, permitting our method to scale to various tasks in both unsupervised and supervised settings. No additional constraints (e.g., cycle consistency) are needed, contributing to a very clean and simple method. Multi-modal image synthesis with arbitrary style control is made possible. A systematic study compares the proposed method with several state-of-the-art task-specific baselines, verifying its effectiveness in both perceptual quality and quantitative evaluations.", "venue": "arXiv", "keywords": ["adversarial learning", "gans", "image synthesis", "image-to-image", "instance normalization", "siamese networks"]}
{"id": "jiaoCausalInferenceMeets2024", "title": "Causal Inference Meets Deep Learning: A Comprehensive Survey", "abstract": "Deep learning relies on learning from extensive data to generate prediction results. This approach may inadvertently capture spurious correlations within the data, leading to models that lack interpretability and robustness. Researchers have developed more profound and stable causal inference methods based on cognitive neuroscience. By replacing the correlation model with a stable and interpretable causal model, it is possible to mitigate the misleading nature of spurious correlations and overcome the limitations of model calculations. In this survey, we provide a comprehensive and structured review of causal inference methods in deep learning. Brain-like inference ideas are discussed from a brain-inspired perspective, and the basic concepts of causal learning are introduced. The article describes the integration of causal inference with traditional deep learning algorithms and illustrates its application to large model tasks as well as specific modalities in deep learning. The current limitations of causal inference and future research directions are discussed. Moreover, the commonly used benchmark datasets and the corresponding download links are summarized.", "venue": "Research", "keywords": []}
{"id": "jingDynamicInstanceNormalization2019", "title": "Dynamic Instance Normalization for Arbitrary Style Transfer", "abstract": "Prior normalization methods rely on affine transformations to produce arbitrary image style transfers, of which the parameters are computed in a pre-defined way. Such manuallydefined nature eventually results in the high-cost and shared encoders for both style and content encoding, making style transfer systems cumbersome to be deployed in resourceconstrained environments like on the mobile-terminal side. In this paper, we propose a new and generalized normalization module, termed as Dynamic Instance Normalization (DIN), that allows for flexible and more efficient arbitrary style transfers. Comprising an instance normalization and a dynamic convolution, DIN encodes a style image into learnable convolution parameters, upon which the content image is stylized. Unlike conventional methods that use shared complex encoders to encode content and style, the proposed DIN introduces a sophisticated style encoder, yet comes with a compact and lightweight content encoder for fast inference. Experimental results demonstrate that the proposed approach yields very encouraging results on challenging style patterns and, to our best knowledge, for the first time enables an arbitrary style transfer using MobileNet-based lightweight architecture, leading to a reduction factor of more than twenty in computational cost as compared to existing approaches. Furthermore, the proposed DIN provides flexible support for stateof-the-art convolutional operations, and thus triggers novel functionalities, such as uniform-stroke placement for nonnatural images and automatic spatial-stroke control.", "venue": "arXiv", "keywords": ["instance normalization", "style transfer"]}
{"id": "jingNeuralStyleTransfer2018", "title": "Neural Style Transfer: A Review", "abstract": "The seminal work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNNs) in creating artistic imagery by separating and recombining image content and style. This process of using CNNs to render a content image in different styles is referred to as Neural Style Transfer (NST). Since then, NST has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention and a variety of approaches are proposed to either improve or extend the original NST algorithm. In this paper, we aim to provide a comprehensive overview of the current progress towards NST. We first propose a taxonomy of current algorithms in the field of NST. Then, we present several evaluation methods and compare different NST algorithms both qualitatively and quantitatively. The review concludes with a discussion of various applications of NST and open problems for future research. A list of papers discussed in this review, corresponding codes, pre-trained models and more comparison results are publicly available at: https://github.com/ycjing/Neural-Style-Transfer-Papers.", "venue": "arXiv", "keywords": ["style transfer", "surveys"]}
{"id": "jingNeuralStyleTransfer2018a", "title": "Neural Style Transfer: A Review", "abstract": "The seminal work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNNs) in creating artistic imagery by separating and recombining image content and style. This process of using CNNs to render a content image in different styles is referred to as Neural Style Transfer (NST). Since then, NST has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention and a variety of approaches are proposed to either improve or extend the original NST algorithm. In this paper, we aim to provide a comprehensive overview of the current progress towards NST. We first propose a taxonomy of current algorithms in the field of NST. Then, we present several evaluation methods and compare different NST algorithms both qualitatively and quantitatively. The review concludes with a discussion of various applications of NST and open problems for future research. A list of papers discussed in this review, corresponding codes, pre-trained models and more comparison results are publicly available at https://github.com/ycjing/Neural-Style-Transfer-Papers.", "venue": "arXiv", "keywords": []}
{"id": "johnsonPerceptualLossesRealTime2016", "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution", "abstract": "We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.", "venue": "arXiv", "keywords": ["foundational", "loss functions", "style transfer", "super-resolution"]}
{"id": "jungArbitraryStyleTransfer2020", "title": "Arbitrary Style Transfer Using Graph Instance Normalization", "abstract": "Style transfer is the image synthesis task, which applies a style of one image to another while preserving the content. In statistical methods, the adaptive instance normalization (AdaIN) whitens the source images and applies the style of target images through normalizing the mean and variance of features. However, computing feature statistics for each instance would neglect the inherent relationship between features, so it is hard to learn global styles while fitting to the individual training dataset. In this paper, we present a novel learnable normalization technique for style transfer using graph convolutional networks, termed Graph Instance Normalization (GrIN). This algorithm makes the style transfer approach more robust by taking into account similar information shared between instances. Besides, this simple module is also applicable to other tasks like image-to-image translation or domain adaptation.", "venue": "arXiv", "keywords": ["graph ml", "instance normalization", "style transfer"]}
{"id": "jungSpectralDistributionAware2021", "title": "Spectral Distribution Aware Image Generation", "abstract": "Recent advances in deep generative models for photo-realistic images have led to high quality visual results. Such models learn to generate data from a given training distribution such that generated images can not be easily distinguished from real images by the human eye. Yet, recent work on the detection of such fake images pointed out that they are actually easily distinguishable by artifacts in their frequency spectra. In this paper, we propose to generate images according to the frequency distribution of the real data by employing a spectral discriminator. The proposed discriminator is lightweight, modular and works stably with different commonly used GAN losses. We show that the resulting models can better generate images with realistic frequency spectra, which are thus harder to detect by this cue.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "keywords": ["image synthesis", "spectral methods"]}
{"id": "kakaniAutomaticDistortionRectification2020", "title": "Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks", "abstract": "The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.", "venue": "Sensors", "keywords": ["homography estimation"]}
{"id": "kalbPrinciplesForgettingDomainIncremental2023", "title": "Principles of Forgetting in Domain-Incremental Semantic Segmentation in Adverse Weather Conditions", "abstract": "Deep neural networks for scene perception in automated vehicles achieve excellent results for the domains they were trained on. However, in real-world conditions, the domain of operation and its underlying data distribution are subject to change. Adverse weather conditions, in particular, can significantly decrease model performance when such data are not available during training. Additionally, when a model is incrementally adapted to a new domain, it suffers from catastrophic forgetting, causing a significant drop in performance on previously observed domains. Despite recent progress in reducing catastrophic forgetting, its causes and effects remain obscure. Therefore, we study how the representations of semantic segmentation models are affected during domain-incremental learning in adverse weather conditions. Our experiments and representational analyses indicate that catastrophic forgetting is primarily caused by changes to low-level features in domain-incremental learning and that learning more general features on the source domain using pre-training and image augmentations leads to efficient feature reuse in subsequent tasks, which drastically reduces catastrophic forgetting. These findings highlight the importance of methods that facilitate generalized features for effective continual learning algorithms.", "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["adverse weather", "bias sources", "continual da", "promising", "segformer", "semantic segmentation"]}
{"id": "kalischekBiasBedRigorousTexture2023", "title": "BiasBed -- Rigorous Texture Bias Evaluation", "abstract": "The well-documented presence of texture bias in modern convolutional neural networks has led to a plethora of algorithms that promote an emphasis on shape cues, often to support generalization to new domains. Yet, common datasets, benchmarks and general model selection strategies are missing, and there is no agreed, rigorous evaluation protocol. In this paper, we investigate difficulties and limitations when training networks with reduced texture bias. In particular, we also show that proper evaluation and meaningful comparisons between methods are not trivial. We introduce BiasBed, a testbed for texture- and style-biased training, including multiple datasets and a range of existing algorithms. It comes with an extensive evaluation protocol that includes rigorous hypothesis testing to gauge the significance of the results, despite the considerable training instability of some style bias methods. Our extensive experiments, shed new light on the need for careful, statistically founded evaluation protocols for style bias (and beyond). E.g., we find that some algorithms proposed in the literature do not significantly mitigate the impact of style bias at all. With the release of BiasBed, we hope to foster a common understanding of consistent and meaningful comparisons, and consequently faster progress towards learning methods free of texture bias. Code is available at https://github.com/D1noFuzi/BiasBed", "venue": "arXiv", "keywords": []}
{"id": "kamallooWhenChosenWisely2022", "title": "When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation", "abstract": "Data Augmentation (DA) is known to improve the generalizability of deep neural networks. Most existing DA techniques naively add a certain number of augmented samples without considering the quality and the added computational cost of these samples. To tackle this problem, a common strategy, adopted by several state-of-the-art DA methods, is to adaptively generate or re-weight augmented samples with respect to the task objective during training. However, these adaptive DA methods: (1) are computationally expensive and not sample-efficient, and (2) are designed merely for a specific setting. In this work, we present a universal DA technique, called Glitter, to overcome both issues. Glitter can be plugged into any DA method, making training sample-efficient without sacrificing performance. From a pre-generated pool of augmented samples, Glitter adaptively selects a subset of worst-case samples with maximal loss, analogous to adversarial DA. Without altering the training strategy, the task objective can be optimized on the selected subset. Our thorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three widely used training setups including consistency training, self-distillation and knowledge distillation reveal that Glitter is substantially faster to train and achieves a competitive performance, compared to strong baselines.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "bias sources"]}
{"id": "kamannBenchmarkingRobustnessSemantic2020", "title": "Benchmarking the Robustness of Semantic Segmentation Models", "abstract": "When designing a semantic segmentation module for a practical application, such as autonomous driving, it is crucial to understand the robustness of the module with respect to a wide range of image corruptions. While there are recent robustness studies for full-image classification, we are the first to present an exhaustive study for semantic segmentation, based on the state-of-the-art model DeepLabv3+. To increase the realism of our study, we utilize almost 400,000 images generated from Cityscapes, PASCAL VOC 2012, and ADE20K. Based on the benchmark study, we gain several new insights. Firstly, contrary to full-image classification, model robustness increases with model performance, in most cases. Secondly, some architecture properties affect robustness significantly, such as a Dense Prediction Cell, which was designed to maximize performance on clean data only.", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["adversarial robustness", "adverse weather", "augmentation stability", "domain generalization", "robustness analysis", "semantic segmentation"]}
{"id": "kamoiWhyMahalanobisDistance2020", "title": "Why Is the Mahalanobis Distance Effective for Anomaly Detection?", "abstract": "The Mahalanobis distance-based confidence score, a recently proposed anomaly detection method for pre-trained neural classifiers, achieves state-of-the-art performance on both out-of-distribution (OoD) and adversarial examples detection. This work analyzes why this method exhibits such strong performance in practical settings while imposing an implausible assumption; namely, that class conditional distributions of pre-trained features have tied covariance. Although the Mahalanobis distance-based method is claimed to be motivated by classification prediction confidence, we find that its superior performance stems from information not useful for classification. This suggests that the reason the Mahalanobis confidence score works so well is mistaken, and makes use of different information from ODIN, another popular OoD detection method based on prediction confidence. This perspective motivates us to combine these two methods, and the combined detector exhibits improved performance and robustness. These findings provide insight into the behavior of neural classifiers in response to anomalous inputs.", "venue": "arXiv", "keywords": ["anomaly detection", "ood detection"]}
{"id": "karanicolasTooLongDidnt2021", "title": "Too Long; Didn't Read: Finding Meaning in Platforms' Terms of Service Agreements", "abstract": "", "venue": "SSRN Electronic Journal", "keywords": []}
{"id": "karlssonfaroniusSolvingPartialDifferential2023", "title": "Solving Partial Differential Equations With Neural Networks", "abstract": "DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.", "venue": "", "keywords": []}
{"id": "karthikCompleteIoU2020", "title": "Complete IoU", "abstract": "The loss functions are the major driving force in training a good model. In Object detection and Instance segmentation tasks, the most widely used loss function is Intersection over Union (IOU). In Enhancing Geometric Factors for Object Detection and Instance Segmentation Loss function. paper, a new loss function called as Complete Intersection over Union is proposed by considering three geometric factors.", "venue": "Karthik", "keywords": []}
{"id": "kaushikUnderstandingCatastrophicForgetting2021", "title": "Understanding Catastrophic Forgetting and Remembering in Continual Learning with Optimal Relevance Mapping", "abstract": "Catastrophic forgetting in neural networks is a significant problem for continual learning. A majority of the current methods replay previous data during training, which violates the constraints of an ideal continual learning system. Additionally, current approaches that deal with forgetting ignore the problem of catastrophic remembering, i.e. the worsening ability to discriminate between data from different tasks. In our work, we introduce Relevance Mapping Networks (RMNs) which are inspired by the Optimal Overlap Hypothesis. The mappings reflects the relevance of the weights for the task at hand by assigning large weights to essential parameters. We show that RMNs learn an optimized representational overlap that overcomes the twin problem of catastrophic forgetting and remembering. Our approach achieves state-of-the-art performance across all common continual learning datasets, even significantly outperforming data replay methods while not violating the constraints for an ideal continual learning system. Moreover, RMNs retain the ability to detect data from new tasks in an unsupervised manner, thus proving their resilience against catastrophic remembering.", "venue": "arXiv", "keywords": ["bias sources"]}
{"id": "keAdaptiveAffinityFields2018", "title": "Adaptive Affinity Fields for Semantic Segmentation", "abstract": "Semantic segmentation has made much progress with increasingly powerful pixel-wise classifiers and incorporating structural priors via Conditional Random Fields (CRF) or Generative Adversarial Networks (GAN). We propose a simpler alternative that learns to verify the spatial structure of segmentation during training only. Unlike existing approaches that enforce semantic labels on individual pixels and match labels between neighbouring pixels, we propose the concept of Adaptive Affinity Fields (AAF) to capture and match the semantic relations between neighbouring pixels in the label space. We use adversarial learning to select the optimal affinity field size for each semantic category. It is formulated as a minimax problem, optimizing our segmentation neural network in a best worst-case learning scenario. AAF is versatile for representing structures as a collection of pixel-centric relations, easier to train than GAN and more efficient than CRF without run-time inference. Our extensive evaluations on PASCAL VOC 2012, Cityscapes, and GTA5 datasets demonstrate its above-par segmentation performance and robust generalization across domains.", "venue": "Computer Vision -- ECCV 2018", "keywords": []}
{"id": "keAdaptiveAffinityFields2018a", "title": "Adaptive Affinity Fields for Semantic Segmentation", "abstract": "Semantic segmentation has made much progress with increasingly powerful pixel-wise classifiers and incorporating structural priors via Conditional Random Fields (CRF) or Generative Adversarial Networks (GAN). We propose a simpler alternative that learns to verify the spatial structure of segmentation during training only. Unlike existing approaches that enforce semantic labels on individual pixels and match labels between neighbouring pixels, we propose the concept of Adaptive Affinity Fields (AAF) to capture and match the semantic relations between neighbouring pixels in the label space. We use adversarial learning to select the optimal affinity field size for each semantic category. It is formulated as a minimax problem, optimizing our segmentation neural network in a best worst-case learning scenario. AAF is versatile for representing structures as a collection of pixel-centric relations, easier to train than GAN and more efficient than CRF without run-time inference. Our extensive evaluations on PASCAL VOC 2012, Cityscapes, and GTA5 datasets demonstrate its above-par segmentation performance and robust generalization across domains.", "venue": "arXiv", "keywords": ["affinity modeling", "loss functions", "promising", "semantic segmentation"]}
{"id": "keExactInferenceLatent2020", "title": "Exact Inference with Latent Variables in an Arbitrary Domain", "abstract": "We analyze the necessary and sufficient conditions for exact inference of a latent model. In latent models, each entity is associated with a latent variable following some probability distribution. The challenging question we try to solve is: can we perform exact inference without observing the latent variables, even without knowing what the domain of the latent variables is? We show that exact inference can be achieved using a semidefinite programming (SDP) approach without knowing either the latent variables or their domain. Our analysis predicts the experimental correctness of SDP with high accuracy, showing the suitability of our focus on the Karush-Kuhn-Tucker (KKT) conditions and the spectrum of a properly defined matrix. As a byproduct of our analysis, we also provide concentration inequalities with dependence on latent variables, both for bounded moment generating functions as well as for the spectra of matrices. To the best of our knowledge, these results are novel and could be useful for many other problems.", "venue": "arXiv", "keywords": ["distribution estimation", "feature engineering", "highly-analytical"]}
{"id": "keskarLargeBatchTrainingDeep2017", "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \\ -\\ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.", "venue": "arXiv", "keywords": ["domain generalization", "generalization quantification"]}
{"id": "kessyOptimalWhiteningDecorrelation2018", "title": "Optimal Whitening and Decorrelation", "abstract": "Whitening, or sphering, is a common preprocessing step in statistical analysis to transform random variables to orthogonality. However, due to rotational freedom there are infinitely many possible whitening procedures. Consequently, there is a diverse range of sphering methods in use, for example based on principal component analysis (PCA), Cholesky matrix decomposition and zero-phase component analysis (ZCA), among others. Here we provide an overview of the underlying theory and discuss five natural whitening procedures. Subsequently, we demonstrate that investigating the cross-covariance and the cross-correlation matrix between sphered and original variables allows to break the rotational invariance and to identify optimal whitening transformations. As a result we recommend two particular approaches: ZCA-cor whitening to produce sphered variables that are maximally similar to the original variables, and PCA-cor whitening to obtain sphered variables that maximally compress the original variables.", "venue": "The American Statistician", "keywords": ["feature whitening", "feature-level augmentation", "highly-analytical"]}
{"id": "khoeeDomainGeneralizationMetalearning2024", "title": "Domain Generalization through Meta-Learning: A Survey", "abstract": "Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Metalearning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the concept of meta-learning for domain generalization and introduce a novel taxonomy based on the feature extraction strategy and the classifier learning methodology, offering a granular view of methodologies. Additionally, we present a decision graph to assist readers in navigating the taxonomy based on data availability and domain shifts, enabling them to select and develop a proper model tailored to their specific problem requirements. Through an exhaustive review of existing methods and underlying theories, we map out the fundamentals of the field. Our survey provides practical insights and an informed discussion on promising research directions.", "venue": "Artificial Intelligence Review", "keywords": ["domain generalization", "meta-learning", "surveys"]}
{"id": "khoslaEnhancingPerformanceDeep2020", "title": "Enhancing Performance of Deep Learning Models with Different Data Augmentation Techniques: A Survey", "abstract": "Deep convolutional neural networks have shown impressive results on different computer vision tasks. Nowadays machines are fed by new artificial intelligence techniques which makes them intelligent enough to cognize the visual world better than humans. These computer vision algorithms rely heavily on large data sets. Having a large training data set plays a very crucial role in the performance of deep convolutional neural networks. We can enhance the performance of the model by augmenting the data of the image. Data augmentation is a set of techniques that are used to increase the size and quality of the image with label preserving transformations. This survey paper focuses on different data augmentation techniques based on data warping and oversampling. In addition to data augmentation techniques, this paper gives a brief discussion on different solutions of reducing the overfitting problem.", "venue": "2020 International Conference on Intelligent Engineering and Management (ICIEM)", "keywords": ["gans", "surveys", "traditional augmentation"]}
{"id": "khoslaUndoingDamageDataset2012", "title": "Undoing the Damage of Dataset Bias", "abstract": "", "venue": "Computer Vision -- ECCV 2012", "keywords": ["bias sources", "domain generalization"]}
{"id": "kienzleSegformerEfficientTokenMerging2024", "title": "Segformer++: Efficient Token-Merging Strategies for High-Resolution Semantic Segmentation", "abstract": "Utilizing transformer architectures for semantic segmentation of high-resolution images is hindered by the attention's quadratic computational complexity in the number of tokens. A solution to this challenge involves decreasing the number of tokens through token merging, which has exhibited remarkable enhancements in inference speed, training efficiency, and memory utilization for image classification tasks. In this paper, we explore various token merging strategies within the framework of the Segformer architecture and perform experiments on multiple semantic segmentation and human pose estimation datasets. Notably, without model re-training, we, for example, achieve an inference acceleration of 61% on the Cityscapes dataset while maintaining the mIoU performance. Consequently, this paper facilitates the deployment of transformer-based architectures on resource-constrained devices and in real-time applications.", "venue": "arXiv", "keywords": ["segformer", "semantic segmentation", "transformers"]}
{"id": "kimLearningTextureInvariant2020", "title": "Learning Texture Invariant Representation for Domain Adaptation of Semantic Segmentation", "abstract": "Since annotating pixel-level labels for semantic segmentation is laborious, leveraging synthetic data is an attractive solution. However, due to the domain gap between synthetic domain and real domain, it is challenging for a model trained with synthetic data to generalize to real data. In this paper, considering the fundamental difference between the two domains as the texture, we propose a method to adapt to the texture of the target domain. First, we diversity the texture of synthetic images using a style transfer algorithm. The various textures of generated images prevent a segmentation model from overfitting to one specific (synthetic) texture. Then, we fine-tune the model with self-training to get direct supervision of the target texture. Our results achieve state-of-the-art performance and we analyze the properties of the model trained on the stylized dataset with extensive experiments.", "venue": "arXiv", "keywords": ["domain adaptation", "representation learning", "semantic segmentation", "texture transfer"]}
{"id": "kimLipschitzConstantSelfAttention2021", "title": "The Lipschitz Constant of Self-Attention", "abstract": "Lipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks. Such works have focused on bounding the Lipschitz constant of fully connected or convolutional networks, composed of linear maps and pointwise non-linearities. In this paper, we investigate the Lipschitz constant of self-attention, a non-linear neural network module widely used in sequence modelling. We prove that the standard dot-product self-attention is not Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that is Lipschitz. We derive an upper bound on the Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. To demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language modelling task.", "venue": "Proceedings of the 38th International Conference on Machine Learning", "keywords": ["lipschitz-constraints"]}
{"id": "kimStabilityPlasticityDilemmaClassIncremental2023", "title": "On the Stability-Plasticity Dilemma of Class-Incremental Learning", "abstract": "A primary goal of class-incremental learning is to strike a balance between stability and plasticity, where models should be both stable enough to retain knowledge learned from previously seen classes, and plastic enough to learn concepts from new classes. While previous works demonstrate strong performance on class-incremental benchmarks, it is not clear whether their success comes from the models being stable, plastic, or a mixture of both. This paper aims to shed light on how effectively recent class-incremental learning algorithms address the stability-plasticity trade-off. We establish analytical tools that measure the stability and plasticity of feature representations, and employ such tools to investigate models trained with various algorithms on large-scale class-incremental benchmarks. Surprisingly, we find that the majority of class-incremental learning algorithms heavily favor stability over plasticity, to the extent that the feature extractor of a model trained on the initial set of classes is no less effective than that of the final incremental model. Our observations not only inspire two simple algorithms that highlight the importance of feature representation analysis, but also suggest that class-incremental learning approaches, in general, should strive for better feature representation learning.", "venue": "arXiv", "keywords": []}
{"id": "kimStatisticalInferenceCluster2017", "title": "Statistical Inference for Cluster Trees", "abstract": "A cluster tree provides a highly-interpretable summary of a density function by representing the hierarchy of its high-density clusters. It is estimated using the empirical tree, which is the cluster tree constructed from a density estimator. This paper addresses the basic question of quantifying our uncertainty by assessing the statistical significance of topological features of an empirical cluster tree. We first study a variety of metrics that can be used to compare different trees, analyze their properties and assess their suitability for inference. We then propose methods to construct and summarize confidence sets for the unknown true cluster tree. We introduce a partial ordering on cluster trees which we use to prune some of the statistically insignificant features of the empirical tree, yielding interpretable and parsimonious cluster trees. Finally, we illustrate the proposed methods on a variety of synthetic examples and furthermore demonstrate their utility in the analysis of a Graft-versus-Host Disease (GvHD) data set.", "venue": "arXiv", "keywords": []}
{"id": "kimTextureLearningDomain2023", "title": "Texture Learning Domain Randomization for Domain Generalized Segmentation", "abstract": "Deep Neural Networks (DNNs)-based semantic segmentation models trained on a source domain often struggle to generalize to unseen target domains, i.e., a domain gap problem. Texture often contributes to the domain gap, making DNNs vulnerable to domain shift because they are prone to be texture-biased. Existing Domain Generalized Semantic Segmentation (DGSS) methods have alleviated the domain gap problem by guiding models to prioritize shape over texture. On the other hand, shape and texture are two prominent and complementary cues in semantic segmentation. This paper argues that leveraging texture is crucial for improving performance in DGSS. Specifically, we propose a novel framework, coined Texture Learning Domain Randomization (TLDR). TLDR includes two novel losses to effectively enhance texture learning in DGSS: (1) a texture regularization loss to prevent overfitting to source domain textures by using texture features from an ImageNet pretrained model and (2) a texture generalization loss that utilizes random style images to learn diverse texture representations in a self-supervised manner. Extensive experimental results demonstrate the superiority of the proposed TLDR; e.g., TLDR achieves 46.5 mIoU on GTA Cityscapes using ResNet-50, which improves the prior state-of-the-art method by 1.9 mIoU. The source code is available at https://github.com/ssssshwan/TLDR.", "venue": "arXiv", "keywords": ["domain randomization", "texture transfer"]}
{"id": "kimTransportBasedNeuralStyle2019", "title": "Transport-Based Neural Style Transfer for Smoke Simulations", "abstract": "Artistically controlling fluids has always been a challenging task. Optimization techniques rely on approximating simulation states towards target velocity or density field configurations, which are often handcrafted by artists to indirectly control smoke dynamics. Patch synthesis techniques transfer image textures or simulation features to a target flow field. However, these are either limited to adding structural patterns or augmenting coarse flows with turbulent structures, and hence cannot capture the full spectrum of different styles and semantically complex structures. In this paper, we propose the first Transport-based Neural Style Transfer (TNST) algorithm for volumetric smoke data. Our method is able to transfer features from natural images to smoke simulations, enabling general content-aware manipulations ranging from simple patterns to intricate motifs. The proposed algorithm is physically inspired, since it computes the density transport from a source input smoke to a desired target configuration. Our transport-based approach allows direct control over the divergence of the stylization velocity field by optimizing incompressible and irrotational potentials that transport smoke towards stylization. Temporal consistency is ensured by transporting and aligning subsequent stylized velocities, and 3D reconstructions are computed by seamlessly merging stylizations from different camera viewpoints.", "venue": "ACM Transactions on Graphics", "keywords": ["optimal transport"]}
{"id": "kimVisionTransformersNatural2024", "title": "Vision Transformers with Natural Language Semantics", "abstract": "Tokens or patches within Vision Transformers (ViT) lack essential semantic information, unlike their counterparts in natural language processing (NLP). Typically, ViT tokens are associated with rectangular image patches that lack specific semantic context, making interpretation difficult and failing to effectively encapsulate information. We introduce a novel transformer model, Semantic Vision Transformers (sViT), which leverages recent progress on segmentation models to design novel tokenizer strategies. sViT effectively harnesses semantic information, creating an inductive bias reminiscent of convolutional neural networks while capturing global dependencies and contextual information within images that are characteristic of transformers. Through validation using real datasets, sViT demonstrates superiority over ViT, requiring less training data while maintaining similar or superior performance. Furthermore, sViT demonstrates significant superiority in out-of-distribution generalization and robustness to natural distribution shifts, attributed to its scale invariance semantic characteristic. Notably, the use of semantic tokens significantly enhances the model's interpretability. Lastly, the proposed paradigm facilitates the introduction of new and powerful augmentation techniques at the token (or segment) level, increasing training data diversity and generalization capabilities. Just as sentences are made of words, images are formed by semantic objects; our proposed methodology leverages recent progress in object segmentation and takes an important and natural step toward interpretable and robust vision transformers.", "venue": "arXiv", "keywords": ["domain generalization", "semantic tokenization", "transformers"]}
{"id": "kimWEDGEWebImageAssisted2023", "title": "WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation", "abstract": "Domain generalization for semantic segmentation is highly demanded in real applications, where a trained model is expected to work well in previously unseen domains. One challenge lies in the lack of data which could cover the diverse distributions of the possible unseen domains for training. In this paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme, which is the first to exploit the diversity of web-crawled images for generalizable semantic segmentation. To explore and exploit the real-world data distributions, we collect web-crawled images which present large diversity in terms of weather conditions, sites, lighting, camera styles, etc. We also present a method which injects styles of the webcrawled images into training images on-the-fly during training, which enables the network to experience images of diverse styles with reliable labels for effective training. Moreover, we use the web-crawled images with their predicted pseudo labels for training to further enhance the capability of the network. Extensive experiments demonstrate that our method clearly outperforms existing domain generalization techniques.", "venue": "arXiv", "keywords": ["self-training", "semantic segmentation", "semi-supervised learning", "style transfer"]}
{"id": "kingmaGlowGenerativeFlow2018", "title": "Glow: Generative Flow with Invertible 1x1 Convolutions", "abstract": "Flow-based generative models are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood and qualitative sample quality. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient synthesis of large and subjectively realistic-looking images.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["image synthesis", "normalizing flows"]}
{"id": "kingmaGlowGenerativeFlow2018a", "title": "Glow: Generative Flow with Invertible 1x1 Convolutions", "abstract": "Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow", "venue": "arXiv", "keywords": ["image synthesis", "normalizing flows"]}
{"id": "kingmaImprovingVariationalInference2017", "title": "Improving Variational Inference with Inverse Autoregressive Flow", "abstract": "The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.", "venue": "arXiv", "keywords": ["autoregressive flows", "distribution estimation", "vaes", "variational inference"]}
{"id": "kingmaImprovingVariationalInference2017a", "title": "Improving Variational Inference with Inverse Autoregressive Flow", "abstract": "The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.", "venue": "arXiv", "keywords": []}
{"id": "kirchheimPyTorchOODLibraryOutofDistribution2022", "title": "PyTorch-OOD: A Library for Out-of-Distribution Detection Based on PyTorch", "abstract": "Machine Learning models based on Deep Neural Networks behave unpredictably when presented with inputs that do not stem from the training distribution and sometimes make egregiously wrong predictions with high confidence. This property undermines the trustworthiness of systems depending on such models and potentially threatens the safety of their users. Out-of-Distribution (OOD) detection mechanisms can be used to prevent errors by detecting inputs that are so dissimilar from the training set that the model can not be expected to make reliable predictions. In this paper, we present PyTorch-OOD, a Python library for OOD detection based on PyTorch. Its primary goals are to accelerate OOD detection research and improve the reproducibility and comparability of experiments. PyTorch-OOD provides well-tested and documented implementations of OOD detection methods with a unified interface, as well as training and benchmark datasets, architectures, pre-trained models, and utility functions. The library is available online1 under the permissive Apache 2.0 license and can be installed via Python Package Index (PyPI).", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "keywords": []}
{"id": "kitaevReformerEfficientTransformer2020", "title": "Reformer: The Efficient Transformer", "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\\ ) to O(\\ ), where \\ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \\ times, where \\ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.", "venue": "arXiv", "keywords": ["transformers"]}
{"id": "kitovDepthAwareArbitraryStyle2020", "title": "Depth-Aware Arbitrary Style Transfer Using Instance Normalization", "abstract": "Style transfer is the process of rendering one image with some content in the style of another image, representing the style. Recent studies of Liu et al. (2017) show that traditional style transfer methods of Gatys et al. (2016) and Johnson et al. (2016) fail to reproduce the depth of the content image, which is critical for human perception. They suggest to preserve the depth map by additional regularizer in the optimized loss function, forcing preservation of the depth map. However these traditional methods are either computationally inefficient or require training a separate neural network for each style. AdaIN method of Huang et al. (2017) allows efficient transferring of arbitrary style without training a separate model but is not able to reproduce the depth map of the content image. We propose an extension to this method, allowing depth map preservation by applying variable stylization strength. Qualitative analysis and results of user evaluation study indicate that the proposed method provides better stylizations, compared to the original AdaIN style transfer method.", "venue": "arXiv", "keywords": ["instance normalization", "style transfer"]}
{"id": "kitovRealTimeStyleTransfer2019", "title": "Real-Time Style Transfer With Strength Control", "abstract": "Style transfer is a problem of rendering a content image in the style of another style image. A natural and common practical task in applications of style transfer is to adjust the strength of stylization. Algorithm of Gatys et al. (2016) provides this ability by changing the weighting factors of content and style losses but is computationally inefficient. Real-time style transfer introduced by Johnson et al. (2016) enables fast stylization of any image by passing it through a pre-trained transformer network. Although fast, this architecture is not able to continuously adjust style strength. We propose an extension to real-time style transfer that allows direct control of style strength at inference, still requiring only a single transformer network. We conduct qualitative and quantitative experiments that demonstrate that the proposed method is capable of smooth stylization strength control and removes certain stylization artifacts appearing in the original real-time style transfer method. Comparisons with alternative real-time style transfer algorithms, capable of adjusting stylization strength, show that our method reproduces style with more details.", "venue": "arXiv", "keywords": ["style transfer"]}
{"id": "kobyzevNormalizingFlowsIntroduction2021", "title": "Normalizing Flows: An Introduction and Review of Current Methods", "abstract": "Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "keywords": ["distribution estimation", "manifold learning", "normalizing flows", "surveys", "variational inference"]}
{"id": "koehlerRepresentationalAspectsDepth2021", "title": "Representational Aspects of Depth and Conditioning in Normalizing Flows", "abstract": "Normalizing flows are among the most popular paradigms in generative modeling, especially for images, primarily because we can efficiently evaluate the likelihood of a data point. This is desirable both for evaluating the fit of a model, and for ease of training, as maximizing the likelihood can be done by gradient descent. However, training normalizing flows comes with difficulties as well: models which produce good samples typically need to be extremely deep -- which comes with accompanying vanishing/exploding gradient problems. A very related problem is that they are often poorly \\: since they are parametrized as invertible maps from \\ , and typical training data like images intuitively is lower-dimensional, the learned maps often have Jacobians that are close to being singular. In our paper, we tackle representational aspects around depth and conditioning of normalizing flows: both for general invertible architectures, and for a particular common architecture, affine couplings. We prove that \\ affine coupling layers suffice to exactly represent a permutation or \\ convolution, as used in GLOW, showing that representationally the choice of partition is not a bottleneck for depth. We also show that shallow affine coupling networks are universal approximators in Wasserstein distance if ill-conditioning is allowed, and experimentally investigate related phenomena involving padding. Finally, we show a depth lower bound for general flow architectures with few neurons per layer and bounded Lipschitz constant.", "venue": "Proceedings of the 38th International Conference on Machine Learning", "keywords": ["lipschitz-constraints", "normalizing flows"]}
{"id": "koenigKANODEsKolmogorovArnoldNetwork2024", "title": "KAN-ODEs: Kolmogorov-Arnold Network Ordinary Differential Equations for Learning Dynamical Systems and Hidden Physics", "abstract": "Kolmogorov-Arnold networks (KANs) as an alternative to multi-layer perceptrons (MLPs) are a recent development demonstrating strong potential for data-driven modeling. This work applies KANs as the backbone of a neural ordinary differential equation (ODE) framework, generalizing their use to the time-dependent and temporal grid-sensitive cases often seen in dynamical systems and scientific machine learning applications. The proposed KAN-ODEs retain the flexible dynamical system modeling framework of Neural ODEs while leveraging the many benefits of KANs compared to MLPs, including higher accuracy and faster neural scaling, stronger interpretability and generalizability, and lower parameter counts. First, we quantitatively demonstrated these improvements in a comprehensive study of the classical Lotka-Volterra predator-prey model. We then showcased the KAN-ODE framework's ability to learn symbolic source terms and complete solution profiles in higher-complexity and data-lean scenarios including wave propagation and shock formation, the complex Schr \"odinger equation, and the Allen-Cahn phase separation equation. The successful training of KAN-ODEs, and their improved performance compared to traditional Neural ODEs, implies significant potential in leveraging this novel network architecture in myriad scientific machine learning applications for discovering hidden physics and predicting dynamic evolution.", "venue": "Computer Methods in Applied Mechanics and Engineering", "keywords": []}
{"id": "kohWILDSBenchmarkIntheWild2021", "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts", "abstract": "Distribution shifts -- where the training distribution differs from the test distribution -- can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu.", "venue": "arXiv", "keywords": ["dataset debut", "domain generalization"]}
{"id": "koksalRFGANLightReconfigurable2021", "title": "RF-GAN: A Light and Reconfigurable Network for Unpaired Image-to-Image Translation", "abstract": "Generative adversarial networks (GANs) have been widely studied for unpaired image-to-image translation in recent years. On the other hand, state-of-the-art translation GANs are often constrained by large model sizes and inflexibility in translating across various domains. Inspired by the observation that the mappings between two domains are often approximately invertible, we design an innovative reconfigurable GAN (RF-GAN) that has a small size but is versatile in high-fidelity image translation either across two domains or among multiple domains. One unique feature of RF-GAN lies with its single generator which is reconfigurable and can perform bidirectional image translations by swapping its parameters. In addition, a multi-domain discriminator is designed which allows joint discrimination of original and translated samples in multiple domains. Experiments over eight unpaired image translation datasets (on various tasks such as object transfiguration, season transfer, and painters' style transfer, etc.) show that RF-GAN reduces the model size by up to 75% as compared with state-of-the-art translation GANs but produces superior image translation performance with lower Fr Inception Distance consistently.", "venue": "Computer Vision -- ACCV 2020", "keywords": ["adversarial learning", "gans", "image-to-image"]}
{"id": "kolkinNeuralNeighborStyle2022", "title": "Neural Neighbor Style Transfer", "abstract": "We propose Neural Neighbor Style Transfer (NNST), a pipeline that offers state-of-the-art quality, generalization, and competitive efficiency for artistic style transfer. Our approach is based on explicitly replacing neural features extracted from the content input (to be stylized) with those from a style exemplar, then synthesizing the final output based on these rearranged features. While the spirit of our approach is similar to prior work, we show that our design decisions dramatically improve the final visual quality.", "venue": "arXiv", "keywords": ["cnns", "gans", "style transfer"]}
{"id": "kolkinStyleTransferRelaxed2019", "title": "Style Transfer by Relaxed Optimal Transport and Self-Similarity", "abstract": "", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": ["favorite", "optimal transport", "promising", "style transfer"]}
{"id": "kolouriNeuralNetworksHypersurfaces2019", "title": "Neural Networks, Hypersurfaces, and Radon Transforms", "abstract": "Connections between integration along hypersufaces, Radon transforms, and neural networks are exploited to highlight an integral geometric mathematical interpretation of neural networks. By analyzing the properties of neural networks as operators on probability distributions for observed data, we show that the distribution of outputs for any node in a neural network can be interpreted as a nonlinear projection along hypersurfaces defined by level surfaces over the input data space. We utilize these descriptions to provide new interpretation for phenomena such as nonlinearity, pooling, activation functions, and adversarial examples in neural network-based learning problems.", "venue": "arXiv", "keywords": ["distribution estimation", "highly-analytical", "surveys"]}
{"id": "kolouriNeuralNetworksHypersurfaces2020", "title": "Neural Networks, Hypersurfaces, and the Generalized Radon Transform [Lecture Notes]", "abstract": "", "venue": "IEEE Signal Processing Magazine", "keywords": []}
{"id": "kostrikovImageAugmentationAll2021", "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels", "abstract": "We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.", "venue": "arXiv", "keywords": []}
{"id": "kouwIntroductionDomainAdaptation2019", "title": "An Introduction to Domain Adaptation and Transfer Learning", "abstract": "In machine learning, if the training data is an unbiased sample of an underlying distribution, then the learned classification function will make accurate predictions for new samples. However, if the training data is not an unbiased sample, then there will be differences between how the training data is distributed and how the test data is distributed. Standard classifiers cannot cope with changes in data distributions between training and test phases, and will not perform well. Domain adaptation and transfer learning are sub-fields within machine learning that are concerned with accounting for these types of changes. Here, we present an introduction to these fields, guided by the question: when and how can a classifier generalize from a source to a target domain? We will start with a brief introduction into risk minimization, and how transfer learning and domain adaptation expand upon this framework. Following that, we discuss three special cases of data set shift, namely prior, covariate and concept shift. For more complex domain shifts, there are a wide variety of approaches. These are categorized into: importance-weighting, subspace mapping, domain-invariant spaces, feature augmentation, minimax estimators and robust algorithms. A number of points will arise, which we will discuss in the last section. We conclude with the remark that many open questions will have to be addressed before transfer learners and domain-adaptive classifiers become practical.", "venue": "arXiv", "keywords": ["surveys", "transfer learning", "unsupervised da"]}
{"id": "kozerawskiTamingLongTail2022", "title": "Taming the Long Tail of Deep Probabilistic Forecasting", "abstract": "Deep probabilistic forecasting is gaining attention in numerous applications ranging from weather prognosis, through electricity consumption estimation, to autonomous vehicle trajectory prediction. However, existing approaches focus on improvements on the most common scenarios without addressing the performance on rare and difficult cases. In this work, we identify a long tail behavior in the performance of state-of-the-art deep learning methods on probabilistic forecasting. We present two moment-based tailedness measurement concepts to improve performance on the difficult tail examples: Pareto Loss and Kurtosis Loss. Kurtosis loss is a symmetric measurement as the fourth moment about the mean of the loss distribution. Pareto loss is asymmetric measuring right tailedness, modeling the loss using a generalized Pareto distribution (GPD). We demonstrate the performance of our approach on several real-world datasets including time series and spatiotemporal trajectories, achieving significant improvements on the tail examples.", "venue": "arXiv", "keywords": []}
{"id": "krishnamoorthyImprovedDataAugmentation2023", "title": "An Improved Data Augmentation Scheme for Model Predictive Control Policy Approximation", "abstract": "This paper considers the problem of data generation for MPC policy approximation. Learning an approximate MPC policy from expert demonstrations requires a large data set consisting of optimal state-action pairs, sampled across the feasible state space. Yet, the key challenge of efficiently generating the training samples has not been studied widely. Recently, a sensitivity-based data augmentation framework for MPC policy approximation was proposed, where the parametric sensitivities are exploited to cheaply generate several additional samples from a single offline MPC computation. The error due to augmenting the training data set with inexact samples was shown to increase with the size of the neighborhood around each sample used for data augmentation. Building upon this work, this letter paper presents an improved data augmentation scheme based on predictor-corrector steps that enforces a userdefined level of accuracy, and shows that the error bound of the augmented samples are independent of the size of the neighborhood used for data augmentation.", "venue": "IEEE Control Systems Letters", "keywords": []}
{"id": "krizhevskyImageNetClassificationDeep2012", "title": "ImageNet Classification with Deep Convolutional Neural Networks", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.", "venue": "Neural Information Processing Systems", "keywords": ["cnns", "foundational"]}
{"id": "kukelovaRadialDistortionHomography2015", "title": "Radial Distortion Homography", "abstract": "The importance of precise homography estimation is often underestimated even though it plays a crucial role in various vision applications such as plane or planarity detection, scene degeneracy tests, camera motion classification, image stitching, and many more. Ignoring the radial distortion component in homography estimation---even for classical perspective cameras---may lead to significant errors or totally wrong estimates. In this paper, we fill the gap among the homography estimation methods by presenting two algorithms for estimating homography between two cameras with different radial distortions. Both algorithms can handle planar scenes as well as scenes where the relative motion between the cameras is a pure rotation. The first algorithm uses the minimal number of five image point correspondences and solves a nonlinear system of polynomial equations using Gro basis method. The second algorithm uses a non-minimal number of six image point correspondences and leads to a simple system of two quadratic equations in two unknowns and one system of six linear equations. The proposed algorithms are fast, stable, and can be efficiently used inside a RANSAC loop.", "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["homography estimation"]}
{"id": "kumaraswamyFractalDimensionData", "title": "Fractal Dimension for Data Mining", "abstract": "In this project, we introduce the concept of intrinsic ``fractal'' dimension of a data set and show how this can be used to aid in several data mining tasks. We are interested in answering questions about the performance of a method and also in comparing between the methods quickly. In particular, we discuss two specific problems -- dimensionality reduction and vector quantization. In each of these problems, we show how the performance of a method is related to the fractal dimension of the data set. Using real and synthetic data sets, we validate these relationships and show how we can use this for faster evaluation and comparison of the methods.", "venue": "", "keywords": ["topological data analysis"]}
{"id": "kumarCertifyingModelAccuracy2022", "title": "Certifying Model Accuracy under Distribution Shifts", "abstract": "Certified robustness in machine learning has primarily focused on adversarial perturbations of the input with a fixed attack budget for each point in the data distribution. In this work, we present provable robustness guarantees on the accuracy of a model under bounded Wasserstein shifts of the data distribution. We show that a simple procedure that randomizes the input of the model within a transformation space is provably robust to distributional shifts under the transformation. Our framework allows the datum-specific perturbation size to vary across different points in the input distribution and is general enough to include fixed-sized perturbations as well. Our certificates produce guaranteed lower bounds on the performance of the model for any (natural or adversarial) shift of the input distribution within a Wasserstein ball around the original distribution. We apply our technique to: (i) certify robustness against natural (non-adversarial) transformations of images such as color shifts, hue shifts and changes in brightness and saturation, (ii) certify robustness against adversarial shifts of the input distribution, and (iii) show provable lower bounds (hardness results) on the performance of models trained on so-called\"unlearnable\"datasets that have been poisoned to interfere with model training.", "venue": "ArXiv", "keywords": ["accuracy certification", "generalization certification", "generalization quantification", "highly-analytical", "uncertainty quantification"]}
{"id": "kumarFisheyeDistanceNetSelfSupervisedFisheye2021", "title": "FisheyeDistanceNet++: Self-Supervised Fisheye Distance Estimation with Self-Attention, Robust Loss Function and Camera View Generalization", "abstract": "FisheyeDistanceNet proposed a self-supervised monocular depth estimation method for fisheye cameras with a large field of view (&gt; 180&#xB0;). To achieve scale-invariant depth estimation, FisheyeDistanceNet supervises depth map predictions over multiple scales during training. To overcome this bottleneck, we incorporate self-attention layers and robust loss function to FisheyeDistanceNet. A general adaptive robust loss function helps obtain sharp depth maps without a need to train over multiple scales and allows us to learn hyperparameters in loss function to aid in better optimization in terms of convergence speed and accuracy. We also ablate the importance of Instance Normalization over Batch Normalization in the network architecture. Finally, we generalize the network to be invariant to camera views by training multiple perspectives using front, rear, and side cameras. Proposed algorithm improvements, FisheyeDistanceNet++, result in 30% relative improvement in RMSE while reducing the training time by 25% on the WoodScape dataset. We also obtain state-of-the-art results on the KITTI dataset, in comparison to other self-supervised monocular methods.", "venue": "Electronic Imaging", "keywords": []}
{"id": "kumarImageDataAugmentation2023", "title": "Image Data Augmentation Approaches: A Comprehensive Survey and Future Directions", "abstract": "Deep learning algorithms have demonstrated remarkable performance in various computer vision tasks, however, limited labeled data can lead to overfitting problems, hindering the network's performance on unseen data. To address this issue, various generalization techniques have been proposed, including dropout, normalization, and advanced data augmentation. Among these techniques, image data augmentation which increases the dataset size by incorporating sample diversity - has received significant attention in recent times. In this survey, we focus on advanced image data augmentation techniques. We provide an overview of data augmentation, present a novel and comprehensive taxonomy of the reviewed data augmentation techniques, and discuss their strengths and limitations. Furthermore, we provide comprehensive results of the impact of data augmentation on three popular computer vision tasks: image classification, object detection, and semantic segmentation. For results reproducibility, the available codes of all data augmentation techniques have been compiled. Finally, we discuss the challenges and difficulties, as well as possible future directions for the research community. This survey provides several benefits: i) readers will gain a deeper understanding of how data augmentation can help address overfitting problems, ii) researchers will save time searching for comparison results, iii) the codes for the data augmentation techniques are available for result reproducibility, and iv) the discussion of future work will spark interest in the research community.", "venue": "arXiv", "keywords": ["surveys"]}
{"id": "kumarOmniDetSurroundView2023", "title": "OmniDet: Surround View Cameras Based Multi-task Visual Perception Network for Autonomous Driving", "abstract": "Surround View fisheye cameras are commonly deployed in automated driving for 360 \\\\ near-field sensing around the vehicle. This work presents a multi-task visual perception network on unrectified fisheye images to enable the vehicle to sense its surrounding environment. It consists of six primary tasks necessary for an autonomous driving system: depth estimation, visual odometry, semantic segmentation, motion segmentation, object detection, and lens soiling detection. We demonstrate that the jointly trained model performs better than the respective single task versions. Our multi-task model has a shared encoder providing a significant computational advantage and has synergized decoders where tasks support each other. We propose a novel camera geometry based adaptation mechanism to encode the fisheye distortion model both at training and inference. This was crucial to enable training on the WoodScape dataset, comprised of data from different parts of the world collected by 12 different cameras mounted on three different cars with different intrinsics and viewpoints. Given that bounding boxes is not a good representation for distorted fisheye images, we also extend object detection to use a polygon with non-uniformly sampled vertices. We additionally evaluate our model on standard automotive datasets, namely KITTI and Cityscapes. We obtain the state-of-the-art results on KITTI for depth estimation and pose estimation tasks and competitive performance on the other tasks. We perform extensive ablation studies on various architecture choices and task weighting methodologies. A short video at https://youtu.be/xbSjZ5OfPes provides qualitative results.", "venue": "arXiv", "keywords": ["automotive occlusion", "dataset debut", "fisheye distortion"]}
{"id": "kumarProvableRobustnessWasserstein2023", "title": "Provable Robustness against Wasserstein Distribution Shifts via Input Randomization", "abstract": "Certified robustness in machine learning has primarily focused on adversarial perturbations with a fixed attack budget for each sample in the input distribution. In this work, we present provable robustness guarantees on the accuracy of a model under bounded Wasserstein shifts of the data distribution. We show that a simple procedure that randomizes the input of the model within a transformation space is provably robust to distributional shifts under that transformation. Our framework allows the datum-specific perturbation size to vary across different points in the input distribution and is general enough to include fixed-sized perturbations as well. Our certificates produce guaranteed lower bounds on the performance of the model for any shift (natural or adversarial) of the input distribution within a Wasserstein ball around the original distribution. We apply our technique to certify robustness against natural (non-adversarial) transformations of images such as color shifts, hue shifts, and changes in brightness and saturation. We obtain strong performance guarantees for the robust model under clearly visible shifts in the input images. Our experiments establish the non-vacuousness of our certificates by showing that the certified lower bound on a robust model's accuracy is higher than the empirical accuracy of an undefended model under a distribution shift. We also show provable distributional robustness against adversarial attacks. Moreover, our results also imply guaranteed lower bounds (hardness result) on the performance of models trained on so-called ``unlearnable'' datasets that have been poisoned to interfere with model training. We show that the performance of a robust model is guaranteed to remain", "venue": "International Conference on Learning Representations", "keywords": ["domain randomization", "generalization quantification", "robustness analysis", "topological analysis"]}
{"id": "kumarPROVABLEROBUSTNESSWASSERSTEIN2023a", "title": "PROVABLE ROBUSTNESS AGAINST WASSERSTEIN DIS- TRIBUTION SHIFTS VIA INPUT RANDOMIZATION", "abstract": "Certified robustness in machine learning has primarily focused on adversarial perturbations with a fixed attack budget for each sample in the input distribution. In this work, we present provable robustness guarantees on the accuracy of a model under bounded Wasserstein shifts of the data distribution. We show that a simple procedure that randomizes the input of the model within a transformation space is provably robust to distributional shifts under that transformation. Our framework allows the datum-specific perturbation size to vary across different points in the input distribution and is general enough to include fixed-sized perturbations as well. Our certificates produce guaranteed lower bounds on the performance of the model for any shift (natural or adversarial) of the input distribution within a Wasserstein ball around the original distribution. We apply our technique to certify robustness against natural (non-adversarial) transformations of images such as color shifts, hue shifts, and changes in brightness and saturation. We obtain strong performance guarantees for the robust model under clearly visible shifts in the input images. Our experiments establish the non-vacuousness of our certificates by showing that the certified lower bound on a robust model's accuracy is higher than the empirical accuracy of an undefended model under a distribution shift. We also show provable distributional robustness against adversarial attacks. Moreover, our results also imply guaranteed lower bounds (hardness result) on the performance of models trained on so-called ``unlearnable'' datasets that have been poisoned to interfere with model training. We show that the performance of a robust model is guaranteed to remain above a certain threshold on the test distribution even when the base model is trained on the poisoned dataset.", "venue": "", "keywords": []}
{"id": "kumarRSMDARandomSlices2023", "title": "RSMDA: Random Slices Mixing Data Augmentation", "abstract": "Advanced data augmentation techniques have demonstrated great success in deep learning algorithms. Among these techniques, single-image-based data augmentation (SIBDA), in which a single image's regions are randomly erased in different ways, has shown promising results. However, randomly erasing image regions in SIBDA can cause a loss of the key discriminating features, consequently misleading neural networks and lowering their performance. To alleviate this issue, in this paper, we propose the random slices mixing data augmentation (RSMDA) technique, in which slices of one image are placed onto another image to create a third image that enriches the diversity of the data. RSMDA also mixes the labels of the original images to create an augmented label for the new image to exploit label smoothing. Furthermore, we propose and investigate three strategies for RSMDA: (i) the vertical slices mixing strategy, (ii) the horizontal slices mixing strategy, and (iii) a random mix of both strategies. Of these strategies, the horizontal slice mixing strategy shows the best performance. To validate the proposed technique, we perform several experiments using different neural networks across four datasets: fashion-MNIST, CIFAR10, CIFAR100, and STL10. The experimental results of the image classification with RSMDA showed better accuracy and robustness than the state-of-the-art (SOTA) single-image-based and multi-image-based methods. Finally, class activation maps are employed to visualize the focus of the neural network and compare maps using the SOTA data augmentation methods.", "venue": "Applied Sciences", "keywords": ["adversarial robustness", "cnns", "mixture augmentations"]}
{"id": "kumarUnRectDepthNetSelfSupervisedMonocular2023", "title": "UnRectDepthNet: Self-Supervised Monocular Depth Estimation Using a Generic Framework for Handling Common Camera Distortion Models", "abstract": "In classical computer vision, rectification is an integral part of multi-view depth estimation. It typically includes epipolar rectification and lens distortion correction. This process simplifies the depth estimation significantly, and thus it has been adopted in CNN approaches. However, rectification has several side effects, including a reduced field of view (FOV), resampling distortion, and sensitivity to calibration errors. The effects are particularly pronounced in case of significant distortion (e.g., wide-angle fisheye cameras). In this paper, we propose a generic scale-aware self-supervised pipeline for estimating depth, euclidean distance, and visual odometry from unrectified monocular videos. We demonstrate a similar level of precision on the unrectified KITTI dataset with barrel distortion comparable to the rectified KITTI dataset. The intuition being that the rectification step can be implicitly absorbed within the CNN model, which learns the distortion model without increasing complexity. Our approach does not suffer from a reduced field of view and avoids computational costs for rectification at inference time. To further illustrate the general applicability of the proposed framework, we apply it to wide-angle fisheye cameras with 190\\ horizontal field of view. The training framework UnRectDepthNet takes in the camera distortion model as an argument and adapts projection and unprojection functions accordingly. The proposed algorithm is evaluated further on the KITTI rectified dataset, and we achieve state-of-the-art results that improve upon our previous work FisheyeDistanceNet. Qualitative results on a distorted test scene video sequence indicate excellent performance https://youtu.be/K6pbx3bU4Ss.", "venue": "arXiv", "keywords": ["depth estimation", "fisheye distortion", "homography estimation"]}
{"id": "kunduFeatureSpaceOptimization2016", "title": "Feature Space Optimization for Semantic Video Segmentation", "abstract": "", "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["crfs", "semantic segmentation"]}
{"id": "kuriyamaLatentAugmentDynamicallyOptimized2023", "title": "LatentAugment: Dynamically Optimized Latent Probabilities of Data Augmentation", "abstract": "Although data augmentation is a powerful technique for improving the performance of image classification tasks, it is difficult to identify the best augmentation policy. The optimal augmentation policy, which is the latent variable, cannot be directly observed. To address this problem, this study proposes \\ , which estimates the latent probability of optimal augmentation. The proposed method is appealing in that it can dynamically optimize the augmentation strategies for each input and model parameter in learning iterations. Theoretical analysis shows that LatentAugment is a general model that includes other augmentation methods as special cases, and it is simple and computationally efficient in comparison with existing augmentation methods. Experimental results show that the proposed LatentAugment has higher test accuracy than previous augmentation methods on the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "kurzmanClassBasedStylingRealtime2019", "title": "Class-Based Styling: Real-time Localized Style Transfer with Semantic Segmentation", "abstract": "We propose a Class-Based Styling method (CBS) that can map different styles for different object classes in realtime. CBS achieves real-time performance by carrying out two steps simultaneously. While a semantic segmentation method is used to obtain the mask of each object class in a video frame, a styling method is used to style that frame globally. Then an object class can be styled by combining the segmentation mask and the styled image. The user can also select multiple styles so that different object classes can have different styles in a single frame. For semantic segmentation, we leverage DABNet that achieves high accuracy, yet only has 0.76 million parameters and runs at 104 FPS. For the style transfer step, we use the popular real-time method proposed by Johnson et al. . We evaluated CBS on a video of the CityScapes dataset and observed high-quality localized style transfer results for different object classes and real-time performance. The code is available at https: //github.com/IssamLaradji/CBStyling.", "venue": "arXiv", "keywords": ["masked style transfer", "semantic segmentation", "style transfer"]}
{"id": "kutchClearPathsTraining2024", "title": "Beyond Clear Paths: Training with Neural Style Transfer and Auto-Augmentation for Domain-Generalized Segmentation of Soiled Images", "abstract": "The use of image segmentation within mission-critical autonomous vehicle applications is contingent on reliability of visual sensors, which may become significantly compromised by environmental occlusions such as mud, water, dust, or ice. This thesis explores semantic segmentation techniques tailored for identifying occluded regions, addressing the limitations of conventional methods under unpredictable real-world conditions. Given the scarcity of training datasets with realistic occlusions and the resulting reliance on synthetic data, this research leverages style transfer augmentations and a novel augmentation policy optimization framework to diversify training datasets and improve model generalization. A newly-curated dataset of naturally occurring occlusions evaluates the efficacy of these augmentation methods to out-of-domain image samples. By combining empirical experimentation with theoretical insights into domain generalization, this work presents a robust approach to enhancing segmentation performance while investigating model generalization outcomes after training under a variety of occluded environments.", "venue": "", "keywords": []}
{"id": "kuzborskijBetterthanKLPACBayesBounds2024", "title": "Better-than-KL PAC-Bayes Bounds", "abstract": "Let \\ \\ \\ be a sequence of random elements, where \\ is a fixed scalar function, \\ are independent random variables (data), and \\ is a random parameter distributed according to some data-dependent posterior distribution \\ . In this paper, we consider the problem of proving concentration inequalities to estimate the mean of the sequence. An example of such a problem is the estimation of the generalization error of some predictor trained by a stochastic algorithm, such as a neural network where \\ is a loss function. Classically, this problem is approached through a PAC-Bayes analysis where, in addition to the posterior, we choose a prior distribution which captures our belief about the inductive bias of the learning problem. Then, the key quantity in PAC-Bayes concentration bounds is a divergence that captures the complexity of the learning problem where the de facto standard choice is the KL divergence. However, the tightness of this choice has rarely been questioned. In this paper, we challenge the tightness of the KL-divergence-based bounds by showing that it is possible to achieve a strictly tighter bound. In particular, we demonstrate new high-probability PAC-Bayes bounds with a novel and better-than-KL divergence that is inspired by Zhang et al. (2022). Our proof is inspired by recent advances in regret analysis of gambling algorithms, and its use to derive concentration inequalities. Our result is first-of-its-kind in that existing PAC-Bayes bounds with non-KL divergences are not known to be strictly better than KL. Thus, we believe our work marks the first step towards identifying optimal rates of PAC-Bayes bounds.", "venue": "arXiv", "keywords": ["domain generalization", "generalization certification", "pac learning"]}
{"id": "kwittStatisticalTopologicalData2015", "title": "Statistical Topological Data Analysis - A Kernel Perspective", "abstract": "We consider the problem of statistical computations with persistence diagrams, a summary representation of topological features in data. These diagrams encode persistent homology, a widely used invariant in topological data analysis. While several avenues towards a statistical treatment of the diagrams have been explored recently, we follow an alternative route that is motivated by the success of methods based on the embedding of probability measures into reproducing kernel Hilbert spaces. In fact, a positive definite kernel on persistence diagrams has recently been proposed, connecting persistent homology to popular kernel-based learning techniques such as support vector machines. However, important properties of that kernel which would enable a principled use in the context of probability measure embeddings remain to be explored. Our contribution is to close this gap by proving universality of a variant of the original kernel, and to demonstrate its effective use in two-sample hypothesis testing on synthetic as well as real-world data.", "venue": "Advances in Neural Information Processing Systems", "keywords": []}
{"id": "kwonPOMOPolicyOptimization2021", "title": "POMO: Policy Optimization with Multiple Optima for Reinforcement Learning", "abstract": "In neural combinatorial optimization (CO), reinforcement learning (RL) can turn a deep neural net into a fast, powerful heuristic solver of NP-hard problems. This approach has a great potential in practical applications because it allows near-optimal solutions to be found without expert guides armed with substantial domain knowledge. We introduce Policy Optimization with Multiple Optima (POMO), an end-to-end approach for building such a heuristic solver. POMO is applicable to a wide range of CO problems. It is designed to exploit the symmetries in the representation of a CO solution. POMO uses a modified REINFORCE algorithm that forces diverse rollouts towards all optimal solutions. Empirically, the low-variance baseline of POMO makes RL training fast and stable, and it is more resistant to local minima compared to previous approaches. We also introduce a new augmentation-based inference method, which accompanies POMO nicely. We demonstrate the effectiveness of POMO by solving three popular NP-hard problems, namely, traveling salesman (TSP), capacitated vehicle routing (CVRP), and 0-1 knapsack (KP). For all three, our solver based on POMO shows a significant improvement in performance over all recent learned heuristics. In particular, we achieve the optimality gap of 0.14% with TSP100 while reducing inference time by more than an order of magnitude.", "venue": "arXiv", "keywords": ["ppo", "reinforcement learning"]}
{"id": "LabelPixelsSemantic", "title": "Label Pixels for Semantic Segmentation - MATLAB & Simulink", "abstract": "", "venue": "", "keywords": []}
{"id": "lahaneColorImageImplementation2017", "title": "Color Image Implementation of Guided Filter Derived By Local Linear Model", "abstract": "Filtering is widely used in image processing for various applications now. The guided filter has been proposed and became one of the popular filtering methods. Derived from a local linear model, the guided filter generates the filtering output by considering the content of a guidance image, which can be the input image itself or another different image .Image Enhancement is one of the most important and difficult techniques in digital image Processing. Image Enhancement is used for improve the quality. Guided filter uses the color images for implementation because color guidance image can better preserves the edges that are not distinguishable in gray-scale. Guided filter simulation done using MATLAB. We demonstrate that the guided filter is both effective and efficient in a great variety of computer vision and computer graphics applications including noise reduction, detail smoothing/enhancement. And then calculating its PSNR value using VHDL.", "venue": "", "keywords": ["filter augmentations", "guided filters"]}
{"id": "LearningRobustGlobal", "title": "Learning Robust Global Representations by Penalizing Local Predictive Power", "abstract": "", "venue": "", "keywords": []}
{"id": "ledoitWellconditionedEstimatorLargedimensional2004", "title": "A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices", "abstract": "Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For largedimensional covariance matrices, the usual estimator---the sample covariance matrix---is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample.", "venue": "Journal of Multivariate Analysis", "keywords": []}
{"id": "leeCollisionRiskAssessment2018", "title": "Collision Risk Assessment of Occluded Vehicle Based on the Motion Predictions Using the Precise Road Map", "abstract": "A perception system based on vehicle detection sensors, which are mounted on an ego vehicle, has restricted visibility because of blockage by obstacles. Estimating the risk of collision with moving vehicles in an occluded area is difficult because their locations and speeds cannot be detected. To address the occlusion problem, this paper proposes a probabilistic collision risk assessment method for a potential collision vehicle in an occluded area. The proposed method estimates the collision risk in three steps: occlusion boundary modeling of perception, motion prediction of the potential collision vehicles, and probabilistic collision risk assessment. The first step models the occlusion boundary to classify the free space and the unknown region. In the second step, the moving path of each potential collision vehicle is predicted considering its future behavior. The final step estimates the collision probability with a potential collision vehicle based on the speed distribution of the vehicles on the road. We evaluate the proposed probabilistic collision risk assessment method in several occlusion scenarios with real traffic, including an alleyway, a merging lane, and blockage by a bulky vehicle.", "venue": "Robotics and Autonomous Systems", "keywords": ["automotive occlusion"]}
{"id": "leeDecomposeAdjustCompose2023", "title": "Decompose, Adjust, Compose: Effective Normalization by Playing with Frequency for Domain Generalization", "abstract": "Domain generalization (DG) is a principal task to evaluate the robustness of computer vision models. Many previous studies have used normalization for DG. In normalization, statistics and normalized features are regarded as style and content, respectively. However, it has a content variation problem when removing style because the boundary between content and style is unclear. This study addresses this problem from the frequency domain perspective, where amplitude and phase are considered as style and content, respectively. First, we verify the quantitative phase variation of normalization through the mathematical derivation of the Fourier transform formula. Then, based on this, we propose a novel normalization method, P CN orm, which eliminates style only as the preserving content through spectral decomposition. Furthermore, we propose advanced P CN orm variants, CCN orm and SCN orm, which adjust the degrees of variations in content and style, respectively. Thus, they can learn domain-agnostic representations for DG. With the normalization methods, we propose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domain gap. The proposed models outperform other recent DG methods. The DAC-SC achieves an average state-of-the-art performance of 65.6% on five datasets: PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.", "venue": "arXiv", "keywords": ["domain generalization", "instance normalization", "spectral methods", "style transfer"]}
{"id": "leeDRITDiverseImagetoImage2019", "title": "DRIT++: Diverse Image-to-Image Translation via Disentangled Representations", "abstract": "Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for this task: 1) lack of aligned training pairs and 2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for generating diverse outputs without paired training images. To synthesize diverse outputs, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Our model takes the encoded content features extracted from a given input and attribute vectors sampled from the attribute space to synthesize diverse outputs at test time. To handle unpaired training data, we introduce a cross-cycle consistency loss based on disentangled representations. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks without paired training data. For quantitative evaluations, we measure realism with user study and Fr ' inception distance, and measure diversity with the perceptual distance metric, Jensen-Shannon divergence, and number of statistically-different bins.", "venue": "arXiv", "keywords": ["gans", "image-to-image"]}
{"id": "leeGeometryFeatureSpace2023", "title": "The Geometry of Feature Space in Deep Learning Models: A Holistic Perspective and Comprehensive Review", "abstract": "As the field of deep learning experiences a meteoric rise, the urgency to decipher the complex geometric properties of feature spaces, which underlie the effectiveness of diverse learning algorithms and optimization techniques, has become paramount. In this scholarly review, a comprehensive, holistic outlook on the geometry of feature spaces in deep learning models is provided in order to thoroughly probe the interconnections between feature spaces and a multitude of influential factors such as activation functions, normalization methods, and model architectures. The exploration commences with an all-encompassing examination of deep learning models, followed by a rigorous dissection of feature space geometry, delving into manifold structures, curvature, wide neural networks and Gaussian processes, critical points and loss landscapes, singular value spectra, and adversarial robustness, among other notable topics. Moreover, transfer learning and disentangled representations in feature space are illuminated, accentuating the progress and challenges in these areas. In conclusion, the challenges and future research directions in the domain of feature space geometry are outlined, emphasizing the significance of comprehending overparameterized models, unsupervised and semi-supervised learning, interpretable feature space geometry, topological analysis, and multimodal and multi-task learning. Embracing a holistic perspective, this review aspires to serve as an exhaustive guide for researchers and practitioners alike, clarifying the intricacies of the geometry of feature spaces in deep learning models and mapping the trajectory for future advancements in this enigmatic and enthralling domain.", "venue": "Mathematics", "keywords": []}
{"id": "leeImprovedSemanticSegmentation2024", "title": "Improved Semantic Segmentation by Fisheye Image Augmentation for Driving Scene", "abstract": "The camera sensor plays a pivotal role among the array of sensors utilized for perception in self-driving cars. The appearance of the image captured by the camera depends on the type of lens used, among which the Fisheye lens extends the camera's coverage to a wider field of view, enabling vehicle designers to employ fewer cameras to achieve a 360 degree~view. Consequently, the integration of fisheye cameras has the potential to reduce production costs, prompting many designers to explore their use in self-driving cars. Nonetheless, data availability, particularly annotated data such as semantic annotations, for fisheye cameras is scarce compared to conventional cameras. Moreover, the process of collecting large dataset for model training is time-intensive. In this paper, we propose a novel data augmentation algorithm to address the scarcity of data. To this end, we leverage not only images from cameras but also point cloud data from LiDAR sensor. We employ both Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures to demonstrate the augmentation effects. Ultimately, our findings reveal that our augmentation technique enhances model performance, especially detecting small objects.", "venue": "", "keywords": ["fisheye distortion", "semantic segmentation"]}
{"id": "leeImprovingTransferabilityRepresentations", "title": "Improving Transferability of Representations via Augmentation-Aware Self-Supervision", "abstract": "Recent unsupervised representation learning methods have shown to be effective in a range of vision tasks by learning representations invariant to data augmentations such as random cropping and color jittering. However, such invariance could be harmful to downstream tasks if they rely on the characteristics of the data augmentations, e.g., location- or color-sensitive. To avoid such failures and obtain more generalizable representations, we suggest to optimize an auxiliary self-supervised loss, coined AugSelf, that learns the difference of augmentation parameters (e.g., cropping positions, color adjustment intensities) between two randomly augmented samples. Our intuition is that AugSelf encourages to preserve augmentation-aware information in learned representations, which could be beneficial for their transferability. Furthermore, AugSelf can easily be incorporated into recent stateof-the-art representation learning methods with a negligible additional training cost. Extensive experiments demonstrate that our simple idea consistently improves the transferability of representations learned by supervised and unsupervised methods in various transfer learning scenarios.", "venue": "", "keywords": ["auto-augmentation policies", "representation learning", "self-supervised learning"]}
{"id": "leeRemovingUndesirableFeature2021", "title": "Removing Undesirable Feature Contributions Using Out-of-Distribution Data", "abstract": "Several data augmentation methods deploy unlabeled-in-distribution (UID) data to bridge the gap between the training and inference of neural networks. However, these methods have clear limitations in terms of availability of UID data and dependence of algorithms on pseudo-labels. Herein, we propose a data augmentation method to improve generalization in both adversarial and standard learning by using out-of-distribution (OOD) data that are devoid of the abovementioned issues. We show how to improve generalization theoretically using OOD data in each learning scenario and complement our theoretical analysis with experiments on CIFAR-10, CIFAR-100, and a subset of ImageNet. The results indicate that undesirable features are shared even among image data that seem to have little correlation from a human point of view. We also present the advantages of the proposed method through comparison with other data augmentation methods, which can be used in the absence of UID data. Furthermore, we demonstrate that the proposed method can further improve the existing state-of-the-art adversarial training.", "venue": "arXiv", "keywords": ["augmentation stability", "domain generalization", "feature engineering"]}
{"id": "leeSimpleUnifiedFramework2018", "title": "A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks", "abstract": "Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.", "venue": "arXiv", "keywords": ["anomaly detection", "ood detection"]}
{"id": "leeTsanetTemporalScale2023", "title": "Tsanet: Temporal and Scale Alignment for Unsupervised Video Object Segmentation", "abstract": "Unsupervised Video Object Segmentation (UVOS) refers to the challenging task of segmenting the prominent object in videos without manual guidance. In other words, the network detects the accurate region of the target object in a sequence of RGB frames without prior knowledge. In recent works, two approaches for UVOS have been discussed that can be divided into: appearance and appearance-motion based methods. Appearance based methods utilize the correlation information of inter-frames to capture target object that commonly appears in a sequence. However, these methods does not consider the motion of target object due to exploit the correlation information between randomly paired frames. Appearance-motion based methods, on the other hand, fuse the appearance features from RGB frames with the motion features from optical flow. Motion cue provides useful information since salient objects typically show distinctive motion in a sequence. However, these approaches have the limitation that the dependency on optical flow is dominant. In this paper, we propose a novel framework for UVOS that can address aforementioned limitations of two approaches in terms of both time and scale. Temporal Alignment Fusion aligns the saliency information of adjacent frames with the target frame to leverage the information of adjacent frames. Scale Alignment Decoder predicts the target object mask precisely by aggregating differently scaled feature maps via continuous mapping with implicit neural representation. We present experimental results on public benchmark datasets, DAVIS 2016 and FBMS, which demonstrate the effectiveness of our method. Furthermore, we outperform the state-of-the-art methods on DAVIS 2016.", "venue": "2023 IEEE International Conference on Image Processing (ICIP)", "keywords": ["temporal consistency", "unsupervised learning"]}
{"id": "leeWildNetLearningDomain2022", "title": "WildNet: Learning Domain Generalized Semantic Segmentation from the Wild", "abstract": "We present a new domain generalized semantic segmentation network named WildNet, which learns domain-generalized features by leveraging a variety of contents and styles from the wild. In domain generalization, the low generalization ability for unseen target domains is clearly due to overfitting to the source domain. To address this problem, previous works have focused on generalizing the domain by removing or diversifying the styles of the source domain. These alleviated overfitting to the source-style but overlooked overfitting to the source-content. In this paper, we propose to diversify both the content and style of the source domain with the help of the wild. Our main idea is for networks to naturally learn domain-generalized semantic information from the wild. To this end, we diversify styles by augmenting source features to resemble wild styles and enable networks to adapt to a variety of styles. Furthermore, we encourage networks to learn class-discriminant features by providing semantic variations borrowed from the wild to source contents in the feature space. Finally, we regularize networks to capture consistent semantic information even when both the content and style of the source domain are extended to the wild. Extensive experiments on five different datasets validate the effectiveness of our WildNet, and we significantly outperform state-of-the-art methods. The source code and model are available online: https://github.com/suhyeonlee/WildNet.", "venue": "arXiv", "keywords": ["domain generalization", "feature-level augmentation", "instance normalization", "semantic segmentation"]}
{"id": "leiBlindVideoTemporal2020", "title": "Blind Video Temporal Consistency via Deep Video Prior", "abstract": "Applying image processing algorithms independently to each video frame often leads to temporal inconsistency in the resulting video. To address this issue, we present a novel and general approach for blind video temporal consistency. Our method is only trained on a pair of original and processed videos directly instead of a large dataset. Unlike most previous methods that enforce temporal consistency with optical flow, we show that temporal consistency can be achieved by training a convolutional network on a video with the Deep Video Prior. Moreover, a carefully designed iteratively reweighted training strategy is proposed to address the challenging multimodal inconsistency problem. We demonstrate the effectiveness of our approach on 7 computer vision tasks on videos. Extensive quantitative and perceptual experiments show that our approach obtains superior performance than state-of-the-art methods on blind video temporal consistency. Our source codes are publicly available at github.com/ChenyangLEI/deep-video-prior.", "venue": "arXiv", "keywords": ["cnns", "domain adaptation", "model adaptation", "temporal consistency", "transfer learning"]}
{"id": "leinoFeatureWiseBiasAmplification2019", "title": "Feature-Wise Bias Amplification", "abstract": "We study the phenomenon of bias amplification in classifiers, wherein a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. We demonstrate that bias amplification can arise via an inductive bias in gradient descent methods that results in the overestimation of the importance of moderately-predictive \"weak\" features if insufficient training data is available. This overestimation gives rise to feature-wise bias amplification -- a previously unreported form of bias that can be traced back to the features of a trained model. Through analysis and experiments, we show that while some bias cannot be mitigated without sacrificing accuracy, feature-wise bias amplification can be mitigated through targeted feature selection. We present two new feature selection algorithms for mitigating bias amplification in linear models, and show how they can be adapted to convolutional neural networks efficiently. Our experiments on synthetic and real data demonstrate that these algorithms consistently lead to reduced bias without harming accuracy, in some cases eliminating predictive bias altogether while providing modest gains in accuracy.", "venue": "arXiv", "keywords": ["bias sources", "cnns", "feature engineering"]}
{"id": "lemleySmartAugmentationLearning2017", "title": "Smart Augmentation - Learning an Optimal Data Augmentation Strategy", "abstract": "A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks(DNN). There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method which we call Smart Augmentation and we show how to use it to increase the accuracy and reduce overfitting on a target network. Smart Augmentation works by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network.", "venue": "IEEE Access", "keywords": ["auto-augmentation policies", "representation learning"]}
{"id": "levinClosedFormSolution2008", "title": "A Closed Form Solution to Natural Image Matting", "abstract": "Interactive digital matting, the process of extracting a foreground object from an image based on limited user input, is an important task in image and video editing. From a computer vision perspective, this task is extremely challenging because it is massively ill-posed --- at each pixel we must estimate the foreground and the background colors, as well as the foreground opacity (``alpha matte'') from a single color measurement. Current approaches either restrict the estimation to a small part of the image, estimating foreground and background colors based on nearby pixels where they are known, or perform iterative nonlinear estimation by alternating foreground and background color estimation with alpha estimation.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "keywords": ["layer separation"]}
{"id": "liangMultiHeadEnsembleMultiTask2023", "title": "A Multi-Head Ensemble Multi-Task Learning Approach for Dynamical Computation Offloading", "abstract": "Computation offloading has become a popular solution to support computationally intensive and latency-sensitive applications by transferring computing tasks to mobile edge servers (MESs) for execution, which is known as mobile/multi-access edge computing (MEC). To improve the MEC performance, it is required to design an optimal offloading strategy that includes offloading decision (i.e., whether offloading or not) and computational resource allocation of MEC. The design can be formulated as a mixed-integer nonlinear programming (MINLP) problem, which is generally NP-hard and its effective solution can be obtained by performing online inference through a well-trained deep neural network (DNN) model. However, when the system environments change dynamically, the DNN model may lose efficacy due to the drift of input parameters, thereby decreasing the generalization ability of the DNN model. To address this unique challenge, in this paper, we propose a multi-head ensemble multi-task learning (MEMTL) approach with a shared backbone and multiple prediction heads (PHs). Specifically, the shared backbone will be invariant during the PHs training and the inferred results will be ensembled, thereby significantly reducing the required training overhead and improving the inference performance. As a result, the joint optimization problem for offloading decision and resource allocation can be efficiently solved even in a time-varying wireless environment. Experimental results show that the proposed MEMTL outperforms benchmark methods in both the inference accuracy and mean square error without requiring additional training data.", "venue": "arXiv", "keywords": ["multi-task learning"]}
{"id": "liangStyleSteganLeakfreeStyle2023", "title": "StyleStegan: Leak-free Style Transfer Based on Feature Steganography", "abstract": "In modern social networks, existing style transfer methods suffer from a serious content leakage issue, which hampers the ability to achieve serial and reversible stylization, thereby hindering the further propagation of stylized images in social networks. To address this problem, we propose a leak-free style transfer method based on feature steganography. Our method consists of two main components: a style transfer method that accomplishes artistic stylization on the original image and an image steganography method that embeds content feature secrets on the stylized image. The main contributions of our work are as follows: 1) We identify and explain the phenomenon of content leakage and its underlying causes, which arise from content inconsistencies between the original image and its subsequent stylized image. 2) We design a neural flow model for achieving loss-free and biased-free style transfer. 3) We introduce steganography to hide content feature information on the stylized image and control the subsequent usage rights. 4) We conduct comprehensive experimental validation using publicly available datasets MS-COCO and Wikiart. The results demonstrate that StyleStegan successfully mitigates the content leakage issue in serial and reversible style transfer tasks. The SSIM performance metrics for these tasks are 14.98% and 7.28% higher, respectively, compared to a suboptimal baseline model.", "venue": "2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)", "keywords": ["normalizing flows", "promising", "style transfer"]}
{"id": "liaoDeepLearningCamera", "title": "Deep Learning for Camera Calibration and Beyond: A Survey", "abstract": "Camera calibration involves estimating camera parameters to infer geometric features from captured sequences, which is crucial for computer vision and robotics. However, conventional calibration is laborious and requires dedicated collection. Recent efforts show that learning-based solutions have the potential to be used in place of the repeatability works of manual calibrations. Among these solutions, various learning strategies, networks, geometric priors, and datasets have been investigated. In this paper, we provide a comprehensive survey of learning-based camera calibration techniques, by analyzing their strengths and limitations. Our main calibration categories include the standard pinhole camera model, distortion camera model, cross-view model, and cross-sensor model, following the research trend and extended applications. As there is no benchmark in this community, we collect a holistic calibration dataset that can serve as a public platform to evaluate the generalization of existing methods. It comprises both synthetic and realworld data, with images and videos captured by different cameras in diverse scenes. Toward the end of this paper, we discuss the challenges and provide further research directions. To our knowledge, this is the first survey for the learning-based camera calibration (spanned 8 years). The summarized methods, datasets, and benchmarks are available and will be regularly updated at https://github. com/KangLiao929/Awesome-Deep-Camera-Calibration.", "venue": "", "keywords": []}
{"id": "liaoDeepLearningCamera2023", "title": "Deep Learning for Camera Calibration and Beyond: A Survey", "abstract": "Camera calibration involves estimating camera parameters to infer geometric features from captured sequences, which is crucial for computer vision and robotics. However, conventional calibration is laborious and requires dedicated collection. Recent efforts show that learning-based solutions have the potential to be used in place of the repeatability works of manual calibrations. Among these solutions, various learning strategies, networks, geometric priors, and datasets have been investigated. In this paper, we provide a comprehensive survey of learning-based camera calibration techniques, by analyzing their strengths and limitations. Our main calibration categories include the standard pinhole camera model, distortion camera model, cross-view model, and cross-sensor model, following the research trend and extended applications. As there is no benchmark in this community, we collect a holistic calibration dataset that can serve as a public platform to evaluate the generalization of existing methods. It comprises both synthetic and real-world data, with images and videos captured by different cameras in diverse scenes. Toward the end of this paper, we discuss the challenges and provide further research directions. To our knowledge, this is the first survey for the learning-based camera calibration (spanned 8 years). The summarized methods, datasets, and benchmarks are available and will be regularly updated at https://github.com/KangLiao929/Awesome-Deep-Camera-Calibration.", "venue": "arXiv", "keywords": ["camera calibration", "surveys"]}
{"id": "liBidirectionalLearningDomain2019", "title": "Bidirectional Learning for Domain Adaptation of Semantic Segmentation", "abstract": "Domain adaptation for semantic image segmentation is very necessary since manually labeling large datasets with pixel-level labels is expensive and time consuming. Existing domain adaptation techniques either work on limited datasets, or yield not so good performance compared with supervised learning. In this paper, we propose a novel bidirectional learning framework for domain adaptation of segmentation. Using the bidirectional learning, the image translation model and the segmentation adaptation model can be learned alternatively and promote to each other. Furthermore, we propose a self-supervised learning algorithm to learn a better segmentation adaptation model and in return improve the image translation model. Experiments show that our method is superior to the state-of-the-art methods in domain adaptation of segmentation with a big margin. The source code is available at https://github.com/liyunsheng13/BDL.", "venue": "arXiv", "keywords": ["bidirectional learning", "domain adaptation", "semantic segmentation"]}
{"id": "liBlindGeometricDistortion2019", "title": "Blind Geometric Distortion Correction on Images Through Deep Learning", "abstract": "We propose the first general framework to automatically correct different types of geometric distortion in a single input image. Our proposed method employs convolutional neural networks (CNNs) trained by using a large synthetic distortion dataset to predict the displacement field between distorted images and corrected images. A model fitting method uses the CNN output to estimate the distortion parameters, achieving a more accurate prediction. The final corrected image is generated based on the predicted flow using an efficient, high-quality resampling method. Experimental results demonstrate that our algorithm outperforms traditional correction methods, and allows for interesting applications such as distortion transfer, distortion exaggeration, and co-occurring distortion correction.", "venue": "arXiv", "keywords": []}
{"id": "liClosedformSolutionPhotorealistic2018", "title": "A Closed-form Solution to Photorealistic Image Stylization", "abstract": "Photorealistic image stylization concerns transferring style of a reference photo to a content photo with the constraint that the stylized photo should remain photorealistic. While several photorealistic image stylization methods exist, they tend to generate spatially inconsistent stylizations with noticeable artifacts. In this paper, we propose a method to address these issues. The proposed method consists of a stylization step and a smoothing step. While the stylization step transfers the style of the reference photo to the content photo, the smoothing step ensures spatially consistent stylizations. Each of the steps has a closed-form solution and can be computed efficiently. We conduct extensive experimental validations. The results show that the proposed method generates photorealistic stylization outputs that are more preferred by human subjects as compared to those by the competing methods while running much faster. Source code and additional results are available at https://github.com/NVIDIA/FastPhotoStyle .", "venue": "arXiv", "keywords": ["augmentation stability", "feature whitening", "feature-level augmentation", "instance normalization", "style transfer"]}
{"id": "liCrossContrastingFeature2023", "title": "Cross Contrasting Feature Perturbation for Domain Generalization", "abstract": "", "venue": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)", "keywords": ["cross-domain augmentation", "instance normalization", "promising", "semi-supervised learning"]}
{"id": "liDADADifferentiableAutomatic2020", "title": "DADA: Differentiable Automatic Data Augmentation", "abstract": "Data augmentation (DA) techniques aim to increase data variability, and thus train deep networks with better generalisation. The pioneering AutoAugment automated the search for optimal DA policies with reinforcement learning. However, AutoAugment is extremely computationally expensive, limiting its wide applicability. Followup works such as Population Based Augmentation (PBA) and Fast AutoAugment improved efficiency, but their optimization speed remains a bottleneck. In this paper, we propose Differentiable Automatic Data Augmentation (DADA) which dramatically reduces the cost. DADA relaxes the discrete DA policy selection to a differentiable optimization problem via Gumbel-Softmax. In addition, we introduce an unbiased gradient estimator, RELAX, leading to an efficient and effective one-pass optimization strategy to learn an efficient and accurate DA policy. We conduct extensive experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate the value of Auto DA in pre-training for downstream detection problems. Results show our DADA is at least one order of magnitude faster than the state-of-the-art while achieving very comparable accuracy. The code is available at https://github.com/VDIGPKU/DADA.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "liDADADifferentiableAutomatic2020a", "title": "DADA: Differentiable Automatic Data Augmentation", "abstract": "Data augmentation (DA) techniques aim to increase data variability, and thus train deep networks with better generalisation. The pioneering AutoAugment automated the search for optimal DA policies with reinforcement learning. However, AutoAugment is extremely computationally expensive, limiting its wide applicability. Followup works such as Population Based Augmentation (PBA) and Fast AutoAugment improved efficiency, but their optimization speed remains a bottleneck. In this paper, we propose Differentiable Automatic Data Augmentation (DADA) which dramatically reduces the cost. DADA relaxes the discrete DA policy selection to a differentiable optimization problem via Gumbel-Softmax. In addition, we introduce an unbiased gradient estimator, RELAX, leading to an efficient and effective one-pass optimization strategy to learn an efficient and accurate DA policy. We conduct extensive experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate the value of Auto DA in pre-training for downstream detection problems. Results show our DADA is at least one order of magnitude faster than the state-of-the-art while achieving very comparable accuracy. The code is available at https://github.com/VDIGPKU/DADA.", "venue": "arXiv", "keywords": []}
{"id": "liDeeperBroaderArtier2017", "title": "Deeper, Broader and Artier Domain Generalization", "abstract": "The problem of domain generalization is to learn from multiple training domains, and extract a domain-agnostic model that can then be applied to an unseen domain. Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training. For example recognition in sketch images, which are distinctly more abstract and rarer than photos. Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on alleviating the dataset bias where both problems of domain distinctiveness and data sparsity can be minimal. We argue that these benchmarks are overly straightforward, and show that simple deep learning baselines perform surprisingly well on them. In this paper, we make two main contributions: Firstly, we build upon the favorable domain shift-robust properties of deep learning methods, and develop a low-rank parameterized CNN model for end-to-end DG learning. Secondly, we develop a DG benchmark dataset covering photo, sketch, cartoon and painting domains. This is both more practically relevant, and harder (bigger domain shift) than existing benchmarks. The results show that our method outperforms existing DG alternatives, and our dataset provides a more significant DG challenge to drive future research.", "venue": "arXiv", "keywords": ["bias sources", "dataset debut", "domain generalization"]}
{"id": "liDemystifyingNeuralStyle2017", "title": "Demystifying Neural Style Transfer", "abstract": "Neural Style Transfer [Gatys et al., 2016] has recently demonstrated very exciting results which catches eyes in both academia and industry. Despite the amazing results, the principle of neural style transfer, especially why the Gram matrices could represent style remains unclear. In this paper, we propose a novel interpretation of neural style transfer by treating it as a domain adaptation problem. Specifically, we theoretically show that matching the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. Thus, we argue that the essence of neural style transfer is to match the feature distributions between the style images and the generated images. To further support our standpoint, we experiment with several other distribution alignment methods, and achieve appealing results. We believe this novel interpretation connects these two important research fields, and could enlighten future researches.", "venue": "arXiv", "keywords": ["generalization quantification", "style transfer", "surveys"]}
{"id": "liDiffStylerDiffusionbasedLocalized2024", "title": "DiffStyler: Diffusion-based Localized Image Style Transfer", "abstract": "Image style transfer aims to imbue digital imagery with the distinctive attributes of style targets, such as colors, brushstrokes, shapes, whilst concurrently preserving the semantic integrity of the content. Despite the advancements in arbitrary style transfer methods, a prevalent challenge remains the delicate equilibrium between content semantics and style attributes. Recent developments in large-scale text-to-image diffusion models have heralded unprecedented synthesis capabilities, albeit at the expense of relying on extensive and often imprecise textual descriptions to delineate artistic styles. Addressing these limitations, this paper introduces DiffStyler, a novel approach that facilitates efficient and precise arbitrary image style transfer. DiffStyler lies the utilization of a text-to-image Stable Diffusion model-based LoRA to encapsulate the essence of style targets. This approach, coupled with strategic cross-LoRA feature and attention injection, guides the style transfer process. The foundation of our methodology is rooted in the observation that LoRA maintains the spatial feature consistency of UNet, a discovery that further inspired the development of a mask-wise style transfer technique. This technique employs masks extracted through a pre-trained FastSAM model, utilizing mask prompts to facilitate feature fusion during the denoising process, thereby enabling localized style transfer that preserves the original image's unaffected regions. Moreover, our approach accommodates multiple style targets through the use of corresponding masks. Through extensive experimentation, we demonstrate that DiffStyler surpasses previous methods in achieving a more harmonious balance between content preservation and style integration.", "venue": "arXiv", "keywords": ["diffusion models", "favorite", "prompt-based", "style transfer"]}
{"id": "liDistributedJointlySparse2018", "title": "Distributed Jointly Sparse Multitask Learning Over Networks", "abstract": "Distributed data processing over networks has received a lot of attention due to its wide applicability. In this paper, we consider the multitask problem of in-network distributed estimation. For the multitask problem, the unknown parameter vectors (tasks) for different nodes can be different. Moreover, considering some real application scenarios, it is also assumed that there are some similarities among the tasks. Thus, the intertask cooperation is helpful to enhance the estimation performance. In this paper, we exploit an additional special characteristic of the vectors of interest, namely, joint sparsity, aiming to further enhance the estimation performance. A distributed jointly sparse multitask algorithm for the collaborative sparse estimation problem is derived. In addition, an adaptive intertask cooperation strategy is adopted to improve the robustness against the degree of difference among the tasks. The performance of the proposed algorithm is analyzed theoretically, and its effectiveness is verified by some simulations.", "venue": "IEEE Transactions on Cybernetics", "keywords": ["multi-task learning"]}
{"id": "liDistributedMultitaskLearning2017", "title": "Distributed Multi-task Learning for Sensor Network", "abstract": "A sensor in a sensor network is expected to be able to make prediction or decision utilizing the models learned from the data observed on this sensor. However, in the early stage of using a sensor, there may be not a lot of data available to train the model for this sensor. A solution is to leverage the observation data from other sensors which have similar conditions and models with the given sensor. We thus propose a novel distributed multi-task learning approach which incorporates neighborhood relations among sensors to learn multiple models simultaneously in which each sensor corresponds to one task. It may be not cheap for each sensor to transfer the observation data from other sensors; broadcasting the observation data of a sensor in the entire network is not satisfied for the reason of privacy protection; each sensor is expected to make real-time prediction independently from neighbor sensors. Therefore, this approach shares the model parameters as regularization terms in the objective function by assuming that neighbor sensors have similar model parameters. We conduct the experiments on two real datasets by predicting the temperature with the regression. They verify that our approach is effective, especially when the bias of an independent model which does not utilize the data from other sensors is high such as when there is not plenty of training data available.", "venue": "Machine Learning and Knowledge Discovery in Databases", "keywords": ["multi-task learning"]}
{"id": "liEfficientLongShortTemporal2024", "title": "Efficient Long-Short Temporal Attention Network for Unsupervised Video Object Segmentation", "abstract": "Unsupervised Video Object Segmentation (VOS) aims at identifying the contours of primary foreground objects in videos without any prior knowledge. However, previous methods do not fully use spatial-temporal context and fail to tackle this challenging task in real-time. This motivates us to develop an efficient LongShort Temporal Attention network (termed LSTA) for unsupervised VOS task from a holistic view. Specifically, LSTA consists of two dominant modules, i.e., Long Temporal Memory and Short Temporal Attention. The former captures the long-term global pixel relations of the past frames and the current frame, which models constantly present objects by encoding appearance pattern. Meanwhile, the latter reveals the short-term local pixel relations of one nearby frame and the current frame, which models moving objects by encoding motion pattern. To speedup the inference, the efficient projection and the locality-based sliding window are adopted to achieve nearly linear time complexity for the two light modules, respectively. Extensive empirical studies on several benchmarks have demonstrated promising performances of the proposed method with high efficiency.", "venue": "Pattern Recognition", "keywords": ["temporal consistency", "unsupervised da", "unsupervised learning"]}
{"id": "liGeneralizedRobustTestTime2023", "title": "Generalized Robust Test-Time Adaptation in Continuous Dynamic Scenarios", "abstract": "Test-time adaptation (TTA) adapts the pre-trained models to test distributions during the inference phase exclusively employing unlabeled test data streams, which holds great value for the deployment of models in real-world applications. Numerous studies have achieved promising performance on simplistic test streams, characterized by independently and uniformly sampled test data originating from a fixed target data distribution. However, these methods frequently prove ineffective in practical scenarios, where both continual covariate shift and continual label shift occur simultaneously, i.e., data and label distributions change concurrently and continually over time. In this study, a more challenging Practical Test-Time Adaptation (PTTA) setup is introduced, which takes into account the concurrent presence of continual covariate shift and continual label shift, and we propose a Generalized Robust Test-Time Adaptation (GRoTTA) method to effectively address the difficult problem. We start by steadily adapting the model through Robust Parameter Adaptation to make balanced predictions for test samples. To be specific, firstly, the effects of continual label shift are eliminated by enforcing the model to learn from a uniform label distribution and introducing recalibration of batch normalization to ensure stability. Secondly, the continual covariate shift is alleviated by employing a source knowledge regularization with the teacher-student model to update parameters. Considering the potential information in the test stream, we further refine the balanced predictions by Bias-Guided Output Adaptation, which exploits latent structure in the feature space and is adaptive to the imbalanced label distribution. Extensive experiments demonstrate GRoTTA outperforms the existing competitors by a large margin under PTTA setting, rendering it highly conducive for adoption in real-world applications.", "venue": "arXiv", "keywords": ["continual da", "domain adaptation", "knowledge distillation", "test-time da"]}
{"id": "liIntraSourceStyleAugmentation2023", "title": "Intra-Source Style Augmentation for Improved Domain Generalization", "abstract": "The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an intra-source style augmentation (ISSA) method to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image preserving its semantic layout through noise prediction. Random masking of the estimated noise enables the style mixing capability of our model, i.e. it allows to alter the global appearance without affecting the semantic layout of an image. Using the proposed masked noise encoder to randomize style and content combinations in the training set, ISSA effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to 12.4% mIoU improvements on driving-scene semantic segmentation under different types of data shifts, i.e., changing geographic locations, adverse weather conditions, and day to night. ISSA is model-agnostic and straightforwardly applicable with CNNs and Transformers. It is also complementary to other domain generalization techniques, e.g., it improves the recent state-of-the-art solution RobustNet by 3% mIoU in Cityscapes to Dark Zu . Code is available at https://github.com/boschresearch/ISSA.", "venue": "arXiv", "keywords": ["domain generalization", "feature pyramids", "gans", "promising", "segformer", "semantic segmentation", "style transfer"]}
{"id": "lijoiPitmanYorMultinomialProcess", "title": "The Pitman-Yor Multinomial Process for Mixture Modeling", "abstract": "Discrete nonparametric priors play a central role in a variety of Bayesian procedures, most notably when used to model latent features as in clustering, mixtures and curve fitting. They are effective and well developed tools, though their infinite-dimensionality is unsuited to several applications. If one confines oneself to a finite-dimensional simplex, there are few nonparametric priors beyond the traditional Dirichlet-multinomial process, which is mainly motivated by conjugacy. Here we introduce an alternative based on the Pitman--Yor process, which ensures greater flexibility while preserving analytical tractability. Urn schemes and posterior characterizations are obtained in closed form, leading to exact sampling methods. In addition, our proposal can be used to accurately approximate the infinite-dimensional Pitman--Yor process, improving over existing truncation-based approaches. An application to convex mixture regression for quantitative risk assessment serves as an illustration of our results and allows comparisons with existing methodologies.", "venue": "", "keywords": ["latent mixture models"]}
{"id": "liLaplacianSteeredNeuralStyle2017", "title": "Laplacian-Steered Neural Style Transfer", "abstract": "Neural Style Transfer based on Convolutional Neural Networks (CNN) aims to synthesize a new image that retains the high-level structure of a content image, rendered in the low-level texture of a style image. This is achieved by constraining the new image to have high-level CNN features similar to the content image, and lower-level CNN features similar to the style image. However in the traditional optimization objective, low-level features of the content image are absent, and the low-level features of the style image dominate the low-level detail structures of the new image. Hence in the synthesized image, many details of the content image are lost, and a lot of inconsistent and unpleasing artifacts appear. As a remedy, we propose to steer image synthesis with a novel loss function: the Laplacian loss. The Laplacian matrix (``Laplacian'' in short), produced by a Laplacian operator, is widely used in computer vision to detect edges and contours. The Laplacian loss measures the difference of the Laplacians, and correspondingly the difference of the detail structures, between the content image and a new image. It is flexible and compatible with the traditional style transfer constraints. By incorporating the Laplacian loss, we obtain a new optimization objective for neural style transfer named Lapstyle. Minimizing this objective will produce a stylized image that better preserves the detail structures of the content image and eliminates the artifacts. Experiments show that Lapstyle produces more appealing stylized images with less artifacts, without compromising their ``stylishness''.", "venue": "Proceedings of the 25th ACM International Conference on Multimedia", "keywords": ["critical citation", "style transfer"]}
{"id": "liLearningGeneralizeMetaLearning2017", "title": "Learning to Generalize: Meta-Learning for Domain Generalization", "abstract": "Domain shift refers to the well known problem that a model trained in one source domain performs poorly when applied to a target domain with different statistics. Generalization\\ (DG) techniques attempt to alleviate this issue by producing models which by design generalize well to novel testing domains. We propose a novel -learning\\ method for domain generalization. Rather than designing a specific model that is robust to domain shift as in most previous DG work, we propose a model agnostic training procedure for DG. Our algorithm simulates train/test domain shift during training by synthesizing virtual testing domains within each mini-batch. The meta-optimization objective requires that steps to improve training domain performance should also improve testing domain performance. This meta-learning procedure trains models with good generalization ability to novel domains. We evaluate our method and achieve state of the art results on a recent cross-domain image classification benchmark, as well demonstrating its potential on two classic reinforcement learning tasks.", "venue": "arXiv", "keywords": ["domain generalization", "meta-learning"]}
{"id": "liLearningLinearTransformations2018", "title": "Learning Linear Transformations for Fast Arbitrary Style Transfer", "abstract": "Given a random pair of images, an arbitrary style transfer method extracts the feel from the reference image to synthesize an output based on the look of the other content image. Recent arbitrary style transfer methods transfer second order statistics from reference image onto content image via a multiplication between content image features and a transformation matrix, which is computed from features with a pre-determined algorithm. These algorithms either require computationally expensive operations, or fail to model the feature covariance and produce artifacts in synthesized images. Generalized from these methods, in this work, we derive the form of transformation matrix theoretically and present an arbitrary style transfer approach that learns the transformation matrix with a feed-forward network. Our algorithm is highly efficient yet allows a flexible combination of multi-level styles while preserving content affinity during style transfer process. We demonstrate the effectiveness of our approach on four tasks: artistic style transfer, video and photo-realistic style transfer as well as domain adaptation, including comparisons with the state-of-the-art methods.", "venue": "arXiv", "keywords": ["affinity modeling", "style transfer"]}
{"id": "liMATMaskAwareTransformer2022", "title": "MAT: Mask-Aware Transformer for Large Hole Image Inpainting", "abstract": "", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["image inpainting", "image-to-image", "transformers"]}
{"id": "liMetaParsingNetworks2020", "title": "Meta Parsing Networks: Towards Generalized Few-shot Scene Parsing with Adaptive Metric Learning", "abstract": "Recent progress in few-shot segmentation usually aims at performing novel object segmentation using a few annotated examples as guidance. In this work, we advance this few-shot segmentation paradigm towards a more challenging yet general scenario, i.e., Generalized Few-shot Scene Parsing (GFSP). In this task, we take a fully annotated image as guidance to segment all pixels in a query image. Our mission is to study a generalizable and robust segmentation network from the meta-learning perspective so that both seen and unseen categories can be correctly recognized. Different from previous practices, this task performs segmentation on a joint label space consisting of both previously seen and novel categories. Moreover, pixels from these multiple categories need to be simultaneously taken into account, which is actually not well explored before. Accordingly, we present Meta Parsing Networks (MPNet) to better exploit the guidance information in the support set. Our MPNet contains two basic modules, i.e., the Adaptive Deep Metric Learning (ADML) module and the Contrastive Inter-class Distraction (CID) module. Specially, the ADML takes the annotated pixels from the support image as the guidance and adaptively produces high-quality prototypes for learning a deep comparison metric. In addition, MPNet further introduces the CID module learning to enlarge the feature discrepancy of different categories in the embedding space, leading the MPNet to generate more discriminative feature embeddings. We conduct experiments on two newly constructed benchmarks, i.e., GFSP-Cityscapes and GFSP-Pascal-Context. Extensive ablation studies well demonstrate the effectiveness and generalization ability of our MPNet.", "venue": "Proceedings of the 28th ACM International Conference on Multimedia", "keywords": []}
{"id": "liMethodBuildingDetection2023", "title": "Method of Building Detection in Optical Remote Sensing Images Based on SegFormer", "abstract": "An appropriate detection network is required to extract building information in remote sensing images and to relieve the issue of poor detection effects resulting from the deficiency of detailed features. Firstly, we embed a transposed convolution sampling module fusing multiple normalization activation layers in the decoder based on the SegFormer network. This step alleviates the issue of missing feature semantics by adding holes and fillings, cascading multiple normalizations and activation layers to hold back over-fitting regularization expression and guarantee steady feature parameter classification. Secondly, the atrous spatial pyramid pooling decoding module is fused to explore multi-scale contextual information and to overcome issues such as the loss of detailed information on local buildings and the lack of long-distance information. Ablation experiments and comparison experiments are performed on the remote sensing image AISD, MBD, and WHU dataset. The robustness and validity of the improved mechanism are demonstrated by control groups of ablation experiments. In comparative experiments with the HRnet, PSPNet, U-Net, DeepLabv3+ networks, and the original detection algorithm, the mIoU of the AISD, the MBD, and the WHU dataset is enhanced by 17.68%, 30.44%, and 15.26%, respectively. The results of the experiments show that the method of this paper is superior to comparative methods such as U-Net. Furthermore, it is better for integrity detection of building edges and reduces the number of missing and false detections.", "venue": "Sensors", "keywords": ["feature pyramids", "segformer"]}
{"id": "limFastAutoAugment2019", "title": "Fast AutoAugment", "abstract": "Data augmentation is an essential technique for improving generalization ability of deep learning models. Recently, AutoAugment has been proposed as an algorithm to automatically search for augmentation policies from a dataset and has significantly enhanced performances on many image recognition tasks. However, its search method requires thousands of GPU hours even for a relatively small dataset. In this paper, we propose an algorithm called Fast AutoAugment that finds effective augmentation policies via a more efficient search strategy based on density matching. In comparison to AutoAugment, the proposed algorithm speeds up the search time by orders of magnitude while achieves comparable performances on image recognition tasks with various models and datasets including CIFAR-10, CIFAR-100, SVHN, and ImageNet.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "linAcceleratingDistributedOnline2020", "title": "Accelerating Distributed Online Meta-Learning via Multi-Agent Collaboration under Limited Communication", "abstract": "Online meta-learning is emerging as an enabling technique for achieving edge intelligence in the IoT ecosystem. Nevertheless, to learn a good meta-model for within-task fast adaptation, a single agent alone has to learn over many tasks, and this is the so-called 'cold-start' problem. Observing that in a multi-agent network the learning tasks across different agents often share some model similarity, we ask the following fundamental question: \"Is it possible to accelerate the online meta-learning across agents via limited communication and if yes how much benefit can be achieved? \" To answer this question, we propose a multi-agent online meta-learning framework and cast it as an equivalent two-level nested online convex optimization (OCO) problem. By characterizing the upper bound of the agent-task-averaged regret, we show that the performance of multi-agent online meta-learning depends heavily on how much an agent can benefit from the distributed network-level OCO for meta-model updates via limited communication, which however is not well understood. To tackle this challenge, we devise a distributed online gradient descent algorithm with gradient tracking where each agent tracks the global gradient using only one communication step with its neighbors per iteration, and it results in an average regret \\ per agent, indicating that a factor of \\ speedup over the optimal single-agent regret \\ after \\ iterations, where \\ is the number of agents. Building on this sharp performance speedup, we next develop a multi-agent online meta-learning algorithm and show that it can achieve the optimal task-average regret at a faster rate of \\ via limited communication, compared to single-agent online meta-learning. Extensive experiments corroborate the theoretic results.", "venue": "arXiv", "keywords": []}
{"id": "linDataAugmentationPolicy", "title": "Data Augmentation with Policy Optimization", "abstract": "Recent work by Google Brain has shown that Reinforcement Learning could be used to automate the process of data augmentation for image classification. Specifically, following successes in applying Reinforcement Learning (RL) to design optimal neural network architectures with Neural Architecture Search (NAS) , the authors have continued their work to meta-learn autonomous data augmentation policies using RL using a RNN as controller . The novel task is formulated as a discrete search problem, where the goal is to find groups of transformations that yield the highest improvement in the performance of an image classifier from an extensive pre-defined list of possible transformations (crop, flip, rotate, shear, etc.). The reward is obtained as the loss function yielded when the image classifier, trained using the augmented data, is tested on a validation set with a child model. In this project, we initially aimed at extending their work to the problem of image segmentation. However, due to the extensive scope of the project's preassumed knowledge and computational power, we instead focused on implementing and improving their closed-source algorithm in the context of image classification.", "venue": "", "keywords": ["auto-augmentation policies", "ppo", "reinforcement learning"]}
{"id": "linDraftingRevisionLaplacian2021", "title": "Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality Artistic Style Transfer", "abstract": "Artistic style transfer aims at migrating the style from an example image to a content image. Currently, optimization-based methods have achieved great stylization quality, but expensive time cost restricts their practical applications. Meanwhile, feed-forward methods still fail to synthesize complex style, especially when holistic global and local patterns exist. Inspired by the common painting process of drawing a draft and revising the details, we introduce a novel feed-forward method named Laplacian Pyramid Network (LapStyle). LapStyle first transfers global style patterns in low-resolution via a Drafting Network. It then revises the local details in high-resolution via a Revision Network, which hallucinates a residual image according to the draft and the image textures extracted by Laplacian filtering. Higher resolution details can be easily generated by stacking Revision Networks with multiple Laplacian pyramid levels. The final stylized image is obtained by aggregating outputs of all pyramid levels. %We also introduce a patch discriminator to better learn local patterns adversarially. Experiments demonstrate that our method can synthesize high quality stylized images in real time, where holistic style patterns are properly transferred.", "venue": "arXiv", "keywords": ["feature pyramids", "style transfer"]}
{"id": "linFocalLossDense2018", "title": "Focal Loss for Dense Object Detection", "abstract": "The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.", "venue": "arXiv", "keywords": ["foundational", "loss functions"]}
{"id": "linLocalPatchAutoAugment2021", "title": "Local Patch AutoAugment with Multi-Agent Collaboration", "abstract": "Data augmentation (DA) plays a critical role in improving the generalization of deep learning models. Recent works on automatically searching for DA policies from data have achieved great success. However, existing automated DA methods generally perform the search at the image level, which limits the exploration of diversity in local regions. In this paper, we propose a more fine-grained automated DA approach, dubbed Patch AutoAugment, to divide an image into a grid of patches and search for the joint optimal augmentation policies for the patches. We formulate it as a multi-agent reinforcement learning (MARL) problem, where each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. We show the effectiveness of our method on multiple benchmark datasets of image classification and fine-grained image recognition (e.g., CIFAR10, CIFAR-100, ImageNet, CUB-200-2011, Stanford Cars and FGVC-Aircraft). Extensive experiments demonstrate that our method outperforms the state-of-theart DA methods while requiring fewer computational resources. The code is available at https://github.com/LinShiqi047/PatchAutoAugment.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "collaborative learning"]}
{"id": "linOnlineHyperparameterLearning2019", "title": "Online Hyper-parameter Learning for Auto-Augmentation Strategy", "abstract": "Data augmentation is critical to the success of modern deep learning techniques. In this paper, we propose Online Hyper-parameter Learning for Auto-Augmentation (OHL-Auto-Aug), an economical solution that learns the augmentation policy distribution along with network training. Unlike previous methods on auto-augmentation that search augmentation strategies in an offline manner, our method formulates the augmentation policy as a parameterized probability distribution, thus allowing its parameters to be optimized jointly with network parameters. Our proposed OHL-Auto-Aug eliminates the need of re-training and dramatically reduces the cost of the overall search process, while establishes significantly accuracy improvements over baseline models. On both CIFAR-10 and ImageNet, our method achieves remarkable on search accuracy, 60x faster on CIFAR-10 and 24x faster on ImageNet, while maintaining competitive accuracies.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "meta-learning", "online da", "promising"]}
{"id": "linOnlineHyperparameterLearning2019a", "title": "Online Hyper-parameter Learning for Auto-Augmentation Strategy", "abstract": "Data augmentation is critical to the success of modern deep learning techniques. In this paper, we propose Online Hyper-parameter Learning for Auto-Augmentation (OHL-Auto-Aug), an economical solution that learns the augmentation policy distribution along with network training. Unlike previous methods on auto-augmentation that search augmentation strategies in an offline manner, our method formulates the augmentation policy as a parameterized probability distribution, thus allowing its parameters to be optimized jointly with network parameters. Our proposed OHL-Auto-Aug eliminates the need of re-training and dramatically reduces the cost of the overall search process, while establishes significantly accuracy improvements over baseline models. On both CIFAR-10 and ImageNet, our method achieves remarkable on search accuracy, 60x faster on CIFAR-10 and 24x faster on ImageNet, while maintaining competitive accuracies.", "venue": "arXiv", "keywords": []}
{"id": "linSelectAugmentHierarchicalDeterministic2021", "title": "SelectAugment: Hierarchical Deterministic Sample Selection for Data Augmentation", "abstract": "Data augmentation (DA) has been widely investigated to facilitate model optimization in many tasks. However, in most cases, data augmentation is randomly performed for each training sample with a certain probability, which might incur content destruction and visual ambiguities. To eliminate this, in this paper, we propose an effective approach, dubbed SelectAugment, to select samples to be augmented in a deterministic and online manner based on the sample contents and the network training status. Specifically, in each batch, we first determine the augmentation ratio, and then decide whether to augment each training sample under this ratio. We model this process as a two-step Markov decision process and adopt Hierarchical Reinforcement Learning (HRL) to learn the augmentation policy. In this way, the negative effects of the randomness in selecting samples to augment can be effectively alleviated and the effectiveness of DA is improved. Extensive experiments demonstrate that our proposed SelectAugment can be adapted upon numerous commonly used DA methods, e.g., Mixup, Cutmix, AutoAugment, etc, and improve their performance on multiple benchmark datasets of image classification and fine-grained image recognition.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "linSelectAugmentHierarchicalDeterministic2021a", "title": "SelectAugment: Hierarchical Deterministic Sample Selection for Data Augmentation", "abstract": "Data augmentation (DA) has been widely investigated to facilitate model optimization in many tasks. However, in most cases, data augmentation is randomly performed for each training sample with a certain probability, which might incur content destruction and visual ambiguities. To eliminate this, in this paper, we propose an effective approach, dubbed SelectAugment, to select samples to be augmented in a deterministic and online manner based on the sample contents and the network training status. Specifically, in each batch, we first determine the augmentation ratio, and then decide whether to augment each training sample under this ratio. We model this process as a two-step Markov decision process and adopt Hierarchical Reinforcement Learning (HRL) to learn the augmentation policy. In this way, the negative effects of the randomness in selecting samples to augment can be effectively alleviated and the effectiveness of DA is improved. Extensive experiments demonstrate that our proposed SelectAugment can be adapted upon numerous commonly used DA methods, e.g., Mixup, Cutmix, AutoAugment, etc, and improve their performance on multiple benchmark datasets of image classification and fine-grained image recognition.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "linSPURIOUSFEATUREDIVERSIFICATION2024", "title": "SPURIOUS FEATURE DIVERSIFICATION IMPROVES OUT-OF-DISTRIBUTION GENERALIZATION", "abstract": "Generalization to out-of-distribution (OOD) data is a critical challenge in machine learning. Ensemble-based methods, like weight space ensembles that interpolate model parameters, have been shown to achieve superior OOD performance. However, the underlying mechanism for their effectiveness remains unclear.", "venue": "", "keywords": []}
{"id": "liOutlierDetectionBased2019", "title": "Outlier Detection Based on Robust Mahalanobis Distance and Its Application", "abstract": "Classical Mahalanobis distance is used as a method of detecting outliers, and is affected by outliers. Some robust Mahalanobis distance is proposed via the fast MCD estimator. However, the bias of the MCD estimator increases significantly as the dimension increases. In this paper, we propose the improved Mahalanobis distance based on a more robust Rocke estimator under high-dimensional data. The results of numerical simulation and empirical analysis show that our proposed method can better detect the outliers in the data than the above two methods when there are outliers in the data and the dimensions of data are very high.", "venue": "Open Journal of Statistics", "keywords": ["anomaly detection", "ood detection"]}
{"id": "liParsingAllAdverse2024", "title": "Parsing All Adverse Scenes: Severity-Aware Semantic Segmentation with Mask-Enhanced Cross-Domain Consistency", "abstract": "Although recent methods in Unsupervised Domain Adaptation (UDA) have achieved success in segmenting rainy or snowy scenes by improving consistency, they face limitations when dealing with more challenging scenarios like foggy and night scenes. We argue that these prior methods excessively focus on weather-specific features in adverse scenes, which exacerbates the existing domain gaps. To address this issue, we propose a new metric to evaluate the severity of all adverse scenes and offer a novel perspective that enables task unification across all adverse scenarios. Our method focuses on Severity, allowing our model to learn more consistent features and facilitate domain distribution alignment, thereby alleviating domain gaps. Unlike the vague descriptions of consistency in previous methods, we introduce Cross-domain Consistency, which is quantified using the Structure Similarity Index Measure (SSIM) to measure the distance between the source and target domains. Specifically, our unified model consists of two key modules: the Merging Style Augmentation Module (MSA) and the Severity Perception Mask Module (SPM). The MSA module transforms all adverse scenes into augmented scenes, effectively eliminating weather-specific features and enhancing Cross-domain Consistency. The SPM module incorporates a Severity Perception mechanism, guiding a Mask operation that enables our model to learn highly consistent features from the augmented scenes. Our unified framework, named PASS (Parsing All adverSe Scenes), achieves significant performance improvements over state-of-the-art methods on widely-used benchmarks for all adverse scenes. Notably, the performance of PASS is superior to Semi-Unified models and even surpasses weather-specific models.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "keywords": []}
{"id": "lippiCLAUDETTEAutomatedDetector2019", "title": "CLAUDETTE: An Automated Detector of Potentially Unfair Clauses in Online Terms of Service", "abstract": "Terms of service of on-line platforms too often contain clauses that are potentially unfair to the consumer. We present an experimental study where machine learning is employed to automatically detect such potentially unfair clauses. Results show that the proposed system could provide a valuable tool for lawyers and consumers alike.", "venue": "Artificial Intelligence and Law", "keywords": []}
{"id": "liRandomStyleTransfer2020", "title": "Random Style Transfer Based Domain Generalization Networks Integrating Shape and Spatial Information", "abstract": "Deep learning (DL)-based models have demonstrated good performance in medical image segmentation. However, the models trained on a known dataset often fail when performed on an unseen dataset collected from different centers, vendors and disease populations. In this work, we present a random style transfer network to tackle the domain generalization problem for multi-vendor and center cardiac image segmentation. Style transfer is used to generate training data with a wider distribution/ heterogeneity, namely domain augmentation. As the target domain could be unknown, we randomly generate a modality vector for the target modality in the style transfer stage, to simulate the domain shift for unknown domains. The model can be trained in a semisupervised manner by simultaneously optimizing a supervised segmentation and a unsupervised style translation objective. Besides, the framework incorporates the spatial information and shape prior of the target by introducing two regularization terms. We evaluated the proposed framework on 40 subjects from the M&Ms challenge2020, and obtained promising performance in the segmentation for data from unknown vendors and centers.", "venue": "arXiv", "keywords": ["adversarial learning", "domain randomization", "gans", "self-supervised learning", "style transfer"]}
{"id": "liSequentialLearningDomain2020", "title": "Sequential Learning for Domain Generalization", "abstract": "In this paper we propose a sequential learning framework for Domain Generalization (DG), the problem of training a model that is robust to domain shift by design. Various DG approaches have been proposed with different motivating intuitions, but they typically optimize for a single step of domain generalization -- training on one set of domains and generalizing to one other. Our sequential learning is inspired by the idea lifelong learning, where accumulated experience means that learning the \\ thing becomes easier than the \\ thing. In DG this means encountering a sequence of domains and at each step training to maximise performance on the next domain. The performance at domain \\ then depends on the previous \\ learning problems. Thus backpropagating through the sequence means optimizing performance not just for the next domain, but all following domains. Training on all such sequences of domains provides dramatically more `practice' for a base DG learner compared to existing approaches, thus improving performance on a true testing domain. This strategy can be instantiated for different base DG algorithms, but we focus on its application to the recently proposed Meta-Learning Domain generalization (MLDG). We show that for MLDG it leads to a simple to implement and fast algorithm that provides consistent performance improvement on a variety of DG benchmarks.", "venue": "arXiv", "keywords": ["continuous dg", "critical citation", "domain generalization", "lifelong learning", "meta-learning", "promising"]}
{"id": "liShapeTextureDebiasedNeural2021", "title": "Shape-Texture Debiased Neural Network Training", "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. Experiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "venue": "arXiv", "keywords": ["bias sources", "domain generalization", "representation learning", "texture transfer"]}
{"id": "liSurveySiameseNetwork2022", "title": "A Survey on Siamese Network: Methodologies, Applications, and Opportunities", "abstract": "Siamese network has obtained growing attention in real-life applications. In this survey, we present an comprehensive review on Siamese network from the aspects of methodologies, applications, and interesting topics for further exploration. We first introduce framework designs of Siamese network, followed by methodologies about learning with unlabeled data. Then, we review application scenarios in terms of classification and regression, together with relative methodologies. We also discuss the promising area of few-shot learning, followed by interesting topics about opportunities and challenges.", "venue": "IEEE Transactions on Artificial Intelligence", "keywords": ["feature engineering", "few-shot learning", "self-supervised learning", "semi-supervised learning", "siamese networks"]}
{"id": "liuAdaAttNRevisitAttention2021", "title": "AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer", "abstract": "", "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "keywords": ["instance normalization", "promising", "style transfer"]}
{"id": "liuAdaptiveDistributionCalibration2022", "title": "Adaptive Distribution Calibration for Few-Shot Learning via Optimal Transport", "abstract": "Few-shot learning (FSL) is a challenging task in the community of data mining but frequently appears in real-world applications. A popular strategy for FSL is transferring information from the domain with extensive amounts of data to the domain with few data. However, two main issues are still open for transfer learning: what kind of information and how much should be transferred. In this work, an Adaptive Distribution Calibration (ADC) is designed to adaptively transfer distribution informase classes for calibrating the biased distributions of novel classes. More specifically, ADC automatically determines the correlations between base classes and novel classes by considering the optimal transport among them. Then, ADC adaptively calibrates the distribution of each novel class according to its correlated base classes. More novel class data can be sampled from the calibrated distribution to train a robust classifier. Furthermore, we theoretically analyze the generalization error bound of the proposed ADC, which shows that the best hypothesis that considers both support and generated data performs at least as good as the best hypothesis learned on support data alone. This generalization error bound guarantees theoretically the effectiveness of the proposed method. Extensive experiments, including comparing with baselines(0.17% \\$ 2.6% improvement on different datasets compared to the next best), ablation studies and hyper-parameter analysis, have been conducted on three widely used FSL datasets (miniImageNet, tieredImageNet and CUB) to demonstrate the effectiveness of ADC.", "venue": "Information Sciences", "keywords": ["distribution estimation", "few-shot learning", "optimal transport"]}
{"id": "liuDataAugmentationTechnology2020", "title": "Data Augmentation Technology Driven By Image Style Transfer in Self-Driving Car Based on End-to-End Learning", "abstract": "With the advent of deep learning, self-driving schemes based on deep learning are becoming more and more popular. Robust perception-action models should learn from data with different scenarios and real behaviors, while current end-to-end model learning is generally limited to training of massive data, innovation of deep network architecture, and learning in-situ model in a simulation environment. Therefore, we introduce a new image style transfer method into data augmentation, and improve the diversity of limited data by changing the texture, contrast ratio and color of the image, and then it is extended to the scenarios that the model has been unobserved before. Inspired by rapid style transfer and artistic style neural algorithms, we propose an arbitrary style generation network architecture, including style transfer network, style learning network, style loss network and multivariate Gaussian distribution function. The style embedding vector is randomly sampled from the multivariate Gaussian distribution and linearly interpolated with the embedded vector predicted by the input image on the style learning network, which provides a set of normalization constants for the style transfer network, and finally realizes the diversity of the image style. In order to verify the effectiveness of the method, image classification and simulation experiments were performed separately. Finally, we built a small-sized smart car experiment platform, and apply the data augmentation technology based on image style transfer drive to the experiment of automatic driving for the first time. The experimental results show that: (1) The proposed scheme can improve the prediction accuracy of the end-to-end model and reduce the model's error accumulation; (2) the method based on image style transfer provides a new scheme for data augmentation technology, and also provides a solution for the high cost that many deep models rely heavily on a large number of label data.", "venue": "Computer Modeling in Engineering & Sciences", "keywords": ["style transfer"]}
{"id": "liuDecadesBattleDataset2024", "title": "A Decade's Battle on Dataset Bias: Are We There Yet?", "abstract": "We revisit the ``dataset classification'' experiment suggested by Torralba and Efros a decade ago , in the new era with largescale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be simply explained by memorization. We hope our discovery will inspire the community to rethink the issue involving dataset bias and model capabilities.", "venue": "arXiv", "keywords": ["bias sources"]}
{"id": "liuDirectDifferentiableAugmentation2021", "title": "Direct Differentiable Augmentation Search", "abstract": "Data augmentation has been an indispensable tool to improve the performance of deep neural networks, however the augmentation can hardly transfer among different tasks and datasets. Consequently, a recent trend is to adopt AutoML technique to learn proper augmentation policy without extensive hand-crafted tuning. In this paper, we propose an efficient differentiable search algorithm called Direct Differentiable Augmentation Search (DDAS). It exploits meta-learning with one-step gradient update and continuous relaxation to the expected training loss for efficient search. Our DDAS can achieve efficient augmentation search without relying on approximations such as Gumbel Softmax or second order gradient approximation. To further reduce the adverse effect of improper augmentations, we organize the search space into a two level hierarchy, in which we first decide whether to apply augmentation, and then determine the specific augmentation policy. On standard image classification benchmarks, our DDAS achieves state-of-the-art performance and efficiency tradeoff while reducing the search cost dramatically, e.g. 0.15 GPU hours for CIFAR-10. In addition, we also use DDAS to search augmentation for object detection task and achieve comparable performance with AutoAugment, while being 1000x faster.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "meta-learning"]}
{"id": "liuDistributedMachineLearning2022", "title": "From Distributed Machine Learning to Federated Learning: A Survey", "abstract": "In recent years, data and computing resources are typically distributed in the devices of end users, various regions or organizations. Because of laws or regulations, the distributed data and computing resources cannot be aggregated or directly shared among different regions or organizations for machine learning tasks. Federated learning emerges as an efficient approach to exploit distributed data and computing resources, so as to collaboratively train machine learning models. At the same time, federated learning obeys the laws and regulations and ensures data security and data privacy. In this paper, we provide a comprehensive survey of existing works for federated learning. First, we propose a functional architecture of federated learning systems and a taxonomy of related techniques. Second, we explain the federated learning systems from four aspects: diverse types of parallelism, aggregation algorithms, data communication, and the security of federated learning systems. Third, we present four widely used federated systems based on the functional architecture. Finally, we summarize the limitations and propose future research directions.", "venue": "Knowledge and Information Systems", "keywords": []}
{"id": "liuDistributedMultiTaskRelationship2017", "title": "Distributed Multi-Task Relationship Learning", "abstract": "Multi-task learning aims to learn multiple tasks jointly by exploiting their relatedness to improve the generalization performance for each task. Traditionally, to perform multi-task learning, one needs to centralize data from all the tasks to a single machine. However, in many real-world applications, data of different tasks may be geo-distributed over different local machines. Due to heavy communication caused by transmitting the data and the issue of data privacy and security, it is impossible to send data of different task to a master machine to perform multi-task learning. Therefore, in this paper, we propose a distributed multi-task learning framework that simultaneously learns predictive models for each task as well as task relationships between tasks alternatingly in the parameter server paradigm. In our framework, we first offer a general dual form for a family of regularized multi-task relationship learning methods. Subsequently, we propose a communication-efficient primal-dual distributed optimization algorithm to solve the dual problem by carefully designing local subproblems to make the dual problem decomposable. Moreover, we provide a theoretical convergence analysis for the proposed algorithm, which is specific for distributed multi-task relationship learning. We conduct extensive experiments on both synthetic and real-world datasets to evaluate our proposed framework in terms of effectiveness and convergence.", "venue": "arXiv", "keywords": ["multi-task learning"]}
{"id": "liuDomainGeneralizationFeature2021", "title": "Domain Generalization via Feature Variation Decorrelation", "abstract": "Domain generalization aims to learn a model that generalizes to unseen target domains from multiple source domains. Various approaches have been proposed to address this problem by adversarial learning, meta-learning, and data augmentation. However, those methods have no guarantee for target domain generalization. Motivated by an observation that the class-irrelevant information of sample in the form of semantic variation would lead to negative transfer, we propose to linearly disentangle the variation out of sample in feature space and impose a novel class decorrelation regularization on the feature variation. By doing so, the model would focus on the high-level categorical concept for model prediction while ignoring the misleading clue from other variations (including domain changes). As a result, we achieve state-of-the-art performances over all of widely used domain generalization benchmarks, namely PACS, VLCS, Office-Home, and Digits-DG with large margins. Further analysis reveals our method could learn a better domain-invariant representation, and decorrelated feature variation could successfully capture semantic meaning.", "venue": "Proceedings of the 29th ACM International Conference on Multimedia", "keywords": ["domain generalization", "feature engineering"]}
{"id": "liuEfficientTrainingVisual", "title": "Efficient Training of Visual Transformers with Small Datasets", "abstract": "Visual Transformers (VTs) are emerging as an architectural paradigm alternative to Convolutional networks (CNNs). Differently from CNNs, VTs can capture global relations between image elements and they potentially have a larger representation capacity. However, the lack of the typical convolutional inductive bias makes these models more data hungry than common CNNs. In fact, some local properties of the visual domain which are embedded in the CNN architectural design, in VTs should be learned from samples. In this paper, we empirically analyse different VTs, comparing their robustness in a small training set regime, and we show that, despite having a comparable accuracy when trained on ImageNet, their performance on smaller datasets can be largely different. Moreover, we propose an auxiliary selfsupervised task which can extract additional information from images with only a negligible computational overhead. This task encourages the VTs to learn spatial relations within an image and makes the VT training much more robust when training data is scarce. Our task is used jointly with the standard (supervised) training and it does not depend on specific architectural choices, thus it can be easily plugged in the existing VTs. Using an extensive evaluation with different VTs and datasets, we show that our method can improve (sometimes dramatically) the final accuracy of the VTs. Our code is available at: https://github.com/ yhlleo/VTs-Drloc.", "venue": "", "keywords": ["self-supervised learning", "transformers"]}
{"id": "liuFuseFormerFusingFineGrained2021", "title": "FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting", "abstract": "Transformer, as a strong and flexible architecture for modelling long-range relations, has been widely explored in vision tasks. However, when used in video inpainting that requires fine-grained representation, existed method still suffers from yielding blurry edges in detail due to the hard patch splitting. Here we aim to tackle this problem by proposing FuseFormer, a Transformer model designed for video inpainting via fine-grained feature fusion based on novel Soft Split and Soft Composition operations. The soft split divides feature map into many patches with given overlapping interval. On the contrary, the soft composition operates by stitching different patches into a whole feature map where pixels in overlapping regions are summed up. These two modules are first used in tokenization before Transformer layers and de-tokenization after Transformer layers, for effective mapping between tokens and features. Therefore, sub-patch level information interaction is enabled for more effective feature propagation between neighboring patches, resulting in synthesizing vivid content for hole regions in videos. Moreover, in FuseFormer, we elaborately insert the soft composition and soft split into the feed-forward network, enabling the 1D linear layers to have the capability of modelling 2D structure. And, the sub-patch level feature fusion ability is further enhanced. In both quantitative and qualitative evaluations, our proposed FuseFormer surpasses state-of-the-art methods. We also conduct detailed analysis to examine its superiority. Code and pretrained models are available at https: //github.com/ruiliu-ai/FuseFormer.", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "keywords": ["image inpainting", "image-to-image", "temporal consistency", "transformers"]}
{"id": "liuInstantPhotorealisticStyle2023", "title": "Instant Photorealistic Style Transfer: A Lightweight and Adaptive Approach", "abstract": "In this paper, we propose an Instant Photorealistic Style Transfer (IPST) approach, designed to achieve instant photorealistic style transfer on super-resolution inputs without the need for pre-training on pair-wise datasets or imposing extra constraints. Our method utilizes a lightweight StyleNet to enable style transfer from a style image to a content image while preserving non-color information. To further enhance the style transfer process, we introduce an instance-adaptive optimization to prioritize the photorealism of outputs and accelerate the convergence of the style network, leading to a rapid training completion within seconds. Moreover, IPST is well-suited for multi-frame style transfer tasks, as it retains temporal and multi-view consistency of the multi-frame inputs such as video and Neural Radiance Field (NeRF). Experimental results demonstrate that IPST requires less GPU memory usage, offers faster multi-frame transfer speed, and generates photorealistic outputs, making it a promising solution for various photorealistic transfer applications.", "venue": "arXiv", "keywords": ["neural radiance fields", "style transfer"]}
{"id": "liuKANKolmogorovArnoldNetworks2024", "title": "KAN: Kolmogorov-Arnold Networks", "abstract": "Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.", "venue": "arXiv", "keywords": []}
{"id": "liuLearningSmoothNeural2022", "title": "Learning Smooth Neural Functions via Lipschitz Regularization", "abstract": "Neural implicit fields have recently emerged as a useful representation for 3D shapes. These fields are commonly represented as neural networks which map latent descriptors and 3D coordinates to implicit function values. The latent descriptor of a neural field acts as a deformation handle for the 3D shape it represents. Thus, smoothness with respect to this descriptor is paramount for performing shape-editing operations. In this work, we introduce a novel regularization designed to encourage smooth latent spaces in neural fields by penalizing the upper bound on the field's Lipschitz constant. Compared with prior Lipschitz regularized networks, ours is computationally fast, can be implemented in four lines of code, and requires minimal hyperparameter tuning for geometric applications. We demonstrate the effectiveness of our approach on shape interpolation and extrapolation as well as partial shape reconstruction from 3D point clouds, showing both qualitative and quantitative improvements over existing state-of-the-art and non-regularized baselines.", "venue": "arXiv", "keywords": ["lipschitz-constraints"]}
{"id": "liUncertaintyModelingOutofDistribution2022", "title": "Uncertainty Modeling for Out-of-Distribution Generalization", "abstract": "Though remarkable progress has been achieved in various vision tasks, deep neural networks still suffer obvious performance degradation when tested in out-of-distribution scenarios. We argue that the feature statistics (mean and standard deviation), which carry the domain characteristics of the training data, can be properly manipulated to improve the generalization ability of deep learning models. Common methods often consider the feature statistics as deterministic values measured from the learned features and do not explicitly consider the uncertain statistics discrepancy caused by potential domain shifts during testing. In this paper, we improve the network generalization ability by modeling the uncertainty of domain shifts with synthesized feature statistics during training. Specifically, we hypothesize that the feature statistic, after considering the potential uncertainties, follows a multivariate Gaussian distribution. Hence, each feature statistic is no longer a deterministic value, but a probabilistic point with diverse distribution possibilities. With the uncertain feature statistics, the models can be trained to alleviate the domain perturbations and achieve better robustness against potential domain shifts. Our method can be readily integrated into networks without additional parameters. Extensive experiments demonstrate that our proposed method consistently improves the network generalization ability on multiple vision tasks, including image classification, semantic segmentation, and instance retrieval. The code can be available at https://github.com/lixiaotong97/DSU.", "venue": "arXiv", "keywords": ["domain generalization", "representation learning", "semantic segmentation", "uncertainty quantification"]}
{"id": "liUniversalStyleTransfer2017", "title": "Universal Style Transfer via Feature Transforms", "abstract": "Universal style transfer aims to transfer arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring.", "venue": "arXiv", "keywords": ["augmentation stability", "feature whitening", "feature-level augmentation", "instance normalization", "promising", "style transfer"]}
{"id": "liuOpenCompoundDomain2020", "title": "Open Compound Domain Adaptation", "abstract": "A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (e.g., sunny weather) for achieving high performance on the test data in a target domain (e.g., rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (e.g., changes in weather). We study an open compound domain adaptation (OCDA) problem, in which the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the model's agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.", "venue": "arXiv", "keywords": ["curriculum learning", "domain adaptation", "semantic segmentation"]}
{"id": "liuRobustOnlineDomain2024", "title": "Towards Robust Online Domain Adaptive Semantic Segmentation under Adverse Weather Conditions", "abstract": "Online Domain Adaptation (OnDA) is designed to handle unforeseeable domain changes at minimal cost that occur during the deployment of the model, lacking clear boundaries between the domain, such as sudden weather events. However, existing OnDA methods that rely solely on the model itself to adapt to the current domain often misidentify ambiguous classes amidst continuous domain shifts and pass on this erroneous knowledge to the next domain. To tackle this, we propose \\, a framework, which dynamically detects domain shifts and adjusts hyper-parameters to minimize training costs and error propagation. Specifically, we introduce the \\ ( Mask\\) strategy, which dynamically selects highly disturbed regions and masks these regions, mitigating error accumulation in ambiguous classes and enhancing the model's robustness against external noise in dynamic natural environments. Additionally, we present the \\ ( Mix\\), a domain-aware mix method that augments target domain scenes with class-level source buffers, reducing the high uncertainty and noisy labels, thereby accelerating adaptation and offering a more efficient solution for online domain adaptation. Our approach outperforms state-of-the-art methods on widely used OnDA benchmarks while maintaining approximately 40 frames per second (FPS).", "venue": "arXiv", "keywords": ["domain adaptation", "online da", "semantic segmentation"]}
{"id": "liuSourceFreeDomainAdaptation2021", "title": "Source-Free Domain Adaptation for Semantic Segmentation", "abstract": "Unsupervised Domain Adaptation (UDA) can tackle the challenge that convolutional neural network (CNN)-based approaches for semantic segmentation heavily rely on the pixel-level annotated data, which is labor-intensive. However, existing UDA approaches in this regard inevitably require the full access to source datasets to reduce the gap between the source and target domains during model adaptation, which are impractical in the real scenarios where the source datasets are private, and thus cannot be released along with the well-trained source models. To cope with this issue, we propose a source-free domain adaptation framework for semantic segmentation, namely SFDA, in which only a well-trained source model and an unlabeled target domain dataset are available for adaptation. SFDA not only enables to recover and preserve the source domain knowledge from the source model via knowledge transfer during model adaptation, but also distills valuable information from the target domain for self-supervised learning. The pixeland patch-level optimization objectives tailored for semantic segmentation are seamlessly integrated in the framework. The extensive experimental results on numerous benchmark datasets highlight the effectiveness of our framework against the existing UDA approaches relying on source data.", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["knowledge distillation", "model adaptation", "self-supervised learning", "semantic segmentation", "source-free da"]}
{"id": "liuStyleRFZeroshot3D2023", "title": "StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields", "abstract": "3D style transfer aims to render stylized novel views of a 3D scene with multi-view consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which high-fidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner.", "venue": "arXiv", "keywords": ["3d scenes", "3d style transfer", "neural radiance fields"]}
{"id": "liuSwinTransformerHierarchical2021", "title": "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows", "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with . The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at ://github.com/microsoft/Swin-Transformer\\.", "venue": "arXiv", "keywords": ["foundational", "semantic segmentation", "transformers"]}
{"id": "liuSwinTransformerV22022", "title": "Swin Transformer V2: Scaling Up Capacity and Resolution", "abstract": "Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536\\ 1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at ://github.com/microsoft/Swin-Transformer\\.", "venue": "arXiv", "keywords": ["transformers"]}
{"id": "liVivoChangesNanoapatite2019", "title": "In Vivo Changes of Nanoapatite Crystals during Bone Reconstruction and the Differences with Native Bone Apatite", "abstract": "Hydroxyapatite (HA) plays an important role in clinical bone repair. However, it remains a challenge to accurately determine its changes during bone reconstruction and to identify its differences from native bone apatite. Here, terbium (Tb) doped uniform HA nanocrystals were implanted into bone tissue and compared with native bone apatite. These comparisons demonstrated the occurrence of compositional and structural alteration of the implanted HA nanocrystals, and their gradual degradation, during bone reconstruction. They also revealed notable differences between HA nanocrystals and bone apatite crystals in crystal size, distribution pattern, and state of existence in bone tissue. Although synthetic HA nanocrystals could osteointegrate with bone tissue, they still seemed to be treated as foreign material in this tissue and thus were gradually degraded. These findings can help to identify and rethink the function of synthetic apatite and bone apatite, which will benefit future design and application of biomimetic bone repair materials.", "venue": "Science Advances", "keywords": []}
{"id": "liWeatherGANMultiDomain2021", "title": "Weather GAN: Multi-Domain Weather Translation Using Generative Adversarial Networks", "abstract": "In this paper, a new task is proposed, namely, weather translation, which refers to transferring weather conditions of the image from one category to another. It is important for photographic style transfer. Although lots of approaches have been proposed in traditional image translation tasks, few of them can handle the multi-category weather translation task, since weather conditions have rich categories and highly complex semantic structures. To address this problem, we develop a multi-domain weather translation approach based on generative adversarial networks (GAN), denoted as Weather GAN, which can achieve the transferring of weather conditions among sunny, cloudy, foggy, rainy and snowy. Specifically, the weather conditions in the image are determined by various weather-cues, such as cloud, blue sky, wet ground, etc. Therefore, it is essential for weather translation to focus the main attention on weathercues. To this end, the generator of Weather GAN is composed of an initial translation module, an attention module and a weather-cue segmentation module. The initial translation module performs global translation during generation procedure. The weather-cue segmentation module identifies the structure and exact distribution of weather-cues. The attention module learns to focus on the interesting areas of the image while keeping other areas unaltered. The final generated result is synthesized by these three parts. This approach suppresses the distortion and deformation caused by weather translation. our approach outperforms the state-of-the-arts has been shown by a large number of experiments and evaluations.", "venue": "arXiv", "keywords": ["adversarial learning", "adverse weather", "automotive occlusion", "gans", "generative augmentation"]}
{"id": "longArbitraryStyleTransfer", "title": "Arbitrary Style Transfer with Structure Enhancement by Combining the Global and Local Loss", "abstract": "Arbitrary style transfer generates an artistic image which combines the structure of a content image and the artistic style of the artwork by using only one trained network. The image representation used in this method contains content structure representation and the style patterns representation, which is usually the features representation of high-level in the pre-trained classification networks. However, the traditional classification networks were designed for classification which usually focus on high-level features and ignore other features. As the result, the stylized images distribute style elements evenly throughout the image and make the overall image structure unrecognizable. To solve this problem, we introduce a novel arbitrary style transfer method with structure enhancement by combining the global and local loss. The local structure details are represented by Lapstyle and the global structure is controlled by the image depth. Experimental results demonstrate that our method can generate higher-quality images with impressive visual effects on several common datasets, comparing with other state-of-the-art methods.", "venue": "", "keywords": ["style transfer"]}
{"id": "longArbitraryStyleTransfer2022", "title": "Arbitrary Style Transfer with Structure Enhancement by Combining the Global and Local Loss", "abstract": "Arbitrary style transfer generates an artistic image which combines the structure of a content image and the artistic style of the artwork by using only one trained network. The image representation used in this method contains content structure representation and the style patterns representation, which is usually the features representation of high-level in the pre-trained classification networks. However, the traditional classification networks were designed for classification which usually focus on high-level features and ignore other features. As the result, the stylized images distribute style elements evenly throughout the image and make the overall image structure unrecognizable. To solve this problem, we introduce a novel arbitrary style transfer method with structure enhancement by combining the global and local loss. The local structure details are represented by Lapstyle and the global structure is controlled by the image depth. Experimental results demonstrate that our method can generate higher-quality images with impressive visual effects on several common datasets, comparing with other state-of-the-art methods.", "venue": "arXiv", "keywords": ["promising", "style transfer"]}
{"id": "longLearningTransferableFeatures2015", "title": "Learning Transferable Features with Deep Adaptation Networks", "abstract": "Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.", "venue": "arXiv", "keywords": ["domain adaptation", "domain generalization", "feature engineering"]}
{"id": "loSpatioTemporalPixelLevelContrastive2023", "title": "Spatio-Temporal Pixel-Level Contrastive Learning-based Source-Free Domain Adaptation for Video Semantic Segmentation", "abstract": "Unsupervised Domain Adaptation (UDA) of semantic segmentation transfers labeled source knowledge to an unlabeled target domain by relying on accessing both the source and target data. However, the access to source data is often restricted or infeasible in real-world scenarios. Under the source data restrictive circumstances, UDA is less practical. To address this, recent works have explored solutions under the Source-Free Domain Adaptation (SFDA) setup, which aims to adapt a source-trained model to the target domain without accessing source data. Still, existing SFDA approaches use only image-level information for adaptation, making them sub-optimal in video applications. This paper studies SFDA for Video Semantic Segmentation (VSS), where temporal information is leveraged to address video adaptation. Specifically, we propose Spatio-Temporal Pixel-Level (STPL) contrastive learning, a novel method that takes full advantage of spatio-temporal information to tackle the absence of source data better. STPL explicitly learns semantic correlations among pixels in the spatio-temporal space, providing strong self-supervision for adaptation to the unlabeled target domain. Extensive experiments show that STPL achieves state-of-the-art performance on VSS benchmarks compared to current UDA and SFDA approaches. Code is available at: https://github.com/shaoyuanlo/STPL", "venue": "arXiv", "keywords": ["contrastive learning", "self-supervised learning", "semantic segmentation", "source-free da", "temporal consistency", "unsupervised da"]}
{"id": "louizosMultiplicativeNormalizingFlows2017", "title": "Multiplicative Normalizing Flows for Variational Bayesian Neural Networks", "abstract": "We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.", "venue": "arXiv", "keywords": ["distribution estimation", "normalizing flows", "variational inference"]}
{"id": "luanDeepPhotoStyle2017", "title": "Deep Photo Style Transfer", "abstract": "", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["style transfer"]}
{"id": "luClosedformSolutionUniversal2019", "title": "A Closed-form Solution to Universal Style Transfer", "abstract": "Universal style transfer tries to explicitly minimize the losses in feature space, thus it does not require training on any pre-defined styles. It usually uses different layers of VGG network as the encoders and trains several decoders to invert the features into images. Therefore, the effect of style transfer is achieved by feature transform. Although plenty of methods have been proposed, a theoretical analysis of feature transform is still missing. In this paper, we first propose a novel interpretation by treating it as the optimal transport problem. Then, we demonstrate the relations of our formulation with former works like Adaptive Instance Normalization (AdaIN) and Whitening and Coloring Transform (WCT). Finally, we derive a closed-form solution named Optimal Style Transfer (OST) under our formulation by additionally considering the content loss of Gatys. Comparatively, our solution can preserve better structure and achieve visually pleasing results. It is simple yet effective and we demonstrate its advantages both quantitatively and qualitatively. Besides, we hope our theoretical analysis can inspire future works in neural style transfer. Code is available at https://github.com/lu-m13/OptimalStyleTransfer.", "venue": "arXiv", "keywords": ["feature-level augmentation", "optimal transport", "style transfer"]}
{"id": "lugmayrSRFlowLearningSuperResolution2020", "title": "SRFlow: Learning the Super-Resolution Space with Normalizing Flow", "abstract": "Super-resolution is an ill-posed problem, since it allows for multiple predictions for a given low-resolution image. This fundamental fact is largely ignored by state-of-the-art deep learning based approaches. These methods instead train a deterministic mapping using combinations of reconstruction and adversarial losses. In this work, we therefore propose SRFlow: a normalizing flow based super-resolution method capable of learning the conditional distribution of the output given the lowresolution input. Our model is trained in a principled manner using a single loss, namely the negative log-likelihood. SRFlow therefore directly accounts for the ill-posed nature of the problem, and learns to predict diverse photo-realistic high-resolution images. Moreover, we utilize the strong image posterior learned by SRFlow to design flexible image manipulation techniques, capable of enhancing super-resolved images by, e.g., transferring content from other images. We perform extensive experiments on faces, as well as on super-resolution in general. SRFlow outperforms state-of-the-art GAN-based approaches in terms of both PSNR and perceptual quality metrics, while allowing for diversity through the exploration of the space of super-resolved solutions. Code: git.io/Jfpyu.", "venue": "Computer Vision -- ECCV 2020", "keywords": ["image synthesis", "normalizing flows"]}
{"id": "lugosiYXU06SYREDUHZHANG12SYREDU", "title": "YXU06@SYR.EDU HZHANG12@SYR.EDU", "abstract": "In this paper we investigate conditions on the features of a continuous kernel so that it may approximate an arbitrary continuous target function uniformly on any compact subset of the input space. A number of concrete examples are given of kernels with this universal approximating property.", "venue": "", "keywords": []}
{"id": "luImplicitNormalizingFlows2021", "title": "Implicit Normalizing Flows", "abstract": "Normalizing flows define a probability distribution by an explicit invertible transformation \\ . In this work, we present implicit normalizing flows (ImpFlows), which generalize normalizing flows by allowing the mapping to be implicitly defined by the roots of an equation \\ . ImpFlows build on residual flows (ResFlows) with a proper balance between expressiveness and tractability. Through theoretical analysis, we show that the function space of ImpFlow is strictly richer than that of ResFlows. Furthermore, for any ResFlow with a fixed number of blocks, there exists some function that ResFlow has a non-negligible approximation error. However, the function is exactly representable by a single-block ImpFlow. We propose a scalable algorithm to train and draw samples from ImpFlows. Empirically, we evaluate ImpFlow on several classification and density modeling tasks, and ImpFlow outperforms ResFlow with a comparable amount of parameters on all the benchmarks.", "venue": "arXiv", "keywords": ["lipschitz-constraints", "normalizing flows"]}
{"id": "luoCategoryLevelAdversarialAdaptation2022", "title": "Category-Level Adversarial Adaptation for Semantic Segmentation Using Purified Features", "abstract": "We target the problem named unsupervised domain adaptive semantic segmentation. A key in this campaign consists in reducing the domain shift, so that a classifier based on labeled data from one domain can generalize well to other domains. With the advancement of adversarial learning method, recent works prefer the strategy of aligning the marginal distribution in the feature spaces for minimizing the domain discrepancy. However, based on the observance in experiments, only focusing on aligning global marginal distribution but ignoring the local joint distribution alignment fails to be the optimal choice. Other than that, the noisy factors existing in the feature spaces, which are not relevant to the target task, entangle with the domain invariant factors improperly and make the domain distribution alignment more difficult. To address those problems, we introduce two new modules, Significance-aware Information Bottleneck (SIB) and Category-level alignment (CLA), to construct a purified embedding-based category-level adversarial network. As the name suggests, our designed network, CLAN, can not only disentangle the noisy factors and suppress their influences for target tasks but also utilize those purified features to conduct a more delicate level domain calibration, i.e., global marginal distribution and local joint distribution alignment simultaneously. In three domain adaptation tasks, i.e., GTA5 Cityscapes, SYNTHIA Cityscapes and Cross Season, we validate that our proposed method matches the state of the art in segmentation accuracy.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "keywords": ["adversarial learning", "domain adaptation", "semantic segmentation", "transfer learning"]}
{"id": "luoGroundingStylisticDomain2024", "title": "Grounding Stylistic Domain Generalization with Quantitative Domain Shift Measures and Synthetic Scene Images", "abstract": "", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": []}
{"id": "luoUnderstandingDiffusionModels2022", "title": "Understanding Diffusion Models: A Unified Perspective", "abstract": "Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.", "venue": "arXiv", "keywords": []}
{"id": "luoUnderstandingEffectiveReceptive", "title": "Understanding the Effective Receptive Field in Deep Convolutional Neural Networks", "abstract": "We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.", "venue": "", "keywords": ["cnns", "feature engineering"]}
{"id": "lvCrossDomainSemanticSegmentation2020", "title": "Cross-Domain Semantic Segmentation via Domain-Invariant Interactive Relation Transfer", "abstract": "Exploiting photo-realistic synthetic data to train semantic segmentation models has received increasing attention over the past years. However, the domain mismatch between synthetic and real images will cause a significant performance drop when the model trained with synthetic images is directly applied to real-world scenarios. In this paper, we propose a new domain adaptation approach, called Pivot Interaction Transfer (PIT). Our method mainly focuses on constructing pivot information that is common knowledge shared across domains as a bridge to promote the adaptation of semantic segmentation model from synthetic domains to real-world domains. Specifically, we first infer the image-level category information about the target images, which is then utilized to facilitate pixel-level transfer for semantic segmentation, with the assumption that the interactive relation between the image-level category information and the pixel-level semantic information is invariant across domains. To this end, we propose a novel multi-level region expansion mechanism that aligns both the imagelevel and pixel-level information. Comprehensive experiments on the adaptation from both GTAV and SYNTHIA to Cityscapes clearly demonstrate the superiority of our method.", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["cross-domain augmentation", "representation learning", "semantic segmentation"]}
{"id": "lyColMixSimpleData2023", "title": "ColMix -- A Simple Data Augmentation Framework to Improve Object Detector Performance and Robustness in Aerial Images", "abstract": "In the last decade, Convolutional Neural Network (CNN) and transformer based object detectors have achieved high performance on a large variety of datasets. Though the majority of detection literature has developed this capability on datasets such as MS COCO, these detectors have still proven effective for remote sensing applications. Challenges in this particular domain, such as small numbers of annotated objects and low object density, hinder overall performance. In this work, we present a novel augmentation method, called collage pasting, for increasing the object density without a need for segmentation masks, thereby improving the detector performance. We demonstrate that collage pasting improves precision and recall beyond related methods, such as mosaic augmentation, and enables greater control of object density. However, we find that collage pasting is vulnerable to certain out-of-distribution shifts, such as image corruptions. To address this, we introduce two simple approaches for combining collage pasting with PixMix augmentation method, and refer to our combined techniques as ColMix. Through extensive experiments, we show that employing ColMix results in detectors with superior performance on aerial imagery datasets and robust to various corruptions.", "venue": "arXiv", "keywords": ["mixture augmentations"]}
{"id": "lyuBarycentericDistributionAlignment2021", "title": "Barycenteric Distribution Alignment and Manifold-Restricted Invertibility for Domain Generalization", "abstract": ",", "venue": "ArXiv", "keywords": ["distribution matching", "generalization certification", "highly-analytical", "manifold learning", "risk optimization"]}
{"id": "lyuInfoStylerDisentanglementInformation2023", "title": "InfoStyler: Disentanglement Information Bottleneck for Artistic Style Transfer", "abstract": "Artistic style transfer aims to transfer the style of an artwork to a photograph while maintaining its original overall content. Many prior works focus on designing various transfer modules to transfer the style statistics to the content image. Although effective, ignoring the clear disentanglement of the content features and the style features from the first beginning, they have difficulty in balancing between content preservation and style transferring. To tackle this problem, we propose a novel information disentanglement method, named InfoStyler, to capture the minimal sufficient information for both content and style representations from the pre-trained encoding network. InfoStyler formulates the disentanglement representation learning as an information compression problem by eliminating style statistics from the content image and removing the content structure from the style image. Besides, to further facilitate disentanglement learning, a cross-domain Information Bottleneck (IB) learning strategy is proposed by reconstructing the content and style domains. Extensive experiments demonstrate that our InfoStyler can synthesize high-quality stylized images while balancing content structure preservation and style pattern richness.", "venue": "arXiv", "keywords": ["style transfer"]}
{"id": "maciejewskiOutDistributionDetectionHighDimensional2022", "title": "Out-of-Distribution Detection in High-Dimensional Data Using Mahalanobis Distance - Critical Analysis", "abstract": "Convolutional neural networks used in real-world recognition must be able to detect inputs that are Out-of-Distribution (OoD) with respect to the known or training data. A popular, simple method is to detect OoD inputs using confidence scores based on the Mahalanobis distance from known data. However, this procedure involves estimating the multivariate normal (MVN) density of high dimensional data using the insufficient number of observations (e.g., the dimensionality of features at the last two layers in the ResNet-101 model are 2048 and 1024, with ca. 1000-5000 examples per class for density estimation). In this work, we analyze the instability of parametric estimates of MVN density in high dimensionality and analyze the impact of this on the performance of Mahalanobis distance-based OoD detection. We show that this effect makes Mahalanobis distance-based methods ineffective for near OoD data. We show that the minimum distance from known data beyond which outliers are detectable depends on the dimensionality and number of training samples and decreases with the growing size of the training dataset. We also analyzed the performance of modifications of the Mahalanobis distance method used to minimize density fitting errors, such as using a common covariance matrix for all classes or diagonal covariance matrices. On OoD benchmarks (on CIFAR-10, CIFAR-100, SVHN, and Noise datasets), using representations from the DenseNet or ResNet models, we show that none of these methods should be considered universally superior.", "venue": "Computational Science -- ICCS 2022", "keywords": ["anomaly detection", "ood detection"]}
{"id": "maDeepDomainShift2023", "title": "Deep into The Domain Shift: Transfer Learning through Dependence Regularization", "abstract": "Classical Domain Adaptation methods acquire transferability by regularizing the overall distributional discrepancies between features in the source domain (labeled) and features in the target domain (unlabeled). They often do not differentiate whether the domain differences come from the marginals or the dependence structures. In many business and financial applications, the labeling function usually has different sensitivities to the changes in the marginals versus changes in the dependence structures. Measuring the overall distributional differences will not be discriminative enough in acquiring transferability. Without the needed structural resolution, the learned transfer is less optimal. This paper proposes a new domain adaptation approach in which one can measure the differences in the internal dependence structure separately from those in the marginals. By optimizing the relative weights among them, the new regularization strategy greatly relaxes the rigidness of the existing approaches. It allows a learning machine to pay special attention to places where the differences matter the most. Experiments on three real-world datasets show that the improvements are quite notable and robust compared to various benchmark domain adaptation models.", "venue": "arXiv", "keywords": ["domain adaptation", "domain generalization", "generalization quantification"]}
{"id": "madryDeepLearningModels2017", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.", "venue": "arXiv.org", "keywords": ["adversarial attacks", "adversarial robustness", "robustness analysis"]}
{"id": "mahdiExploringEvolutionFuture2024", "title": "Exploring the Evolution and Future Trends of Image Style Transfer Techniques: A Comprehensive Literature Review", "abstract": "The rapid evolution of image style transfer techniques, a fascinating intersection of art and technology, represents a significant area of research in the domain of computer vision and machine learning. This comprehensive literature review critically examines the development and progression of various methodologies in image style transfer, tracing their evolution from initial neural algorithm-based approaches to more advanced generative adversarial networks (GANs) like CycleGAN, DiscoGAN, and StarGAN. By scrutinizing studies ranging from foundational works to recent innovative approaches, this paper aims to provide a thorough understanding of the techniques, their effectiveness, and the challenges they address. Emerging trends, such as the incorporation of domain-specific information, attention mechanisms, and human perception-inspired loss functions, are highlighted, reflecting the field's shift towards more context-aware and semantically meaningful image translations. The review identifies gaps in systematic comparative studies, particularly concerning the efficacy of CycleGANs against other prevalent methods, indicating areas ripe for future research. This paper serves as a foundational guide for understanding current image style transfer techniques and sets the stage for exploring new horizons in the blending of artistic creativity and technological innovation.", "venue": "International Research Journal of Innovations in Engineering and Technology", "keywords": ["gans", "style transfer", "surveys"]}
{"id": "maiKungFuMakingTraining", "title": "KungFu: Making Training in Distributed Machine Learning Adaptive", "abstract": "When using distributed machine learning (ML) systems to train models on a cluster of worker machines, users must configure a large number of parameters: hyper-parameters (e.g. the batch size and the learning rate) affect model convergence; system parameters (e.g. the number of workers and their communication topology) impact training performance. In current systems, adapting such parameters during training is ill-supported. Users must set system parameters at deployment time, and provide fixed adaptation schedules for hyper-parameters in the training program.", "venue": "", "keywords": ["adaptive learning"]}
{"id": "maireAffinityCNNLearning2016", "title": "Affinity CNN: Learning Pixel-Centric Pairwise Relations for Figure/Ground Embedding", "abstract": "Spectral embedding provides a framework for solving perceptual organization problems, including image segmentation and figure/ground organization. From an affinity matrix describing pairwise relationships between pixels, it clusters pixels into regions, and, using a complex-valued extension, orders pixels according to layer. We train a convolutional neural network (CNN) to directly predict the pairwise relationships that define this affinity matrix. Spectral embedding then resolves these predictions into a globally-consistent segmentation and figure/ground organization of the scene. Experiments demonstrate significant benefit to this direct coupling compared to prior works which use explicit intermediate stages, such as edge detection, on the pathway from image to affinities. Our results suggest spectral embedding as a powerful alternative to the conditional random field (CRF)-based globalization schemes typically coupled to deep neural networks.", "venue": "arXiv", "keywords": ["affinity modeling", "cnns"]}
{"id": "maLearningSpatialtemporalTexture2022", "title": "Learning a Spatial-Temporal Texture Transformer Network for Video Inpainting", "abstract": "We study video inpainting, which aims to recover realistic textures from damaged frames. Recent progress has been made by taking other frames as references so that relevant textures can be transferred to damaged frames. However, existing video inpainting approaches neglect the ability of the model to extract information and reconstruct the content, resulting in the inability to reconstruct the textures that should be transferred accurately. In this paper, we propose a novel and effective spatial-temporal texture transformer network (STTTN) for video inpainting. STTTN consists of six closely related modules optimized for video inpainting tasks: feature similarity measure for more accurate frame pre-repair, an encoder with strong information extraction ability, embedding module for finding a correlation, coarse low-frequency feature transfer, refinement high-frequency feature transfer, and decoder with accurate content reconstruction ability. Such a design encourages joint feature learning across the input and reference frames. To demonstrate the advancedness and effectiveness of the proposed model, we conduct comprehensive ablation learning and qualitative and quantitative experiments on multiple datasets by using standard stationary masks and more realistic moving object masks. The excellent experimental results demonstrate the authenticity and reliability of the STTTN.", "venue": "Frontiers in Neurorobotics", "keywords": ["image inpainting", "image-to-image", "temporal consistency", "texture transfer", "transformers"]}
{"id": "mandhaneMuZeroSelfcompetitionRate2022", "title": "MuZero with Self-competition for Rate Control in VP9 Video Compression", "abstract": "Video streaming usage has seen a significant rise as entertainment, education, and business increasingly rely on online video. Optimizing video compression has the potential to increase access and quality of content to users, and reduce energy use and costs overall. In this paper, we present an application of the MuZero algorithm to the challenge of video compression. Specifically, we target the problem of learning a rate control policy to select the quantization parameters (QP) in the encoding process of libvpx, an open source VP9 video compression library widely used by popular video-on-demand (VOD) services. We treat this as a sequential decision making problem to maximize the video quality with an episodic constraint imposed by the target bitrate. Notably, we introduce a novel self-competition based reward mechanism to solve constrained RL with variable constraint satisfaction difficulty, which is challenging for existing constrained RL methods. We demonstrate that the MuZero-based rate control achieves an average 6.28% reduction in size of the compressed videos for the same delivered video quality level (measured as PSNR BD-rate) compared to libvpx's two-pass VBR rate control policy, while having better constraint satisfaction behavior.", "venue": "arXiv", "keywords": ["image quality assessment", "video compression"]}
{"id": "marcelTorchvisionMachinevisionPackage2010", "title": "Torchvision the Machine-Vision Package of Torch", "abstract": "This paper presents Torchvision an open source machine vision package for Torch. Torch is a machine learning library providing a series of the state-of-the-art algorithms such as Neural Networks, Support Vector Machines, Gaussian Mixture Models, Hidden Markov Models and many others. Torchvision provides additional functionalities to manipulate and process images with standard image processing algorithms. Hence, the resulting images can be used directly with the Torch machine learning algorithms as Torchvision is fully integrated with Torch. Both Torch and Torchvision are written in C++ language and are publicly available under the Free-BSD License.", "venue": "Proceedings of the 18th ACM International Conference on Multimedia", "keywords": []}
{"id": "marfoqFederatedMultiTaskLearning2021", "title": "Federated Multi-Task Learning under a Mixture of Distributions", "abstract": "The increasing size of data generated by smartphones and IoT devices motivated the development of Federated Learning (FL), a framework for on-device collaborative training of machine learning models. First efforts in FL focused on learning a single global model with good average performance across clients, but the global model may be arbitrarily bad for a given client, due to the inherent heterogeneity of local data distributions. Federated multi-task learning (MTL) approaches can learn personalized models by formulating an opportune penalized optimization problem. The penalization term can capture complex relations among personalized models, but eschews clear statistical assumptions about local data distributions. In this work, we propose to study federated MTL under the flexible assumption that each local data distribution is a mixture of unknown underlying distributions. This assumption encompasses most of the existing personalized FL approaches and leads to federated EM-like algorithms for both client-server and fully decentralized settings. Moreover, it provides a principled way to serve personalized models to clients not seen at training time. The algorithms' convergence is analyzed through a novel federated surrogate optimization framework, which can be of general interest. Experimental results on FL benchmarks show that our approach provides models with higher accuracy and fairness than state-of-the-art methods.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["federated learning", "multi-task learning"]}
{"id": "margossianAmortizedVariationalInference2024", "title": "Amortized Variational Inference: When and Why?", "abstract": "In a probabilistic latent variable model, factorized (or mean-field) variational inference (F-VI) fits a separate parametric distribution for each latent variable. Amortized variational inference (A-VI) instead learns a common inference function, which maps each observation to its corresponding latent variable's approximate posterior. Typically, A-VI is used as a cog in the training of variational autoencoders, however it stands to reason that A-VI could also be used as a general alternative to F-VI. In this paper we study when and why A-VI can be used for approximate Bayesian inference. We derive conditions on a latent variable model which are necessary, sufficient, and verifiable under which A-VI can attain F-VI's optimal solution, thereby closing the amortization gap. We prove these conditions are uniquely verified by simple hierarchical models, a broad class that encompasses many models in machine learning. We then show, on a broader class of models, how to expand the domain of AVI's inference function to improve its solution, and we provide examples, e.g. hidden Markov models, where the amortization gap cannot be closed.", "venue": "arXiv", "keywords": ["distribution estimation", "vaes", "variational inference"]}
{"id": "marsdenContinualUnsupervisedDomain2022", "title": "Continual Unsupervised Domain Adaptation for Semantic Segmentation Using a Class-Specific Transfer", "abstract": "In recent years, there has been tremendous progress in the field of semantic image segmentation. However, one remaining challenging problem is that segmentation models do not generalize to unseen domains. To overcome this problem, one either has to label lots of data covering the whole variety of possible domains, which is often infeasible in practice, or apply unsupervised domain adaptation (UDA), only requiring labeled source data. In this work, we focus on UDA and additionally address the case of adapting not only to a single domain, but to a sequence of target domains. This requires mechanisms preventing the model from forgetting its previously learned knowledge.", "venue": "arXiv", "keywords": ["continual da", "instance normalization", "promising", "semantic segmentation", "style transfer", "unsupervised da"]}
{"id": "martinsSoftmaxSparsemaxSparse2016", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification", "abstract": "We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.", "venue": "arXiv", "keywords": ["loss functions", "multi-label classification", "self-attention"]}
{"id": "MastersThesisProject", "title": "Masters Thesis/Project Proposal", "abstract": "", "venue": "", "keywords": []}
{"id": "mcguireNeuralNetworksTrained2023", "title": "Do Neural Networks Trained with Topological Features Learn Different Internal Representations?", "abstract": "There is a growing body of work that leverages features extracted via topological data analysis to train machine learning models. While this field, sometimes known as topological machine learning (TML), has seen some notable successes, an understanding of how the process of learning from topological features differs from the process of learning from raw data is still limited. In this work, we begin to address one component of this larger issue by asking whether a model trained with topological features learns internal representations of data that are fundamentally different than those learned by a model trained with the original raw data. To quantify ``different'', we exploit two popular metrics that can be used to measure the similarity of the hidden representations of data within neural networks, neural stitching and centered kernel alignment. From these we draw a range of conclusions about how training with topological features does and does not change the representations that a model learns. Perhaps unsurprisingly, we find that structurally, the hidden representations of models trained and evaluated on topological features differ substantially compared to those trained and evaluated on the corresponding raw data. On the other hand, our experiments show that in some cases, these representations can be reconciled (at least to the degree required to solve the corresponding task) using a simple affine transformation. We conjecture that this means that neural networks trained on raw data may extract some limited topological features in the process of making predictions.", "venue": "Proceedings of the 1st NeurIPS Workshop on Symmetry and Geometry in Neural Representations", "keywords": ["feature engineering", "topological ml"]}
{"id": "MeasuringUncertaintyHuman2023", "title": "Measuring Uncertainty in Human Visual Segmentation", "abstract": "Segmenting visual stimuli into distinct groups of features and visual objects is central to visual function. Classical psychophysical methods have helped uncover many rules of human perceptual segmentation, and recent progress in machine learning has produced successful algorithms. Yet, the computational logic of human segmentation remains unclear, partially because we lack well-controlled paradigms to measure perceptual segmentation maps and compare models quantitatively. Here we propose a new, integrated approach: given an image, we measure multiple pixel-based same--different judgments and perform model--based reconstruction of the underlying segmentation map. The reconstruction is robust to several experimental manipulations and captures the variability of individual participants. We demonstrate the validity of the approach on human segmentation of natural images and composite textures. We show that image uncertainty affects measured human variability, and it influences how participants weigh different visual features. Because any putative segmentation algorithm can be inserted to perform the reconstruction, our paradigm affords quantitative tests of theories of perception as well as new benchmarks for segmentation algorithms.", "venue": "PLOS Computational Biology", "keywords": []}
{"id": "mechrezPhotorealisticStyleTransfer2017", "title": "Photorealistic Style Transfer with Screened Poisson Equation", "abstract": "Recent work has shown impressive success in transferring painterly style to images. These approaches, however, fall short of photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. In this paper we propose an approach that takes as input a stylized image and makes it more photorealistic. It relies on the Screened Poisson Equation, maintaining the fidelity of the stylized image while constraining the gradients to those of the original input image. Our method is fast, simple, fully automatic and shows positive progress in making a stylized image photorealistic. Our results exhibit finer details and are less prone to artifacts than the state-of-the-art.", "venue": "arXiv", "keywords": ["promising", "style transfer"]}
{"id": "medelyanHumancompetitiveTaggingUsing2009", "title": "Human-Competitive Tagging Using Automatic Keyphrase Extraction", "abstract": "This paper connects two research areas: automatic tagging on the web and statistical keyphrase extraction. First, we analyze the quality of tags in a collaboratively created folksonomy using traditional evaluation techniques. Next, we demonstrate how documents can be tagged automatically with a state-of-the-art keyphrase extraction algorithm, and further improve performance in this new domain using a new algorithm, ``Maui'', that utilizes semantic information extracted from Wikipedia. Maui outperforms existing approaches and extracts tags that are competitive with those assigned by the best performing human taggers.", "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing Volume 3 - EMNLP '09", "keywords": []}
{"id": "MedicalImageComputing2023", "title": "Medical Image Computing and Computer Assisted Intervention -- MICCAI 2023, 26th International Conference, Vancouver, BC, Canada, October 8--12, 2023, Proceedings, Part VI", "abstract": "Ultrasound imaging can vary in style/appearance due to differences in scanning equipment and other factors, resulting in degraded segmentation and classification performance of deep learning models for ultrasound image analysis. Previous studies have attempted to solve this problem by using style transfer and augmentation techniques, but these methods usually require a large amount of data from multiple sources and source-specific discriminators, which are not feasible for medical datasets with limited samples. Moreover, finding suitable augmentation methods for ultrasound data can be difficult. To address these challenges, we propose a novel style transfer-based augmentation framework that consists of three components: mixed style augmentation (MixStyleAug), feature augmentation (FeatAug), and mask-based style augmentation (MaskAug). MixStyleAug uses a style transfer network to transform the style of a training image into various reference styles, which enriches the information from different sources for the network. FeatAug augments the styles at the feature level to compensate for possible style variations, especially for small-size datasets with limited styles. MaskAug leverages segmentation masks to highlight the key regions in the images, which enhances the model's generalizability. We evaluate our framework on five ultrasound datasets collected from different scanners and centers. Our framework outperforms previous methods on both segmentation and classification tasks, especially on small-size datasets. Our results suggest that our framework can effectively improve the performance of deep learning models across different ultrasound sources with limited data.", "venue": "", "keywords": []}
{"id": "mehraCertifyingImprovingGeneralization2022", "title": "On Certifying and Improving Generalization to Unseen Domains", "abstract": "Domain Generalization (DG) aims to learn models whose performance remains high on unseen domains encountered at test-time by using data from multiple related source domains. Many existing DG algorithms reduce the divergence between source distributions in a representation space to potentially align the unseen domain close to the sources. This is motivated by the analysis that explains generalization to unseen domains using distributional distance (such as the Wasserstein distance) to the sources. However, due to the openness of the DG objective, it is challenging to evaluate DG algorithms comprehensively using a few benchmark datasets. In particular, we demonstrate that the accuracy of the models trained with DG methods varies significantly across unseen domains, generated from popular benchmark datasets. This highlights that the performance of DG methods on a few benchmark datasets may not be representative of their performance on unseen domains in the wild. To overcome this roadblock, we propose a universal certification framework based on distributionally robust optimization (DRO) that can efficiently certify the worst-case performance of any DG method. This enables a data-independent evaluation of a DG method complementary to the empirical evaluations on benchmark datasets. Furthermore, we propose a training algorithm that can be used with any DG method to provably improve their certified performance. Our empirical evaluation demonstrates the effectiveness of our method at significantly improving the worst-case loss (i.e., reducing the risk of failure of these models in the wild) without incurring a significant performance drop on benchmark datasets.", "venue": "arXiv", "keywords": ["domain generalization", "generalization certification", "generalization quantification", "uncertainty quantification"]}
{"id": "mehraDomainGeneralizationMethods2022", "title": "Do Domain Generalization Methods Generalize Well?", "abstract": "Domain Generalization (DG) methods use data from multiple related source domains to learn models whose performance does not degrade on unseen domains at test time. Many DG algorithms rely on reducing the divergence between the source distributions in a representation space to potentially align unseen domains close to the sources. These algorithms are motivated by the analytical works that explain generalization to unseen domains based on their distributional distance (e.g., Wasserstein distance) to the sources. However, we show that the accuracy of a DG model varies significantly on unseen domains equidistant from the sources in the learned representation space. This makes it hard to gauge the generalization performance of DG models only based on their performance on benchmark datasets. Thus, we study the worst-case loss of a DG model at a particular distance from the sources and propose an evaluation methodology based on distributionally robust optimization that efficiently computes the worst-case loss on all distributions within a Wasserstein ball around the sources. Our results show that models trained with popular DG methods incur a high worst-case loss even close to the sources which show their lack of generalization to unseen domains. Moreover, we observe a large gap between the worst-case and the empirical losses of distributions at the same distance, showing the performance of the DG models on benchmark datasets is not representative of their performance on unseen domains. Thus, our (target) data-independent and worst-case loss-based methodology highlights the poor generalization performance of current DG models and provides insights beyond empirical evaluation on benchmark datasets for improving these models.", "venue": "", "keywords": ["distribution estimation", "domain generalization", "generalization certification", "generalization quantification"]}
{"id": "mehrishJointSpatialDiscrete2019", "title": "Joint Spatial and Discrete Cosine Transform Domain-Based Counter Forensics for Adaptive Contrast Enhancement", "abstract": "Contrast Enhancement (CE) is a common post-processing step in image forgery to create visually convincing tampered images. However, the artefacts embedded during this process can be captured to determine the presence of CE. To overcome these artefacts, we propose a novel counter-forensic technique using adaptive contrast enhancement as an enhancement operation whereas previous works only deals with global contrast enhancement. We derive an optimization formulation which enhances the attacked image using the L2 distance in both spatial and DCT domain. The proposed algorithm suppresses the detectable artefacts thereby reducing the CE detection performance. Further, the formulation also preserves natural spatial statistics using Huber Markov Random Field. A major advantage of working jointly in both domains is that the complementary information can be leveraged while suppressing artefacts in both domains. We evaluate the proposed method using various visual quality metrics and against state-of-art CE detectors. In our experiments, we observe a reduction of more than 17% in accuracy for false positive rate of 1% for deep learning as well steganalysis-DCT feature based detectors. We also show that the proposed model generates high visual quality images.", "venue": "IEEE Access", "keywords": ["histogram equalization"]}
{"id": "mengRevisitingMultiScaleFeature2022", "title": "Revisiting Multi-Scale Feature Fusion for Semantic Segmentation", "abstract": "It is commonly believed that high internal resolution combined with expensive operations (e.g. atrous convolutions) are necessary for accurate semantic segmentation, resulting in slow speed and large memory usage. In this paper, we question this belief and demonstrate that neither high internal resolution nor atrous convolutions are necessary. Our intuition is that although segmentation is a dense per-pixel prediction task, the semantics of each pixel often depend on both nearby neighbors and far-away context; therefore, a more powerful multi-scale feature fusion network plays a critical role. Following this intuition, we revisit the conventional multi-scale feature space (typically capped at P51) and extend it to a much richer space, up to P9, where the smallest features are only 1/512 of the input size and thus have very large receptive fields. To process such a rich feature space, we leverage the recent BiFPN to fuse the multi-scale features. Based on these insights, we develop a simplified segmentation model, named ESeg, which has neither high internal resolution nor expensive atrous convolutions. Perhaps surprisingly, our simple method can achieve better accuracy with faster speed than prior art across multiple datasets. In real-time settings, ESeg-Lite-S achieves 76.0% mIoU on CityScapes at 189 FPS, outperforming FasterSeg (73.1% mIoU at 170 FPS). Our ESeg-Lite-L runs at 79 FPS and achieves 80.1% mIoU, largely closing the gap between real-time and highperformance segmentation models.", "venue": "arXiv", "keywords": ["feature fusion", "feature pyramids", "semantic segmentation"]}
{"id": "mermillodStabilityplasticityDilemmaInvestigating2013", "title": "The Stability-Plasticity Dilemma: Investigating the Continuum from Catastrophic Forgetting to Age-Limited Learning Effects", "abstract": "The stability-plasticity dilemma is a well-know constraint for artificial and biological neural systems. The basic idea is that learning in a parallel and distributed system requires plasticity for the integration of new knowledge, but also stability in order to prevent the forgetting of previous knowledge. Too much plasticity will result in previously encoded data being constantly forgotten, whereas too much stability will impede the efficient coding of this data at the level of the synapses. However, for the most part, neural computation has addressed the problems related to excessive plasticity or excessive stability as two different fields in the literature. 1.1. The problem of catastrophic forgetting for distributed neural networks. The problem of catastrophic forgetting has emerged as one of the main problems facing artificial neural networks. The problem can be stated as follow: a distributed neural system, for example any biological or artificial memory, has to learn new inputs from the environment but without being disrupted by them. Catastrophic forgetting is defined as a complete forgetting of previously learned information by a neural network exposed to new information (McCloskey &amp; Cohen, 1989; Ratcliff, 1990). This problem is a general problem that exists in different types of neural networks from standard back-propagation neural networks to unsupervised neural networks like self-organizing maps (Richardson &amp; Thomas, 2008) or for connectionist models of sequence acquisition (Ans, Rousset, French, &amp; Musca, 2002). Concerning artificial connectionist neural networks (such as, for instance, standard backpropagation networks), they are highly sensitive to catastrophic forgetting because of their highly distributed internal representations (French, 1992). Therefore, it is possible to reduce the problem of catastrophic forgetting by reducing the overlap among the internal representations stored in the neural network, for example using larger systems, or for example sparse or interleaved learning (Hetherington &amp; Seidenberg, 1989; McCrae &amp; Hetherington, 1993). For this reason, when learning input patterns, connectionist networks have to alternate between them and adjust the corresponding synaptic weights by small increments in order to appropriately associate each input vector with the related output vector. By contrast, sequential learning in a standard connectionist network would result in the complete forgetting of previously learned input-output patterns. This problem affecting artificial neural networks clearly distinguishes them from the cognitive abilities of biological neural systems that are able to learn new patterns in sequential order without catastrophic forgetting. In order to prevent catastrophic forgetting, various researchers have suggested using a dual-memory system which, fundamentally, simulates the presence of a short-term and a long-term memory (Ans &amp; Rousset, 1997; French, 1997; Mermillod, French, Quinn, &amp; Mareschal, 2003; Robins, 1995). The principle is to consolidate information, initially present in a short-term memory, within a long-term memory in order to prevent catastrophic forgetting in connectionist systems. This principle, investigated within the perspective of neural computation in artificial systems, could also point the way to a more general principle that also applies to biological neural systems (French, 1999).1.2. The entrenchment effect: the opposite extreme of the plasticity-stability dilemma. At the opposite extreme of the stability-plasticity continuum lies the entrenchment effect, which may contribute to age-limited learning effects (Bonin, Barry, M&#233;ot, &amp; Chalard, 2004; Bonin, M&#233;ot, Mermillod, Ferrand, &amp; Barry, 2009; Mermillod, Bonin, Morisseau, M&#233;ot, &amp; Ferrand, 2009; Zevin &amp; Seidenberg, 2002). In the cognitive sciences, this research field emerged as part of the attempt to determine whether items which are acquired early in life are better memorized in adults than those which are acquired later in life. Various studies working within this perspective have shown that words acquired early are processed faster and more accurately than words acquired later in life (see Johnston &amp; Barry, 2006; Juhasz, 2005 for reviews). These so-called age-of-acquisition effects have been found in a large variety of tasks, for example picture naming tasks, as well as in different populations (e.g., children and adults). While distributed neural networks have long been used to address various issues in word recognition and spoken word production studies, they have also recently been used to investigate the computational basis of these age-limited learning effects (e.g., Ellis &amp; Lambon Ralph, 2000; Lambon Ralph &amp; Ehsan, 2006; Zevin &amp; Seidenberg, 2002). In these connectionist models, lexical frequency is encoded in the strength of the connections between the different types of representations which are involved in recognizing and producing words (Plaut, McClelland, Seidenberg, &amp; Patterson, 1996; Seidenberg &amp; McClelland, 1989). As far as connectionist simulations of age-limited learning effects are concerned, Ellis &amp; Lambon Ralph (2000) were the first to show that the order of introduction of the encounters determines the number of errors produced by the neural network at the end of training. More precisely, the items introduced first in their study produced fewer errors than the late-introduced items, even after cumulative frequency had been carefully controlled for. This effect of age-limited learning effects in connectionist networks is referred to as the entrenchment effect.At a computational level, the question is to understand how this entrenchment effect emerges. According to Zevin and Seidenberg (2002), the loss of plasticity in connectionist networks such as Seidenberg and McClelland's (1989) was due to the adjustments of the weights that occur on the basis of the logistic function used by the backpropagation algorithm and permits adjustments to the weights (initially set to random values between 0 and 1). These adjustments are at their largest when the activations occur in the middle of the logistic function (around .5) and become smaller as the weights converge on values that cause unit activations to approximate more closely to the target values (for instance 1 or 0). Thus, there is a loss of plasticity (early trained patterns become entrenched in the weights) associated with the learning of the first patterns in the training regime. Therefore, according to Zevin and Seidenberg (2002), the loss of plasticity in connectionist systems should vary as a function of the transfer function and the error signal computed. For example, a root mean square versus cross-entropy error should produce different sensitivity to the entrenchment effect, but also to catastrophic forgetting. Of course, other factors as competition effects, loss of resources, and assimilation effects are important to produce age limited learning effects (Thomas &amp; Johnson, 2006) and are important to control as possible confounded variables. In the current article, we suggest that the Fahlman offset (Fahlman, 1988) could constitute a simple and efficient way to test the computational basis of the loss of plasticity assumed by Zevin and Seidenberg (2002).1.3. The Fahlman Offset: A way to investigate both ends of the continuum. It is interesting to note that the above-mentioned research fields investigate two extremes of the same continuum. In other words, the entrenchment effect is related to a lack of plasticity (and an excess of stability) in response to newly acquired items, whereas catastrophic interference is related to an excess of plasticity (and a lack of stability) in response to new items presented sequentially. There are a number of ways of overcoming this difficulty, for instance by manipulating the orthogonality or the sparseness of the input-output patterns (French, 1992; Robins, 1995). However, among the different possibilities proposed to modulate the plasticity of neural networks, the method proposed by Fahlman (1988) is both simple and efficient. The basic idea is to add a constant number to the derivative of the sigmoid function (synaptic weights are adjusted by multiplying the error produced by a neuron by the derivative of the transfer function, i.e. the sigmoid function). This method makes it possible to avoid the entrenchment effect in the flat part of the sigmoid function and is relevant because this entrenchment effect is due to the flat spots at which the derivative of the sigmoid function approaches zero. Once the output value of a trained neural network starts to become entrenched around this flat spot of the sigmoid function, it becomes very difficult for the standard backpropagation algorithm to modify the synaptic weights responsible for producing this error. Even if an output value represents the maximum possible error, a unit whose output is close to 0.0 or 1.0 will be able to backpropagate only a tiny fraction of this error to the incoming weights and to units in earlier layers. Although it is theoretically possible to recover from entrenchment, this takes a very long time. The method proposed by Fahlman (1988), which consists of adding a small constant number to the derivative of the sigmoid function so that it does not go to zero for any output value, is therefore both very simple and efficient to improve the efficiency of connectionist networks to simulate human cognitive processes (Mermillod, Bonin, Mondillon, Alleysson, &amp; Vermeulen, 2010; Mermillod, Vermeulen, Lundqvist, &amp; Niedenthal, 2009). For example, adding a constant of 0.1 to the sigmoid function before using it to scale the error prevents neuron values from approaching 0 and avoids the flat spots in the sigmoid function where the synaptic weights can become entrenched. 1.4. New findings and perspective. In a recent article (Mermillod, Bonin, M&#233;ot, Ferrand, &amp; Paindavoine, 2012), we showed that age of acquisition can be considered, at a computational level, as an extreme case of frequency trajectory (i.e. the frequency with which a word is encountered during a certain period of life) and can help explain age-limited learning effects. Interestingly, no age-limited learning effects appeared when we used a Fahlman offset of 0.1 whereas it reappeared when we used a Fahlman offset of 0.0. This result was not consistent with Ellis &amp; Lambon Ralph (2000) who reported an age-limited learning effect despite the improvement in the plasticity of the neural network brought about by modulating the Fahlman offset. This could be due to differences in the number or size of the training set between the two studies (Ellis &amp; Lambon Ralph, 2000 or Mermillod et al., 2012). Therefore, the role of the training set in modulating the effects of learning parameters on age-limited learning and catastrophic interference remains a target of further investigation (since these factors could have a combined effect with neural plasticity). However, our results were not unambiguous: modifying the plasticity of an identical neural network by manipulating the Fahlman offset clearly modified the ability of the neural network to simulate (or not) age-limited learning effects. On the other side of the continuum, when the Fahlman constant was set to 0.0, we observed the age-limited learning effects reported in the literature (Ellis &amp; Lambon Ralph, 2000; Lambon Ralph &amp; Ehsan, 2006; Zevin &amp; Seidenberg, 2002). Moreover, one result that will surprise researchers working in the field of catastrophic forgetting is that this catastrophic forgetting effect was largely reduced after the period of entrenchment of synaptic weights (early acquired patterns for &quot;adult&quot; networks having been learnt at an early stage, compared to the medium and late patterns being learnt sequentially in a later stage). To conclude, we suggest here that investigating the plasticity-stability continuum by modulating the Fahlman offset should help us understand a wide range of cognitive phenomena from age-limited learning effects through to catastrophic forgetting, as well as various forms of memory disorders.", "venue": "Frontiers in Psychology", "keywords": []}
{"id": "meshryStEPStylebasedEncoder2021", "title": "StEP: Style-based Encoder Pre-training for Multi-modal Image Synthesis", "abstract": "We propose a novel approach for multi-modal Image-to-image (I2I) translation. To tackle the one-to-many relationship between input and output domains, previous works use complex training objectives to learn a latent embedding, jointly with the generator, that models the variability of the output domain. In contrast, we directly model the style variability of images, independent of the image synthesis task. Specifically, we pre-train a generic style encoder using a novel proxy task to learn an embedding of images, from arbitrary domains, into a low-dimensional style latent space. The learned latent space introduces several advantages over previous traditional approaches to multi-modal I2I translation. First, it is not dependent on the target dataset, and generalizes well across multiple domains. Second, it learns a more powerful and expressive latent space, which improves the fidelity of style capture and transfer. The proposed style pre-training also simplifies the training objective and speeds up the training significantly. Furthermore, we provide a detailed study of the contribution of different loss terms to the task of multi-modal I2I translation, and propose a simple alternative to VAEs to enable sampling from unconstrained latent spaces. Finally, we achieve state-of-the-art results on six challenging benchmarks with a simple training objective that includes only a GAN loss and a reconstruction loss.", "venue": "arXiv", "keywords": ["style transfer"]}
{"id": "mgModelAutotaggingResearch2017", "title": "A Model for Auto-Tagging of Research Papers Based on Keyphrase Extraction Methods", "abstract": "Tagging provides a way to give token of identification to research article which facilitates recommendation and search process. That is, each uploaded article is mapped to set of tags. This paper contributes a document centered approach for auto-tagging of research articles. The auto-tagging method mainly comprises of two processes: -classification and tag selection. The classification process involves automatic keyword extraction using RAKE algorithm which uses keyword -- score matrix. Cosine similarity is used for classifying the articles into corresponding domain utilizing the extracted keywords. Tag selection concentrates on the selection of proper tags for the research article. Graph- based method is used for tag selection by ranking the tags within each class. Tagging facilitates better search facility and determines the dynamics of research areas in terms of number of publication in a domain by each author. The system generates report regarding the influence and expertise of domains ad faculty respectively in the research community.", "venue": "", "keywords": []}
{"id": "mialonInductiveBiasesMachine2023", "title": "On Inductive Biases for Machine Learning in Data Constrained Settings", "abstract": "Learning with limited data is one of the biggest problems of machine learning. Current approaches to this issue consist in learning general representations from huge amounts of data before fine-tuning the model on a small dataset of interest. While such technique, coined transfer learning, is very effective in domains such as computer vision or natural langage processing, it does not yet solve common problems of deep learning such as model interpretability or the overall need for data. This thesis explores a different answer to the problem of learning expressive models in data constrained settings: instead of relying on big datasets to learn neural networks, we will replace some modules by known functions reflecting the structure of the data. Very often, these functions will be drawn from the rich literature of kernel methods. Indeed, many kernels can reflect the underlying structure of the data, thus sparing learning parameters to some extent. Our approach falls under the hood of \"inductive biases\", which can be defined as hypothesis on the data at hand restricting the space of models to explore during learning. We demonstrate the effectiveness of this approach in the context of sequences, such as sentences in natural language or protein sequences, and graphs, such as molecules. We also highlight the relationship between our work and recent advances in deep learning. Additionally, we study convex machine learning models. Here, rather than proposing new models, we wonder which proportion of the samples in a dataset is really needed to learn a \"good\" model. More precisely, we study the problem of safe sample screening, i.e, executing simple tests to discard uninformative samples from a dataset even before fitting a machine learning model, without affecting the optimal model. Such techniques can be used to prune datasets or mine for rare samples.", "venue": "arXiv", "keywords": ["bias sources", "dissertations", "domain generalization", "kernel methods", "optimal transport", "promising", "risk optimization"]}
{"id": "micchelliUniversalKernels2006", "title": "Universal Kernels", "abstract": "In this paper we investigate conditions on the features of a continuous kernel so that it may approximate an arbitrary continuous target function uniformly on any compact subset of the input space. A number of concrete examples are given of kernels with this universal approximating property.", "venue": "Journal of Machine Learning Research", "keywords": ["highly-analytical", "kernel methods"]}
{"id": "michaelisBenchmarkingRobustnessObject2020", "title": "Benchmarking Robustness in Object Detection: Autonomous Driving When Winter Is Coming", "abstract": "The ability to detect objects regardless of image distortions or weather conditions is crucial for real-world applications of deep learning like autonomous driving. We here provide an easy-to-use benchmark to assess how object detection models perform when image quality degrades. The three resulting benchmark datasets, termed Pascal-C, Coco-C and Cityscapes-C, contain a large variety of image corruptions. We show that a range of standard object detection models suffer a severe performance loss on corrupted images (down to 30--60 % of the original performance). However, a simple data augmentation trick---stylizing the training images---leads to a substantial increase in robustness across corruption type, severity and dataset. We envision our comprehensive benchmark to track future progress towards building robust object detection models. Benchmark, code and data are publicly available.", "venue": "arXiv", "keywords": ["adversarial robustness", "adverse weather", "generalization quantification", "robustness analysis"]}
{"id": "mildenhallNeRFDarkHigh2021", "title": "NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images", "abstract": "Neural Radiance Fields (NeRF) is a technique for high quality novel view synthesis from a collection of posed input images. Like most view synthesis methods, NeRF uses tonemapped low dynamic range (LDR) as input; these images have been processed by a lossy camera pipeline that smooths detail, clips highlights, and distorts the simple noise distribution of raw sensor data. We modify NeRF to instead train directly on linear raw images, preserving the scene's full dynamic range. By rendering raw output images from the resulting NeRF, we can perform novel high dynamic range (HDR) view synthesis tasks. In addition to changing the camera viewpoint, we can manipulate focus, exposure, and tonemapping after the fact. Although a single raw image appears significantly more noisy than a postprocessed one, we show that NeRF is highly robust to the zero-mean distribution of raw noise. When optimized over many noisy raw inputs (25-200), NeRF produces a scene representation so accurate that its rendered novel views outperform dedicated single and multi-image deep raw denoisers run on the same wide baseline input images. As a result, our method, which we call RawNeRF, can reconstruct scenes from extremely noisy images captured in near-darkness.", "venue": "arXiv.org", "keywords": ["image synthesis", "neural fields"]}
{"id": "millsMultiTaskFederatedLearning2022", "title": "Multi-Task Federated Learning for Personalised Deep Neural Networks in Edge Computing", "abstract": "Federated Learning (FL) is an emerging approach for collaboratively training Deep Neural Networks (DNNs) on mobile devices, without private user data leaving the devices. Previous works have shown that non-Independent and Identically Distributed (non-IID) user data harms the convergence speed of the FL algorithms. Furthermore, most existing work on FL measures global-model accuracy, but in many cases, such as user content-recommendation, improving individual User model Accuracy (UA) is the real objective. To address these issues, we propose a Multi-Task FL (MTFL) algorithm that introduces non-federated Batch-Normalization (BN) layers into the federated DNN. MTFL benefits UA and convergence speed by allowing users to train models personalised to their own data. MTFL is compatible with popular iterative FL optimisation algorithms such as Federated Averaging (FedAvg), and we show empirically that a distributed form of Adam optimisation (FedAvg-Adam) benefits convergence speed even further when used as the optimisation strategy within MTFL. Experiments using MNIST and CIFAR10 demonstrate that MTFL is able to significantly reduce the number of rounds required to reach a target UA, by up to 5 5 when using existing FL optimisation strategies, and with a further 3 3 improvement when using FedAvg-Adam. We compare MTFL to competing personalised FL algorithms, showing that it is able to achieve the best UA for MNIST and CIFAR10 in all considered scenarios. Finally, we evaluate MTFL with FedAvg-Adam on an edge-computing testbed, showing that its convergence and UA benefits outweigh its overhead.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "keywords": ["edge computing", "federated learning", "multi-task learning"]}
{"id": "mingHYPOHypersphericalOutDistribution2024", "title": "HYPO: Hyperspherical Out-of-Distribution Generalization", "abstract": "Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines and achieves superior performance. Code is available at https://github.com/deeplearning-wisc/hypo.", "venue": "arXiv", "keywords": []}
{"id": "miyatoSPECTRALNORMALIZATIONGENERATIVE2018", "title": "SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS", "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.", "venue": "", "keywords": ["adversarial learning", "gans", "image-to-image", "spectral methods"]}
{"id": "miyatoSpectralNormalizationGenerative2018a", "title": "Spectral Normalization for Generative Adversarial Networks", "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.", "venue": "arXiv", "keywords": []}
{"id": "mjMultilevelTailPixel2020", "title": "Multi-Level Tail Pixel Cutmix and Scale Attention for Long-Tailed Scene Parsing", "abstract": "CutMix has been proposed to improve the image classi cation model robustness against input corruptions and its out-of-distribution detection performances. In this work, we propose a new multi-level tail pixel cutmix which will cut regions according to the tail pixel distribution for improving the long-tailed Scene Parsing. After getting new CutMix-ed images, we furthermore apply a scale attention model with multi scale augmentation on these images to get a more e ective representation on long-tailed pixel distribution.", "venue": "", "keywords": ["mixture augmentations"]}
{"id": "mohammadianPreventionSnowAccretion2020", "title": "Prevention of Snow Accretion on Camera Lenses of Autonomous Vehicles", "abstract": "With the rapid development of artificial intelligence, the autonomous vehicles (AV) have attracted considerable attention in the automotive industry. However, different factors negatively impact the adoption of the AVs, delaying their successful commercialization. Accretion of atmospheric icing, esp", "venue": "", "keywords": []}
{"id": "mohammadianPreventionSnowAccretion2020a", "title": "Prevention of Snow Accretion on Camera Lenses of Autonomous Vehicles", "abstract": "", "venue": "", "keywords": ["automotive occlusion"]}
{"id": "molnarQuantifyingModelComplexity2020", "title": "Quantifying Model Complexity via Functional Decomposition for Better Post-hoc Interpretability", "abstract": "Post-hoc model-agnostic interpretation methods such as partial dependence plots can be employed to interpret complex machine learning models. While these interpretation methods can be applied regardless of model complexity, they can produce misleading and verbose results if the model is too complex, especially w.r.t. feature interactions. To quantify the complexity of arbitrary machine learning models, we propose model-agnostic complexity measures based on functional decomposition: number of features used, interaction strength and main effect complexity. We show that post-hoc interpretation of models that minimize the three measures is more reliable and compact. Furthermore, we demonstrate the application of these measures in a multi-objective optimization approach which simultaneously minimizes loss and complexity.", "venue": "Machine Learning and Knowledge Discovery in Databases", "keywords": []}
{"id": "mondalFlexAEFlexiblyLearning2021", "title": "FlexAE: Flexibly Learning Latent Priors for Wasserstein Auto-Encoders", "abstract": "Auto-Encoder (AE) based neural generative frameworks model the joint-distribution between the data and the latent space using an Encoder-Decoder pair, with regularization imposed in terms of a prior over the latent space. Despite their advantages, such as stability in training, efficient inference, the performance of AE based models has not reached the superior standards of the other generative models such as Generative Adversarial Networks (GANs). Motivated by this, we examine the effect of the latent prior on the generation quality of deterministic AE models in this paper. Specifically, we consider the class of Generative AE models with deterministic Encoder-Decoder pair (such as Wasserstein Auto-Encoder (WAE), Adversarial Auto-Encoder (AAE)), and show that having a fixed prior distribution, a priori, oblivious to the dimensionality of the `true' latent space, will lead to the infeasibility of the optimization problem considered. As a remedy to the issue mentioned above, we introduce an additional state space in the form of flexibly learnable latent priors, in the optimization objective of WAE/AAE. Additionally, we employ a latent-space interpolation based smoothing scheme to address the non-smoothness that may arise from highly flexible priors. We show the efficacy of our proposed models, called FlexAE and FlexAE-SR, through several experiments on multiple datasets, and demonstrate that FlexAE-SR is the new state-of-the-art for the AE based generative models in terms of generation quality as measured by several metrics such as Fr 'echet Inception Distance, Precision/Recall score.", "venue": "Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence", "keywords": []}
{"id": "mordvintsevGrowingNeuralCellular2020", "title": "Growing Neural Cellular Automata", "abstract": "Training an end-to-end differentiable, self-organising cellular automata model of morphogenesis, able to both grow and regenerate specific patterns.", "venue": "Distill", "keywords": ["cellular automata"]}
{"id": "mordvintsevMuNCATexture2021", "title": "\\ NCA: Texture Generation with Ultra-Compact Neural Cellular Automata", "abstract": "We study the problem of example-based procedural texture synthesis using highly compact models. Given a sample image, we use differentiable programming to train a generative process, parameterised by a recurrent Neural Cellular Automata (NCA) rule. Contrary to the common belief that neural networks should be significantly over-parameterised, we demonstrate that our model architecture and training procedure allows for representing complex texture patterns using just a few hundred learned parameters, making their expressivity comparable to hand-engineered procedural texture generating programs. The smallest models from the proposed \\ NCA family scale down to 68 parameters. When using quantisation to one byte per parameter, proposed models can be shrunk to a size range between 588 and 68 bytes. Implementation of a texture generator that uses these parameters to produce images is possible with just a few lines of GLSL or C code.", "venue": "arXiv", "keywords": ["cellular automata", "texture transfer"]}
{"id": "mordvintsevTextureGenerationNeural2021", "title": "Texture Generation with Neural Cellular Automata", "abstract": "Neural Cellular Automata (NCA) have shown a remarkable ability to learn the required rules to \"grow\" images, classify morphologies, segment images, as well as to do general computation such as path-finding. We believe the inductive prior they introduce lends itself to the generation of textures. Textures in the natural world are often generated by variants of locally interacting reaction-diffusion systems. Human-made textures are likewise often generated in a local manner (textile weaving, for instance) or using rules with local dependencies (regular grids or geometric patterns). We demonstrate learning a texture generator from a single template image, with the generation method being embarrassingly parallel, exhibiting quick convergence and high fidelity of output, and requiring only some minimal assumptions around the underlying state manifold. Furthermore, we investigate properties of the learned models that are both useful and interesting, such as non-stationary dynamics and an inherent robustness to damage. Finally, we make qualitative claims that the behaviour exhibited by the NCA model is a learned, distributed, local algorithm to generate a texture, setting our method apart from existing work on texture generation. We discuss the advantages of such a paradigm.", "venue": "arXiv", "keywords": ["cellular automata", "texture transfer"]}
{"id": "morrisonExploringCorruptionRobustness2021", "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers", "abstract": "Recently, vision transformers and MLP-based models have been developed in order to address some of the prevalent weaknesses in convolutional neural networks. Due to the novelty of transformers being used in this domain along with the self-attention mechanism, it remains unclear to what degree these architectures are robust to corruptions. Despite some works proposing that data augmentation remains essential for a model to be robust against corruptions, we propose to explore the impact that the architecture has on corruption robustness. We find that vision transformer architectures are inherently more robust to corruptions than the ResNet-50 and MLP-Mixers. We also find that vision transformers with 5 times fewer parameters than a ResNet-50 have more shape bias. Our code is available to reproduce.", "venue": "arXiv", "keywords": ["adversarial robustness", "bias sources", "transformers"]}
{"id": "mouDropoutTrainingDatadependent2018", "title": "Dropout Training, Data-dependent Regularization, and Generalization Bounds", "abstract": "We study the problem of generalization guarantees for dropout training. A general framework is first proposed for learning procedures with random perturbation on model parameters. The generalization error is bounded by sum of two offset Rademacher complexities: the main term is Rademacher complexity of the hypothesis class with minus offset induced by the perturbation variance, which characterizes data-dependent regularization by the random perturbation; the auxiliary term is offset Rademacher complexity for the variance class, controlling the degree to which this regularization effect can be weakened. For neural networks, we estimate upper and lower bounds for the variance induced by truthful dropout, a variant of dropout that we propose to ensure unbiased output and fit into our framework, and the variance bounds exhibits connection to adaptive regularization methods. By applying our framework to ReLU networks with one hidden layer, a generalization upper bound is derived with no assumptions on the parameter norms or data distribution, with O(1/n)O(1/n)O(1/n) fast rate and adaptivity to geometry of data points being achieved at the same time.", "venue": "Proceedings of the 35th International Conference on Machine Learning", "keywords": ["domain randomization", "generalization certification", "highly-analytical", "risk optimization"]}
{"id": "moutonInputMarginsCan2024", "title": "Input Margins Can Predict Generalization Too", "abstract": "Understanding generalization in deep neural networks is an active area of research. A promising avenue of exploration has been that of margin measurements: the shortest distance to the decision boundary for a given sample or its representation internal to the network. While margins have been shown to be correlated with the generalization ability of a model when measured at its hidden representations (hidden margins), no such link between large margins and generalization has been established for input margins. We show that while input margins are not generally predictive of generalization, they can be if the search space is appropriately constrained. We develop such a measure based on input margins, which we refer to as 'constrained margins'. The predictive power of this new measure is demonstrated on the 'Predicting Generalization in Deep Learning' (PGDL) dataset and contrasted with hidden representation margins. We find that constrained margins achieve highly competitive scores and outperform other margin measurements in general. This provides a novel insight on the relationship between generalization and classification margins, and highlights the importance of considering the data manifold for investigations of generalization in DNNs.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "keywords": ["domain generalization", "generalization quantification", "manifold learning"]}
{"id": "muandetDomainGeneralizationInvariant2013", "title": "Domain Generalization via Invariant Feature Representation", "abstract": "This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.", "venue": "arXiv", "keywords": ["dataset debut", "domain generalization", "representation learning"]}
{"id": "mudassirNeuralImageStyle2022", "title": "Neural Image Style Transfer Using Modified Histogram Matching", "abstract": "Neural Image Style Transfer is an algorithm which was first introduced in 2016 for transferring the style of an image using a CNN. Over the years, a large portion of the research has been devoted in balancing the trade-off between time taken by the algorithm to process the images and the quality of the results generated. In addition, the application of traditional image processing algorithms has been limited. In this paper, we aim to introduce a modification to the traditional method by utilizing a localized histogram matching algorithm combined with Contrast Limited Adaptive Histogram Equalization (CLAHE). The results show that the quality of style transferred and the colour of images obtained by this method is much better than the traditional method for the same number of iterations.", "venue": "Industrial and Systems Engineering Review", "keywords": ["vgg"]}
{"id": "mullerTrivialAugmentTuningfreeStateoftheArt2021", "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation", "abstract": "Automatic augmentation methods have recently become a crucial pillar for strong model performance in vision tasks. While existing automatic augmentation methods need to trade off simplicity, cost and performance, we present a most simple baseline, TrivialAugment, that outperforms previous methods for almost free. TrivialAugment is parameter-free and only applies a single augmentation to each image. Thus, TrivialAugment's effectiveness is very unexpected to us and we performed very thorough experiments to study its performance. First, we compare TrivialAugment to previous state-of-the-art methods in a variety of image classification scenarios. Then, we perform multiple ablation studies with different augmentation spaces, augmentation methods and setups to understand the crucial requirements for its performance. Additionally, we provide a simple interface to facilitate the widespread adoption of automatic augmentation methods, as well as our full code base for reproducibility. Since our work reveals a stagnation in many parts of automatic augmentation research, we end with a short proposal of best practices for sustained future progress in automatic augmentation methods.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "kornia native", "pytorch native"]}
{"id": "mummadiDoesEnhancedShape2021", "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions?", "abstract": "Convolutional neural networks (CNNs) learn to extract representations of complex features, such as object shapes and textures to solve image recognition tasks. Recent work indicates that CNNs trained on ImageNet are biased towards features that encode textures and that these alone are sufficient to generalize to unseen test data from the same distribution as the training data but often fail to generalize to out-of-distribution data. It has been shown that augmenting the training data with different image styles decreases this texture bias in favor of increased shape bias while at the same time improving robustness to common corruptions, such as noise and blur. Commonly, this is interpreted as shape bias increasing corruption robustness. However, this relationship is only hypothesized. We perform a systematic study of different ways of composing inputs based on natural images, explicit edge information, and stylization. While stylization is essential for achieving high corruption robustness, we do not find a clear correlation between shape bias and robustness. We conclude that the data augmentation caused by style-variation accounts for the improved corruption robustness and increased shape bias is only a byproduct.", "venue": "arXiv", "keywords": ["adversarial robustness", "bias sources", "promising"]}
{"id": "naitzatTopologyDeepNeural", "title": "Topology of Deep Neural Networks", "abstract": "We study how the topology of a data set M = Ma Mb Rd, representing two classes a and b in a binary classification problem, changes as it passes through the layers of a well-trained neural network, i.e., one with perfect accuracy on training set and near-zero generalization error ( 0.01%). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like ReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrarily well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following:  Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of M we begin with, when passed through a well-trained neural network f : Rd Rp, there is a vast reduction in the Betti numbers of both components Ma and Mb; in fact they nearly always reduce to their lowest possible values: k f (Mi) = 0 for k 1 and 0 f (Mi) = 1, i = a, b.  The reduction in Betti numbers is significantly faster for ReLU activation than for hyperbolic tangent activation as the former defines nonhomeomorphic maps that change topology, whereas the latter defines homeomorphic maps that preserve topology.  Shallow and deep networks transform data sets differently --- a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep one spreads topological changes more evenly across all layers.", "venue": "", "keywords": ["topological data analysis"]}
{"id": "namNeuralImageRepresentations2022", "title": "Neural Image Representations for Multi-Image Fusion and Layer Separation", "abstract": "We propose a framework for aligning and fusing multiple images into a single view using neural image representations (NIRs), also known as implicit or coordinate-based neural representations. Our framework targets burst images that exhibit camera ego motion and potential changes in the scene. We describe different strategies for alignment depending on the nature of the scene motion -- namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. With the neural image representation, our framework effectively combines multiple inputs into a single canonical view without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks. The code and results are available at https://shnnam.github.io/research/nir.", "venue": "arXiv", "keywords": ["image-to-image"]}
{"id": "namReducingDomainGap2021", "title": "Reducing Domain Gap by Reducing Style Bias", "abstract": "Convolutional Neural Networks (CNNs) often fail to maintain their performance when they confront new test domains, which is known as the problem of domain shift. Recent studies suggest that one of the main causes of this problem is CNNs' strong inductive bias towards image styles (i.e. textures) which are sensitive to domain changes, rather than contents (i.e. shapes). Inspired by this, we propose to reduce the intrinsic style bias of CNNs to close the gap between domains. Our Style-Agnostic Networks (SagNets) disentangle style encodings from class categories to prevent style biased predictions and focus more on the contents. Extensive experiments show that our method effectively reduces the style bias and makes the model more robust under domain shift. It achieves remarkable performance improvements in a wide range of cross-domain tasks including domain generalization, unsupervised domain adaptation, and semi-supervised domain adaptation on multiple datasets.", "venue": "arXiv", "keywords": ["bias sources", "critical citation", "domain generalization", "promising", "style transfer"]}
{"id": "nanniFeatureTransformsImage2022", "title": "Feature Transforms for Image Data Augmentation", "abstract": "A problem with convolutional neural networks (CNNs) is that they require large datasets to obtain adequate robustness; on small datasets, they are prone to overfitting. Many methods have been proposed to overcome this shortcoming with CNNs. In cases where additional samples cannot easily be collected, a common approach is to generate more data points from existing data using an augmentation technique. In image classification, many augmentation approaches utilize simple image manipulation algorithms. In this work, we propose some new methods for data augmentation based on several image transformations: the Fourier transform (FT), the Radon transform (RT), and the discrete cosine transform (DCT). These and other data augmentation methods are considered in order to quantify their effectiveness in creating ensembles of neural networks. The novelty of this research is to consider different strategies for data augmentation to generate training sets from which to train several classifiers which are combined into an ensemble. Specifically, the idea is to create an ensemble based on a kind of bagging of the training set, where each model is trained on a different training set obtained by augmenting the original training set with different approaches. We build ensembles on the data level by adding images generated by combining fourteen augmentation approaches, with three based on FT, RT, and DCT, proposed here for the first time. Pretrained ResNet50 networks are finetuned on training sets that include images derived from each augmentation method. These networks and several fusions are evaluated and compared across eleven benchmarks. Results show that building ensembles on the data level by combining different data augmentation methods produce classifiers that not only compete competitively against the state-of-the-art but often surpass the best approaches reported in the literature.", "venue": "Neural Computing and Applications", "keywords": ["feature-level augmentation", "spectral methods"]}
{"id": "narayananAviaryTrainingLanguage2024", "title": "Aviary: Training Language Agents on Challenging Scientific Tasks", "abstract": "Solving complex real-world tasks requires cycles of actions and observations. This is particularly true in science, where tasks require many cycles of analysis, tool use, and experimentation. Language agents are promising for automating intellectual tasks in science because they can interact with tools via natural language or code. Yet their flexibility creates conceptual and practical challenges for software implementations, since agents may comprise non-standard components such as internal reasoning, planning, tool usage, as well as the inherent stochasticity of temperature-sampled language models. Here, we introduce Aviary, an extensible gymnasium for language agents. We formalize agents as policies solving language-grounded partially observable Markov decision processes, which we term language decision processes. We then implement five environments, including three challenging scientific environments: (1) manipulating DNA constructs for molecular cloning, (2) answering research questions by accessing scientific literature, and (3) engineering protein stability. These environments were selected for their focus on multi-step reasoning and their relevance to contemporary biology research. Finally, with online training and scaling inference-time compute, we show that language agents backed by open-source, non-frontier LLMs can match and exceed both frontier LLM agents and human experts on multiple tasks at up to 100x lower inference cost.", "venue": "arXiv", "keywords": []}
{"id": "naseerIntriguingPropertiesVision2021", "title": "Intriguing Properties of Vision Transformers", "abstract": "Vision transformers (ViT) have demonstrated impressive performance across numerous machine vision tasks. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility (in attending image-wide context conditioned on a given patch) can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and provide comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a)Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1 accuracy on ImageNet even after randomly occluding 80% of the image content. (b)The robustness towards occlusions is not due to texture bias, instead we show that ViTs are significantly less biased towards local textures, compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c)Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d)Off-the-shelf features from a single ViT model can be combined to create a feature ensemble, leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms. We show effective features of ViTs are due to flexible and dynamic receptive fields possible via self-attention mechanisms. Our code will be publicly released.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["promising", "transformers"]}
{"id": "nassifProximalMultitaskLearning2016", "title": "Proximal Multitask Learning over Networks with Sparsity-inducing Coregularization", "abstract": "In this work, we consider multitask learning problems where clusters of nodes are interested in estimating their own parameter vector. Cooperation among clusters is beneficial when the optimal models of adjacent clusters have a good number of similar entries. We propose a fully distributed algorithm for solving this problem. The approach relies on minimizing a global mean-square error criterion regularized by non-differentiable terms to promote cooperation among neighboring clusters. A general diffusion forward-backward splitting strategy is introduced. Then, it is specialized to the case of sparsity promoting regularizers. A closed-form expression for the proximal operator of a weighted sum of \\ -norms is derived to achieve higher efficiency. We also provide conditions on the step-sizes that ensure convergence of the algorithm in the mean and mean-square error sense. Simulations are conducted to illustrate the effectiveness of the strategy.", "venue": "IEEE Transactions on Signal Processing", "keywords": ["multi-task learning"]}
{"id": "nastlPredictorsCausalFeatures2024", "title": "Predictors from Causal Features Do Not Generalize Better to New Domains", "abstract": "We study how well machine learning models trained on causal features generalize across domains. We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. For each prediction task, we select features that have a causal influence on the target of prediction. Our goal is to test the hypothesis that models trained on causal features generalize better across domains. Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features. Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features. If the goal is to generalize to new domains, practitioners might as well train the best possible model on all available features.", "venue": "arXiv", "keywords": ["domain generalization", "feature engineering", "robustness analysis"]}
{"id": "navonMultiTaskLearningBargaining2022", "title": "Multi-Task Learning as a Bargaining Game", "abstract": "In Multi-task learning (MTL), a joint model is trained to simultaneously make predictions for several tasks. Joint training reduces computation costs and improves data efficiency; however, since the gradients of these different tasks may conflict, training a joint model for MTL often yields lower performance than its corresponding single-task counterparts. A common method for alleviating this issue is to combine per-task gradients into a joint update direction using a particular heuristic. In this paper, we propose viewing the gradients combination step as a bargaining game, where tasks negotiate to reach an agreement on a joint direction of parameter update. Under certain assumptions, the bargaining problem has a unique solution, known as the Nash Bargaining Solution, which we propose to use as a principled approach to multi-task learning. We describe a new MTL optimization procedure, Nash-MTL, and derive theoretical guarantees for its convergence. Empirically, we show that Nash-MTL achieves state-ofthe-art results on multiple MTL benchmarks in various domains.", "venue": "arXiv", "keywords": ["multi-task learning"]}
{"id": "negreHybridSamplingBayesian2014", "title": "Hybrid Sampling Bayesian Occupancy Filter", "abstract": "Modeling and monitoring dynamic environments is a complex task but is crucial in the field of intelligent vehicle. A traditional way of addressing these issues is the modeling of moving objects, through Detection And Tracking of Moving Objects (DATMO) methods. An alternative to a classic object model framework is the occupancy grid filtering domain. Instead of segmenting the scene into objects and track them, the environment is represented as a regular grid of occupancy, in which each cell is tracked at a sub-object level. The Bayesian Occupancy Filter is a generic occupancy grid framework which predicts the spread of spatial occupancy by estimating cell velocity distributions. However its velocity model, corresponding to a transition histogram per cell, leads to huge data management which in practice makes it hardly compatible to severe computational and hardware constraints, like in many embedded systems. In this paper, we present a new representation for the BOF, describing the environment through a mix of static and dynamic occupancy. This differentiation enables the use of a model adapted to the considered nature: static occupancy is described in a classic occupancy grid, while dynamic occupancy is modeled by a set of moving particles. Both static and dynamic parts are jointly generated and evaluated, their distribution over the cells being adjusted. This approach leads to a more compact model and to drastically improve the accuracy of the results, in particular in term of velocities. Experimental results show that the number of values required to model the velocities have been reduced from a typical 900 per cell (for a 30x30 neighborhood) to less than 2 per cell in average. The massive data compression allows to plan dedicated embedded devices.", "venue": "2014 IEEE Intelligent Vehicles Symposium Proceedings", "keywords": []}
{"id": "NEURIPS2021_2578eb9c", "title": "Beyond BatchNorm: Towards a Unified Understanding of Normalization in Deep Learning", "abstract": "", "venue": "Advances in Neural Information Processing Systems", "keywords": ["instance normalization", "surveys"]}
{"id": "newmanRenormalizationGroupAnalysis1999", "title": "Renormalization Group Analysis of the Small-World Network Model", "abstract": "We study the small-world network model, which mimics the transition between regular-lattice and random-lattice behavior in social networks of increasing size. We contend that the model displays a normal continuous phase transition with a divergent correlation length as the degree of randomness tends to zero. We propose a real-space renormalization group transformation for the model and demonstrate that the transformation is exact in the limit of large system size. We use this result to calculate the exact value of the single critical exponent for the system, and to derive the scaling form for the average number of \"degrees of separation\" between two nodes on the network as a function of the three independent variables. We confirm our results by extensive numerical simulation.", "venue": "Physics Letters A", "keywords": ["newman"]}
{"id": "newmanScalingPercolationSmallworld1999", "title": "Scaling and Percolation in the Small-World Network Model", "abstract": "In this paper we study the small-world network model of Watts and Strogatz, which mimics some aspects of the structure of networks of social interactions. We argue that there is one non-trivial length-scale in the model, analogous to the correlation length in other systems, which is well-defined in the limit of infinite system size and which diverges continuously as the randomness in the network tends to zero, giving a normal critical point in this limit. This length-scale governs the cross-over from large- to small-world behavior in the model, as well as the number of vertices in a neighborhood of given radius on the network. We derive the value of the single critical exponent controlling behavior in the critical region and the finite size scaling form for the average vertex-vertex distance on the network, and, using series expansion and Pade approximants, find an approximate analytic form for the scaling function. We calculate the effective dimension of small-world graphs and show that this dimension varies as a function of the length-scale on which it is measured, in a manner reminiscent of multifractals. We also study the problem of site percolation on small-world networks as a simple model of disease propagation, and derive an approximate expression for the percolation probability at which a giant component of connected vertices first forms (in epidemiological terms, the point at which an epidemic occurs). The typical cluster radius satisfies the expected finite size scaling form with a cluster size exponent close to that for a random graph. All our analytic results are confirmed by extensive numerical simulations of the model.", "venue": "Physical Review E", "keywords": []}
{"id": "neyshaburExploringGeneralizationDeep2017", "title": "Exploring Generalization in Deep Learning", "abstract": "With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.", "venue": "Advances in Neural Information Processing Systems", "keywords": []}
{"id": "ngPredictingOutofDomainGeneralization2022", "title": "Predicting Out-of-Domain Generalization with Neighborhood Invariance", "abstract": "Developing and deploying machine learning models safely depends on the ability to characterize and compare their abilities to generalize to new environments. Although recent work has proposed a variety of methods that can directly predict or theoretically bound the generalization capacity of a model, they rely on strong assumptions such as matching train/test distributions and access to model gradients. In order to characterize generalization when these assumptions are not satisfied, we propose neighborhood invariance, a measure of a classifier's output invariance in a local transformation neighborhood. Specifically, we sample a set of transformations and given an input test point, calculate the invariance as the largest fraction of transformed points classified into the same class. Crucially, our measure is simple to calculate, does not depend on the test point's true label, makes no assumptions about the data distribution or model, and can be applied even in out-of-domain (OOD) settings where existing methods cannot, requiring only selecting a set of appropriate data transformations. In experiments on robustness benchmarks in image classification, sentiment analysis, and natural language inference, we demonstrate a strong and robust correlation between our neighborhood invariance measure and actual OOD generalization on over 4,600 models evaluated on over 100 unique train/test domain pairs.", "venue": "arXiv", "keywords": ["domain generalization", "generalization quantification", "manifold learning"]}
{"id": "ngSSMBASelfSupervisedManifold2020", "title": "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness", "abstract": "Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.", "venue": "arXiv", "keywords": ["domain generalization", "manifold learning", "self-supervised learning"]}
{"id": "nguyenHierarchicalSlicedWasserstein2023", "title": "Hierarchical Sliced Wasserstein Distance", "abstract": "Sliced Wasserstein (SW) distance has been widely used in different application scenarios since it can be scaled to a large number of supports without suffering from the curse of dimensionality. The value of sliced Wasserstein distance is the average of transportation cost between one-dimensional representations (projections) of original measures that are obtained by Radon Transform (RT). Despite its efficiency in the number of supports, estimating the sliced Wasserstein requires a relatively large number of projections in high-dimensional settings. Therefore, for applications where the number of supports is relatively small compared with the dimension, e.g., several deep learning applications where the mini-batch approaches are utilized, the complexities from matrix multiplication of Radon Transform become the main computational bottleneck. To address this issue, we propose to derive projections by linearly and randomly combining a smaller number of projections which are named bottleneck projections. We explain the usage of these projections by introducing Hierarchical Radon Transform (HRT) which is constructed by applying Radon Transform variants recursively. We then formulate the approach into a new metric between measures, named Hierarchical Sliced Wasserstein (HSW) distance. By proving the injectivity of HRT, we derive the metricity of HSW. Moreover, we investigate the theoretical properties of HSW including its connection to SW variants and its computational and sample complexities. Finally, we compare the computational cost and generative quality of HSW with the conventional SW on the task of deep generative modeling using various benchmark datasets including CIFAR10, CelebA, and Tiny ImageNet.", "venue": "arXiv", "keywords": ["highly-analytical", "measure theory", "optimal transport"]}
{"id": "nguyenTIPITestTime2023", "title": "TIPI: Test Time Adaptation With Transformation Invariance", "abstract": "", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": ["test-time da", "transformers"]}
{"id": "nicolaouTorMentorDeterministicDynamicpath2022", "title": "TorMentor: Deterministic Dynamic-Path, Data Augmentations with Fractals", "abstract": "We propose the use of fractals as a means of efficient data augmentation. Specifically, we employ plasma fractals for adapting global image augmentation transformations into continuous local transforms. We formulate the diamond square algorithm as a cascade of simple convolution operations allowing efficient computation of plasma fractals on the GPU. We present the TorMentor image augmentation framework that is totally modular and deterministic across images and point-clouds. All image augmentation operations can be combined through pipelining and random branching to form flow networks of arbitrary width and depth. We demonstrate the efficiency of the proposed approach with experiments on document image segmentation (binarization) with the DIBCO datasets. The proposed approach demonstrates superior performance to traditional image augmentation techniques. Finally, we use extended synthetic binary text images in a self-supervision regiment and outperform the same model when trained with limited data and simple extensions.", "venue": "arXiv", "keywords": []}
{"id": "niDataAugmentationMetaLearning2021", "title": "Data Augmentation for Meta-Learning", "abstract": "Conventional image classifiers are trained by randomly sampling mini-batches of images. To achieve state-of-the-art performance, practitioners use sophisticated data augmentation schemes to expand the amount of training data available for sampling. In contrast, meta-learning algorithms sample support data, query data, and tasks on each training step. In this complex sampling scenario, data augmentation can be used not only to expand the number of images available per class, but also to generate entirely new classes/tasks. We systematically dissect the meta-learning pipeline and investigate the distinct ways in which data augmentation can be integrated at both the image and class levels. Our proposed meta-specific data augmentation significantly improves the performance of meta-learners on few-shot classification benchmarks.", "venue": "arXiv", "keywords": ["class balancing", "meta-learning"]}
{"id": "niDistributionAwareContinualTest2023", "title": "Distribution-Aware Continual Test Time Adaptation for Semantic Segmentation", "abstract": "Since autonomous driving systems usually face dynamic and ever-changing environments, continual test-time adaptation (CTTA) has been proposed as a strategy for transferring deployed models to continually changing target domains. However, the pursuit of long-term adaptation often introduces catastrophic forgetting and error accumulation problems, which impede the practical implementation of CTTA in the real world. Recently, existing CTTA methods mainly focus on utilizing a majority of parameters to fit target domain knowledge through self-training. Unfortunately, these approaches often amplify the challenge of error accumulation due to noisy pseudolabels, and pose practical limitations stemming from the heavy computational costs associated with entire model updates. In this paper, we propose a distribution-aware tuning (DAT) method to make the semantic segmentation CTTA efficient and practical in real-world applications. DAT adaptively selects and updates two small groups of trainable parameters based on data distribution during the continual adaptation process, including domainspecific parameters (DSP) and task-relevant parameters (TRP). Specifically, DSP exhibits sensitivity to outputs with substantial distribution shifts, effectively mitigating the problem of error accumulation. In contrast, TRP are allocated to positions that are responsive to outputs with minor distribution shifts, which are fine-tuned to avoid the catastrophic forgetting problem. In addition, since CTTA is a temporal task, we introduce the Parameter Accumulation Update (PAU) strategy to collect the updated DSP and TRP in target domain sequences. We conduct extensive experiments on two widely-used semantic segmentation CTTA benchmarks, achieving promising performance compared to previous state-of-the-art methods.", "venue": "arXiv", "keywords": ["continual da", "domain adaptation", "semantic segmentation", "temporal consistency", "test-time da"]}
{"id": "nieDepthAwareMultiGridDeep2022", "title": "Depth-Aware Multi-Grid Deep Homography Estimation with Contextual Correlation", "abstract": "Homography estimation is an important task in computer vision applications, such as image stitching, video stabilization, and camera calibration. Traditional homography estimation methods heavily depend on the quantity and distribution of feature correspondences, leading to poor robustness in low-texture scenes. The learning solutions, on the contrary, try to learn robust deep features but demonstrate unsatisfying performance in the scenes with low overlap rates. In this paper, we address these two problems simultaneously by designing a contextual correlation layer (CCL). The CCL can efficiently capture the long-range correlation within feature maps and can be flexibly used in a learning framework. In addition, considering that a single homography can not represent the complex spatial transformation in depth-varying images with parallax, we propose to predict multi-grid homography from global to local. Moreover, we equip our network with a depth perception capability, by introducing a novel depth-aware shape-preserved loss. Extensive experiments demonstrate the superiority of our method over state-of-the-art solutions in the synthetic benchmark dataset and real-world dataset. The codes and models will be available at https://github.com/nie-lang/Multi-Grid-Deep-Homography.", "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "keywords": ["depth estimation", "homography estimation"]}
{"id": "niemeijerDomainAdaptationGeneralization2023", "title": "Domain Adaptation and Generalization: A Low-Complexity Approach", "abstract": "Well-performing deep learning methods are essential in today's perception of robotic systems such as autonomous driving vehicles. Ongoing research is due to the real-life demands for robust deep learning models against numerous domain changes and cheap training processes to avoid costly manual-labeling efforts. These requirements are addressed by unsupervised domain adaptation methods, in particular for synthetic to real-world domain changes. Recent top-performing approaches are hybrids consisting of multiple adaptation technologies and complex training processes. In contrast, this work proposes EasyAdap, a simple and easy-to-use unsupervised domain adaptation method achieving near state-of-the-art performance on the synthetic to real-world domain change. Our evaluation consists of a comparison to numerous top-performing methods, and it shows the competitiveness and further potential of domain adaptation and domain generalization capabilities of our method. We contribute and focus on an extensive discussion revealing possible reasons for domain generalization capabilities, which is necessary to satisfy real-life application's demands.", "venue": "Proceedings of The 6th Conference on Robot Learning", "keywords": ["domain adaptation", "domain generalization", "self-training", "semantic segmentation", "unsupervised da"]}
{"id": "niemeijerGeneralizationAdaptationDiffusionBased", "title": "Generalization by Adaptation: Diffusion-Based Domain Extension for Domain-Generalized Semantic Segmentation", "abstract": "", "venue": "", "keywords": ["class balancing", "diffusion models", "domain generalization", "semantic segmentation"]}
{"id": "niklassonSelfOrganisingTextures2021", "title": "Self-Organising Textures", "abstract": "Neural Cellular Automata learn to generate textures, exhibiting surprising properties.", "venue": "Distill", "keywords": ["cellular automata", "texture transfer"]}
{"id": "niklassonSelfOrganisingTextures2021a", "title": "Self-Organising Textures", "abstract": "Neural Cellular Automata learn to generate textures, exhibiting surprising properties.", "venue": "Distill", "keywords": ["cellular automata"]}
{"id": "niuStableTestTimeAdaptation2023", "title": "Towards Stable Test-Time Adaptation in Dynamic Wild World", "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, i.e., group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, i.e., assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably over prior methods and is computationally efficient under the above wild test scenarios. The source code is available at https://github.com/mr-eggplant/SAR.", "venue": "arXiv", "keywords": ["augmentation stability", "class balancing", "domain adaptation", "entropic optimization", "test-time da"]}
{"id": "NVIDIACUDAVideo", "title": "NVIDIA CUDA Video Decoder", "abstract": "", "venue": "", "keywords": []}
{"id": "obermanLIPSCHITZREGULARIZEDDEEP", "title": "LIPSCHITZ REGULARIZED DEEP NEURAL NETWORKS CONVERGE AND GENERALIZE", "abstract": "Generalization of deep neural networks (DNNs) is an open problem which, if solved, could impact the reliability and verification of deep neural network architectures. In this paper, we show that if the usual fidelity term used in training DNNs is augmented by a Lipschitz regularization term, then the networks converge and generalize. The convergence is in the limit as the number of data points, n , while also allowing the network to grow as needed to fit the data. Two regimes are identified: in the case of clean labels, we prove convergence to the label function which corresponds to zero loss, in the case of corrupted labels which we prove convergence to a regularized label function which is the solution of a limiting variational problem. In both cases, a convergence rate is also provided.", "venue": "", "keywords": ["lipschitz-constraints"]}
{"id": "ochotorenaAnisotropicGuidedFiltering2020", "title": "Anisotropic Guided Filtering", "abstract": "The guided filter and its subsequent derivatives have been widely employed in many image processing and computer vision applications primarily brought about by their low complexity and good edge-preservation properties. Despite this success, the different variants of the guided filter are unable to handle more aggressive filtering strengths leading to the manifestation of ``detail halos''. At the same time, these existing filters perform poorly when the input and guide images have structural inconsistencies. In this paper, we demonstrate that these limitations are due to the guided filter operating as a variable-strength locally-isotropic filter that, in effect, acts as a weak anisotropic filter on the image. Our analysis shows that this behaviour stems from the use of unweighted averaging in the final steps of guided filter variants including the adaptive guided filter (AGF), weighted guided image filter (WGIF), and gradientdomain guided image filter (GGIF). We propose a novel filter, the Anisotropic Guided Filter (AnisGF), that utilises weighted averaging to achieve maximum diffusion while preserving strong edges in the image. The proposed weights are optimised based on the local neighbourhood variances to achieve strong anisotropic filtering while preserving the low computational cost of the original guided filter. Synthetic tests show that the proposed method addresses the presence of detail halos and the handling of inconsistent structures found in previous variants of the guided filter. Furthermore, experiments in scale-aware filtering, detail enhancement, texture removal, and chroma upsampling demonstrate the improvements brought about by the technique.", "venue": "IEEE Transactions on Image Processing", "keywords": ["filter augmentations", "guided filters"]}
{"id": "olahFeatureVisualization2017", "title": "Feature Visualization", "abstract": "How neural networks build up their understanding of images", "venue": "Distill", "keywords": ["feature visualization", "visualizations"]}
{"id": "oliveiraConnectingMetricsShapetexture2023", "title": "Connecting Metrics for Shape-Texture Knowledge in Computer Vision", "abstract": "Modern artificial neural networks, including convolutional neural networks and vision transformers, have mastered several computer vision tasks, including object recognition. However, there are many significant differences between the behavior and robustness of these systems and of the human visual system. Deep neural networks remain brittle and susceptible to many changes in the image that do not cause humans to misclassify images. Part of this different behavior may be explained by the type of features humans and deep neural networks use in vision tasks. Humans tend to classify objects according to their shape while deep neural networks seem to rely mostly on texture. Exploring this question is relevant, since it may lead to better performing neural network architectures and to a better understanding of the workings of the vision system of primates. In this work, we advance the state of the art in our understanding of this phenomenon, by extending previous analyses to a much larger set of deep neural network architectures. We found that the performance of models in image classification tasks is highly correlated with their shape bias measured at the output and penultimate layer. Furthermore, our results showed that the number of neurons that represent shape and texture are strongly anti-correlated, thus providing evidence that there is competition between these two types of features. Finally, we observed that while in general there is a correlation between performance and shape bias, there are significant variations between architecture families.", "venue": "arXiv", "keywords": ["bias sources"]}
{"id": "omeadhraVariableResolutionOccupancy2019", "title": "Variable Resolution Occupancy Mapping Using Gaussian Mixture Models", "abstract": "Occupancy mapping is fundamental for active perception systems to enable reasoning about known and unknown regions of the environment. The majority of occupancy mapping approaches enforce an a priori discretization on the environment, resulting in a fixed resolution map that limits the expressiveness of the representation. The proposed approach removes this a priori discretization, learns continuous representations for the evidence of occupied and free space to derive the probability of occupancy, and enables occupancy grid maps to be generated at arbitrary resolution. Efficient methods are also presented that accurately evaluate the probability of occupancy in individual cells and enable multi-resolution mapping and local occupancy evaluation. The efficacy of the approach is demonstrated by comparison to state-of-the-art discrete and continuous mapping techniques in both 2D and 3D. The core contribution of this work is a memory-efficient method for deriving occupancy that is amenable to small or large corrections in pose without the need to regenerate the entire map. The applications under considerations are low-bandwidth scenarios (e.g. multi-robot exploration) and operations in expansive environments where storing an occupancy grid map of the entire environment would be prohibitive.", "venue": "IEEE Robotics and Automation Letters", "keywords": []}
{"id": "openjaDetectionEvaluationBiasinducing2023", "title": "Detection and Evaluation of Bias-Inducing Features in Machine Learning", "abstract": "The cause-to-effect analysis can help us decompose all the likely causes of a problem, such as an undesirable business situation or unintended harm to the individual(s). This implies that we can identify how the problems are inherited, rank the causes to help prioritize fixes, simplify a complex problem and visualize them. In the context of machine learning (ML), one can use cause-to-effect analysis to understand the reason for the biased behavior of the system. For example, we can examine the root causes of biases by checking each feature for a potential cause of bias in the model. To approach this, one can apply small changes to a given feature or a pair of features in the data, following some guidelines and observing how it impacts the decision made by the model (i.e., model prediction). Therefore, we can use cause-to-effect analysis to identify the potential bias-inducing features, even when these features are originally are unknown. This is important since most current methods require a pre-identification of sensitive features for bias assessment and can actually miss other relevant bias-inducing features, which is why systematic identification of such features is necessary. Moreover, it often occurs that to achieve an equitable outcome, one has to take into account sensitive features in the model decision. Therefore, it should be up to the domain experts to decide based on their knowledge of the context of a decision whether bias induced by specific features is acceptable or not. In this study, we propose an approach for systematically identifying all bias-inducing features of a model to help support the decision-making of domain experts. We evaluated our technique using four well-known datasets to showcase how our contribution can help spearhead the standard procedure when developing, testing, maintaining, and deploying fair/equitable machine learning systems.", "venue": "arXiv", "keywords": ["bias sources", "feature engineering", "generalization quantification"]}
{"id": "orbes-arteagaAugmentationBasedUnsupervised2022", "title": "Augmentation Based Unsupervised Domain Adaptation", "abstract": "The insertion of deep learning in medical image analysis had lead to the development of state-of-the art strategies in several applications such a disease classification, as well as abnormality detection and segmentation. However, even the most advanced methods require a huge and diverse amount of data to generalize. Because in realistic clinical scenarios, data acquisition and annotation is expensive, deep learning models trained on small and unrepresentative data tend to outperform when deployed in data that differs from the one used for training (e.g data from different scanners). In this work, we proposed a domain adaptation methodology to alleviate this problem in segmentation models. Our approach takes advantage of the properties of adversarial domain adaptation and consistency training to achieve more robust adaptation. Using two datasets with white matter hyperintensities (WMH) annotations, we demonstrated that the proposed method improves model generalization even in corner cases where individual strategies tend to fail.", "venue": "arXiv", "keywords": ["consistency training", "domain adaptation", "generalization quantification"]}
{"id": "ovinnikovPoincarWassersteinAutoencoder2020", "title": "Poincar 'e Wasserstein Autoencoder", "abstract": "This work presents a reformulation of the recently proposed Wasserstein autoencoder framework on a non-Euclidean manifold, the Poincar 'e ball model of the hyperbolic space. By assuming the latent space to be hyperbolic, we can use its intrinsic hierarchy to impose structure on the learned latent space representations. We demonstrate the model in the visual domain to analyze some of its properties and show competitive results on a graph link prediction task.", "venue": "arXiv", "keywords": ["manifold learning"]}
{"id": "pajouheshgarDyNCARealtimeDynamic2023", "title": "DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata", "abstract": "Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic videos. However, they require a slow iterative optimization process to synthesize a single fixed-size short video, and they do not offer any post-training control over the synthesis process. We propose Dynamic Neural Cellular Automata (DyNCA), a framework for real-time and controllable dynamic texture synthesis. Our method is built upon the recently introduced NCA models and can synthesize infinitely long and arbitrary-sized realistic video textures in real time. We quantitatively and qualitatively evaluate our model and show that our synthesized videos appear more realistic than the existing results. We improve the SOTA DyTS performance by \\ orders of magnitude. Moreover, our model offers several real-time video controls including motion speed, motion direction, and an editing brush tool. We exhibit our trained models in an online interactive demo that runs on local hardware and is accessible on personal computers and smartphones.", "venue": "arXiv", "keywords": ["cellular automata", "texture transfer"]}
{"id": "palaniappanShrinkageEstimatorsShrinking2018", "title": "Shrinkage Estimators: Shrinking Statistical Estimates", "abstract": "In this article series on how to optimize portfolios, we have looked at the existence of market invariants, estimating distribution of", "venue": "Engineer Quant", "keywords": []}
{"id": "pampariUnsupervisedCalibrationCovariate2020", "title": "Unsupervised Calibration under Covariate Shift", "abstract": "A probabilistic model is said to be calibrated if its predicted probabilities match the corresponding empirical frequencies. Calibration is important for uncertainty quantification and decision making in safety-critical applications. While calibration of classifiers has been widely studied, we find that calibration is brittle and can be easily lost under minimal covariate shifts. Existing techniques, including domain adaptation ones, primarily focus on prediction accuracy and do not guarantee calibration neither in theory nor in practice. In this work, we formally introduce the problem of calibration under domain shift, and propose an importance sampling based approach to address it. We evaluate and discuss the efficacy of our method on both real-world datasets and synthetic datasets.", "venue": "arXiv.org", "keywords": ["domain generalization", "representation learning"]}
{"id": "panagiotakopoulosOnlineDomainAdaptation2022", "title": "Online Domain Adaptation for Semantic Segmentation in Ever-Changing Conditions", "abstract": "Unsupervised Domain Adaptation (UDA) aims at reducing the domain gap between training and testing data and is, in most cases, carried out in offline manner. However, domain changes may occur continuously and unpredictably during deployment (e.g. sudden weather changes). In such conditions, deep neural networks witness dramatic drops in accuracy and offline adaptation may not be enough to contrast it. In this paper, we tackle Online Domain Adaptation (OnDA) for semantic segmentation. We design a pipeline that is robust to continuous domain shifts, either gradual or sudden, and we evaluate it in the case of rainy and foggy scenarios. Our experiments show that our framework can effectively adapt to new domains during deployment, while not being affected by catastrophic forgetting of the previous domains.", "venue": "arXiv", "keywords": ["adverse weather", "online da", "self-training", "semantic segmentation", "unsupervised da"]}
{"id": "pandeyGeneralizationUnseenDomains2021", "title": "Generalization on Unseen Domains via Inference-Time Label-Preserving Target Projections", "abstract": "", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": ["domain generalization", "domain projection", "manifold learning", "representation learning"]}
{"id": "panDomainAdaptationTransfer2011", "title": "Domain Adaptation via Transfer Component Analysis", "abstract": "Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.", "venue": "IEEE Transactions on Neural Networks", "keywords": ["domain adaptation", "feature engineering"]}
{"id": "pandyaMethodCustomizableAutomated2020", "title": "Method for Customizable Automated Tagging: Addressing the Problem of Over-tagging and Under-tagging Text Documents", "abstract": "Using author provided tags to predict tags for a new document often results in the overgeneration of tags. In the case where the author doesn't provide any tags, our documents face the severe under-tagging issue. In this paper, we present a method to generate a universal set of tags that can be applied widely to a large document corpus. Using IBM Watson's NLU service, first, we collect keywords/phrases that we call \"complex document tags\" from 8,854 popular reports in the corpus. We apply LDA model over these complex document tags to generate a set of 765 unique \"simple tags\". In applying the tags to a corpus of documents, we run each document through the IBM Watson NLU and apply appropriate simple tags. Using only 765 simple tags, our method allows us to tag 87,397 out of 88,583 total documents in the corpus with at least one tag. About 92.1% of the total 87,397 documents are also determined to be sufficiently-tagged. In the end, we discuss the performance of our method and its limitations.", "venue": "arXiv", "keywords": []}
{"id": "panMoDALeveragingMotion2023", "title": "MoDA: Leveraging Motion Priors from Videos for Advancing Unsupervised Domain Adaptation in Semantic Segmentation", "abstract": "Unsupervised domain adaptation (UDA) is an effective approach to handle the lack of annotations in the target domain for the semantic segmentation task. In this work, we consider a more practical UDA setting where the target domain contains sequential frames of the unlabeled videos which are easy to collect in practice. A recent study suggests selfsupervised learning of the object motion from unlabeled videos with geometric constraints. We design a motion-guided domain adaptive semantic segmentation framework (MoDA), that utilizes self-supervised object motion to learn effective representations in the target domain. MoDA differs from previous methods that use temporal consistency regularization for the target domain frames. Instead, MoDA deals separately with the domain alignment on the foreground and background categories using different strategies. Specifically, MoDA contains foreground object discovery and foreground semantic mining to align the foreground domain gaps by taking the instance-level guidance from the object motion. Additionally, MoDA includes background adversarial training which contains a background category-specific discriminator to handle the background domain gaps. Experimental results on multiple benchmarks highlight the effectiveness of MoDA against existing approaches in the domain adaptive image segmentation and domain adaptive video segmentation. Moreover, MoDA is versatile and can be used in conjunction with existing state-ofthe-art approaches to further improve performance.", "venue": "arXiv", "keywords": ["self-supervised learning", "semantic segmentation", "temporal consistency", "unsupervised da"]}
{"id": "panSurveyVectorDatabase2023", "title": "Survey of Vector Database Management Systems", "abstract": "There are now over 20 commercial vector database management systems (VDBMSs), all produced within the past five years. But embedding-based retrieval has been studied for over ten years, and similarity search a staggering half century and more. Driving this shift from algorithms to systems are new data intensive applications, notably large language models, that demand vast stores of unstructured data coupled with reliable, secure, fast, and scalable query processing capability. A variety of new data management techniques now exist for addressing these needs, however there is no comprehensive survey to thoroughly review these techniques and systems. We start by identifying five main obstacles to vector data management, namely vagueness of semantic similarity, large size of vectors, high cost of similarity comparison, lack of natural partitioning that can be used for indexing, and difficulty of efficiently answering hybrid queries that require both attributes and vectors. Overcoming these obstacles has led to new approaches to query processing, storage and indexing, and query optimization and execution. For query processing, a variety of similarity scores and query types are now well understood; for storage and indexing, techniques include vector compression, namely quantization, and partitioning based on randomization, learning partitioning, and navigable partitioning; for query optimization and execution, we describe new operators for hybrid queries, as well as techniques for plan enumeration, plan selection, and hardware accelerated execution. These techniques lead to a variety of VDBMSs across a spectrum of design and runtime characteristics, including native systems specialized for vectors and extended systems that incorporate vector capabilities into existing systems. We then discuss benchmarks, and finally we outline research challenges and point the direction for future work.", "venue": "arXiv", "keywords": ["surveys", "vector databases"]}
{"id": "panUnsupervisedIntradomainAdaptation2020", "title": "Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision", "abstract": "Convolutional neural network-based approaches have achieved remarkable progress in semantic segmentation. However, these approaches heavily rely on annotated data which are labor intensive. To cope with this limitation, automatically annotated data generated from graphic engines are used to train segmentation models. However, the models trained from synthetic data are difficult to transfer to real images. To tackle this issue, previous works have considered directly adapting models from the source data to the unlabeled target data (to reduce the inter-domain gap). Nonetheless, these techniques do not consider the large distribution gap among the target data itself (intra-domain gap). In this work, we propose a two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together. First, we conduct the interdomain adaptation of the model; from this adaptation, we separate the target domain into an easy and hard split using an entropy-based ranking function. Finally, to decrease the intra-domain gap, we propose to employ a self-supervised adaptation technique from the easy to the hard split. Experimental results on numerous benchmark datasets highlight the effectiveness of our method against existing state-of-theart approaches. The source code is available at https: //github.com/feipan664/IntraDA.git.", "venue": "arXiv", "keywords": ["curriculum learning", "self-supervised learning", "semantic segmentation", "uncertainty quantification", "unsupervised da"]}
{"id": "papamakariosMaskedAutoregressiveFlow2018", "title": "Masked Autoregressive Flow for Density Estimation", "abstract": "Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.", "venue": "arXiv", "keywords": ["autoregressive flows", "distribution estimation", "vaes", "variational inference"]}
{"id": "papamakariosNormalizingFlowsProbabilistic", "title": "Normalizing Flows for Probabilistic Modeling and Inference", "abstract": "Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.", "venue": "", "keywords": ["generative models", "normalizing flows", "surveys"]}
{"id": "papamakariosNormalizingFlowsProbabilistic2021", "title": "Normalizing Flows for Probabilistic Modeling and Inference", "abstract": "Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.", "venue": "Journal of Machine Learning Research", "keywords": ["highly-analytical", "normalizing flows", "surveys"]}
{"id": "PapersCodeAligning", "title": "Papers with Code - Aligning Latent and Image Spaces to Connect the Unconnectable", "abstract": "🏆 SOTA for Infinite Image Generation on LHQ (InfinityFID metric)", "venue": "", "keywords": []}
{"id": "PapersCodeFlare7K", "title": "Papers with Code - Flare7K Dataset", "abstract": "Flare7K, the first nighttime flare removal dataset, which is generated based on the observation and statistic of real-world nighttime lens flares. It offers 5,000 scattering flare images and 2,000 reflective flare images, consisting of 25 types of scattering flares and 10 types of reflective flares. The 7,000 flare patterns can be randomly added to the flare-free images, forming the flare-corrupted and flare-free image pairs.", "venue": "", "keywords": ["automotive occlusion", "dataset debut"]}
{"id": "PapersCodeLSUN", "title": "Papers with Code - LSUN Dataset", "abstract": "The Large-scale Scene Understanding (LSUN) challenge aims to provide a different benchmark for large-scale scene classification and understanding. The LSUN classification dataset contains 10 scene categories, such as dining room, bedroom, chicken, outdoor church, and so on. For training data, each category contains a huge number of images, ranging from around 120,000 to 3,000,000. The validation data includes 300 images, and the test data has 1000 images for each category.", "venue": "", "keywords": []}
{"id": "parkArbitraryStyleTransfer2019", "title": "Arbitrary Style Transfer with Style-Attentional Networks", "abstract": "Arbitrary style transfer aims to synthesize a content image with the style of an image to create a third image that has never been seen before. Recent arbitrary style transfer algorithms find it challenging to balance the content structure and the style patterns. Moreover, simultaneously maintaining the global and local style patterns is difficult due to the patch-based mechanism. In this paper, we introduce a novel style-attentional network (SANet) that efficiently and flexibly integrates the local style patterns according to the semantic spatial distribution of the content image. A new identity loss function and multi-level feature embeddings enable our SANet and decoder to preserve the content structure as much as possible while enriching the style patterns. Experimental results demonstrate that our algorithm synthesizes stylized images in real-time that are higher in quality than those produced by the state-of-the-art algorithms.", "venue": "arXiv", "keywords": ["cnns", "style transfer"]}
{"id": "parkNeuralRenderingBased3D2023", "title": "Neural Rendering-Based 3D Scene Style Transfer Method via Semantic Understanding Using a Single Style Image", "abstract": "In the rapidly emerging era of untact (``contact-free'') technologies, the requirement for three-dimensional (3D) virtual environments utilized in virtual reality (VR)/augmented reality (AR) and the metaverse has seen significant growth, owing to their extensive application across various domains. Current research focuses on the automatic transfer of the style of rendering images within a 3D virtual environment using artificial intelligence, which aims to minimize human intervention. However, the prevalent studies on rendering-based 3D environment-style transfers have certain inherent limitations. First, the training of a style transfer network dedicated to 3D virtual environments demands considerable style image data. These data must align with viewpoints that closely resemble those of the virtual environment. Second, there was noticeable inconsistency within the 3D structures. Predominant studies often neglect 3D scene geometry information instead of relying solely on 2D input image features. Finally, style adaptation fails to accommodate the unique characteristics inherent in each object. To address these issues, we propose a novel approach: a neural rendering-based 3D scene-style conversion technique. This methodology employs semantic nearest-neighbor feature matching, thereby facilitating the transfer of style within a 3D scene while considering the distinctive characteristics of each object, even when employing a single style image. The neural radiance field enables the network to comprehend the geometric information of a 3D scene in relation to its viewpoint. Subsequently, it transfers style features by employing the unique features of a single style image via semantic nearest-neighbor feature matching. In an empirical context, our proposed semantic 3D scene style transfer method was applied to 3D scene style transfers for both interior and exterior environments. This application utilizes the replica, 3DFront, and Tanks and Temples datasets for testing. The results illustrate that the proposed methodology surpasses existing style transfer techniques in terms of maintaining 3D viewpoint consistency, style uniformity, and semantic coherence.", "venue": "Mathematics", "keywords": ["3d scenes", "style transfer"]}
{"id": "parkPACPredictionSets2021", "title": "PAC Prediction Sets Under Covariate Shift", "abstract": "An important challenge facing modern machine learning is how to rigorously quantify the uncertainty of model predictions. Conveying uncertainty is especially important when there are changes to the underlying data distribution that might invalidate the predictive model. Yet, most existing uncertainty quantification algorithms break down in the presence of such shifts. We propose a novel approach that addresses this challenge by constructing approximately correct (PAC)\\ prediction sets in the presence of covariate shift. Our approach focuses on the setting where there is a covariate shift from the source distribution (where we have labeled training examples) to the target distribution (for which we want to quantify uncertainty). Our algorithm assumes given importance weights that encode how the probabilities of the training examples change under the covariate shift. In practice, importance weights typically need to be estimated; thus, we extend our algorithm to the setting where we are given confidence intervals for the importance weights. We demonstrate the effectiveness of our approach on covariate shifts based on DomainNet and ImageNet. Our algorithm satisfies the PAC constraint, and gives prediction sets with the smallest average normalized size among approaches that always satisfy the PAC constraint.", "venue": "arXiv.org", "keywords": ["domain generalization", "domain projection", "highly-analytical", "prediction sets"]}
{"id": "parkPretrainedVisionLanguage2024", "title": "Pre-Trained Vision and Language Transformers Are Few-Shot Incremental Learners", "abstract": "Few-Shot Class Incremental Learning (FSCIL) is a task that requires a model to learn new classes incrementally without forgetting when only a few samples for each class are given. FSCIL encounters two significant challenges: catastrophic forgetting and overfitting, and these challenges have driven prior studies to primarily rely on shallow models, such as ResNet-18. Even though their limited capacity can mitigate both forgetting and overfitting issues, it leads to inadequate knowledge transfer during few-shot incremental sessions. In this paper, we argue that large models such as vision and language transformers pre-trained on large datasets can be excellent few-shot incremental learners. To this end, we propose a novel FSCIL framework called PriViLege, Pre-trained Vision and Language transformers with prompting functions and knowledge distillation. Our framework effectively addresses the challenges of catastrophic forgetting and overfitting in large models through new pre-trained knowledge tuning (PKT) and two losses: entropy-based divergence loss and semantic knowledge distillation loss. Experimental results show that the proposed PriViLege significantly outperforms the existing state-of-the-art methods with a large margin, e.g., +9.38% in CUB200, +20.58% in CIFAR-100, and +13.36% in miniImageNet. Our implementation code is available at https://github.com/KHU-AGI/PriViLege.", "venue": "arXiv", "keywords": ["bias sources", "few-shot learning", "promising", "transfer learning", "transformers"]}
{"id": "parkRealTimeAccurateConsistent2022", "title": "Real-Time, Accurate, and Consistent Video Semantic Segmentation via Unsupervised Adaptation and Cross-Unit Deployment on Mobile Device", "abstract": "This demonstration showcases our innovations on efficient, accurate, and temporally consistent video semantic segmentation on mobile device. We employ our test-time unsupervised scheme, AuxAdapt, to enable the segmentation model to adapt to a given video in an online manner. More specifically, we leverage a small auxiliary network to perform weight updates and keep the large, main segmentation network frozen. This significantly reduces the computational cost of adaptation when compared to previous methods (e.g., Tent, DVP), and at the same time, prevents catastrophic forgetting. By running AuxAdapt, we can considerably improve the temporal consistency of video segmentation while maintaining the accuracy.", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["auxillary networks", "semantic segmentation", "temporal consistency", "test-time da", "unsupervised da", "unsupervised learning"]}
{"id": "parkSemanticImageSynthesis2019", "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization", "abstract": "We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at https://github.com/NVlabs/SPADE .", "venue": "arXiv", "keywords": ["image synthesis", "instance normalization"]}
{"id": "parkStyleformerTransformerBased2022", "title": "Styleformer: Transformer Based Generative Adversarial Networks with Style Vector", "abstract": "We propose Styleformer, which is a style-based generator for GAN architecture, but a convolution-free transformer-based generator. In our paper, we explain how a transformer can generate high-quality images, overcoming the disadvantage that convolution operations are difficult to capture global features in an image. Furthermore, we change the demodulation of StyleGAN2 and modify the existing transformer structure (e.g., residual connection, layer normalization) to create a strong style-based generator with a convolution-free structure. We also make Styleformer lighter by applying Linformer, enabling Styleformer to generate higher resolution images and result in improvements in terms of speed and memory. We experiment with the low-resolution image dataset such as CIFAR-10, as well as the high-resolution image dataset like LSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark dataset, which is comparable performance to the current state-of-the-art and outperforms all GAN-based generative models, including StyleGAN2-ADA with fewer parameters on the unconditional setting. We also both achieve new state-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10 and CelebA. We release our code at https://github.com/Jeeseung-Park/Styleformer.", "venue": "arXiv", "keywords": ["gans", "style transfer", "transformers"]}
{"id": "paulFishSegSSLSemiSupervisedSemantic2024", "title": "FishSegSSL: A Semi-Supervised Semantic Segmentation Framework for Fish-Eye Images", "abstract": "The application of large field-of-view (FoV) cameras equipped with fish-eye lenses brings notable advantages to various real-world computer vision applications, including autonomous driving. While deep learning has proven successful in conventional computer vision applications using regular perspective images, its potential in fish-eye camera contexts remains largely unexplored due to limited datasets for fully supervised learning. Semi-supervised learning comes as a potential solution to manage this challenge. In this study, we explore and benchmark two popular semi-supervised methods from the perspective image domain for fish-eye image segmentation. We further introduce FishSegSSL, a novel fish-eye image segmentation framework featuring three semi-supervised components: pseudo-label filtering, dynamic confidence thresholding, and robust strong augmentation. Evaluation on the WoodScape dataset, collected from vehicle-mounted fish-eye cameras, demonstrates that our proposed method enhances the model's performance by up to 10.49% over fully supervised methods using the same amount of labeled data. Our method also improves the existing image segmentation methods by 2.34%. To the best of our knowledge, this is the first work on semi-supervised semantic segmentation on fish-eye images. Additionally, we conduct a comprehensive ablation study and sensitivity analysis to showcase the efficacy of each proposed method in this research.", "venue": "Journal of Imaging", "keywords": ["curriculum learning", "fisheye distortion", "mixture augmentations", "semantic segmentation", "semi-supervised learning"]}
{"id": "pawanprasadVDESIRRVeryFast2021", "title": "V-DESIRR: Very Fast Deep Embedded Single Image Reflection Removal", "abstract": "Real world images often gets corrupted due to unwanted reflections and their removal is highly desirable. A major share of such images originate from smart phone cameras capable of very high resolution captures. Most of the existing methods either focus on restoration quality by compromising on processing speed and memory requirements or, focus on removing reflections at very low resolutions, there by limiting their practical deploy-ability. We propose a light weight deep learning model for reflection removal using a novel scale space architecture. Our method processes the corrupted image in two stages, a Low Scale Sub-network (LSSNet) to process the lowest scale and a Progressive Inference (PI) stage to process all the higher scales. In order to reduce the computational complexity, the sub-networks in PI stage are designed to be much shallower than LSSNet. Moreover, we employ weight sharing between various scales within the PI stage to limit the model size. This also allows our method to generalize to very high resolutions without explicit retraining. Our method is superior both qualitatively and quantitatively compared to the state of the art methods and at the same time 20 faster with 50 less number of parameters compared to the most recent stateof-the-art algorithm RAGNet. We implemented our method on an android smart phone, where a high resolution 12 MP image is restored in under 5 seconds.", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "keywords": ["image-to-image"]}
{"id": "PDFPrincipledDisentanglement", "title": "[PDF] Towards Principled Disentanglement for Domain Generalization Semantic Scholar", "abstract": "", "venue": "", "keywords": []}
{"id": "pearlNANNoiseAwareNeRFs2022", "title": "NAN: Noise-Aware NeRFs for Burst-Denoising", "abstract": "Burst denoising is now more relevant than ever, as computational photography helps overcome sensitivity issues inherent in mobile phones and small cameras. A major challenge in burst-denoising is in coping with pixel misalignment, which was so far handled with rather simplistic assumptions of simple motion, or the ability to align in pre-processing. Such assumptions are not realistic in the presence of large motion and high levels of noise. We show that Neural Radiance Fields (NeRFs), originally suggested for physics-based novel-view rendering, can serve as a powerful framework for burst denoising. NeRFs have an inherent capability of handling noise as they integrate information from multiple images, but they are limited in doing so, mainly since they build on pixel-wise operations which are suitable to ideal imaging conditions. Our approach, termed NAN, leverages inter-view and spatial information in NeRFs to better deal with noise. It achieves state-of-the-art results in burst denoising and is especially successful in coping with large movement and occlusions, under very high levels of noise. With the rapid advances in accelerating NeRFs, it could provide a powerful platform for denoising in challenging environments.", "venue": "arXiv.org", "keywords": ["neural radiance fields"]}
{"id": "pedregosaScikitlearnMachineLearning2018", "title": "Scikit-Learn: Machine Learning in Python", "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.", "venue": "arXiv", "keywords": []}
{"id": "pegalajarMunsellColourbasedApproach2020", "title": "A Munsell Colour-Based Approach for Soil Classification Using Fuzzy Logic and Artificial Neural Networks", "abstract": "Munsell soil-colour charts are widely used for soil classification. These charts contain 238 standardised colours in small rectangular chips arranged in seven charts and encoded in the Munsell system. Each chart uses three coordinates well correlated with the visual colour attributes: hue, value and chroma. The colour of a soil sample is commonly estimated by visual comparison between the actual soil colour and the Munsell chips, looking for the closest one and taking its Munsell notation. Consequently, the visual determination of soil colour with Munsell charts is a difficult task due to the subjectivity of the observer to match the colour of a soil sample with a single standard Munsell chip. For this reason, to avoid misclassification caused by subjective coincidence, we propose an intelligent method to provide the closest values of the Munsell chips to an unknown colour of a soil sample by using artificial neural networks and fuzzy logic.", "venue": "Fuzzy Sets and Systems", "keywords": []}
{"id": "pengLargeKernelMatters2017", "title": "Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network", "abstract": "One of recent trends in network architec- ture design is stacking small filters (e.g., 1x1 or 3x3) in the entire network because the stacked small filters is more ef- ficient than a large kernel, given the same computational complexity. However, in the field of semantic segmenta- tion, where we need to perform dense per-pixel prediction, we find that the large kernel (and effective receptive field) plays an important role when we have to perform the clas- sification and localization tasks simultaneously. Following our design principle, we propose a Global Convolutional Network to address both the classification and localization issues for the semantic segmentation. We also suggest a residual-based boundary refinement to further refine the ob- ject boundaries. Our approach achieves state-of-art perfor- mance on two public benchmarks and significantly outper- forms previous results, 82.2% (vs 80.2%) on PASCAL VOC 2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset.", "venue": "arXiv", "keywords": ["bias sources", "cnns", "semantic segmentation"]}
{"id": "pengOutofDomainGeneralizationSingle2024", "title": "Out-of-Domain Generalization From a Single Source: An Uncertainty Quantification Approach", "abstract": "We are concerned with a worst-case scenario in model generalization, in the sense that a model aims to perform well on many unseen domains while there is only one single domain available for training. We propose Meta-Learning based Adversarial Domain Augmentation to solve this Out-of-Domain generalization problem. The key idea is to leverage adversarial training to create ``fictitious'' yet ``challenging'' populations, from which a model can learn to generalize with theoretical guarantees. To facilitate fast and desirable domain augmentation, we cast the model training in a meta-learning scheme and use a Wasserstein Auto-Encoder to relax the widely used worst-case constraint. We further improve our method by integrating uncertainty quantification for efficient domain generalization. Extensive experiments on multiple benchmark datasets indicate its superior performance in tackling single domain generalization.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "keywords": ["domain generalization", "generalization certification", "generalization quantification", "uncertainty quantification"]}
{"id": "pengSemanticAwareDomainGeneralized2022", "title": "Semantic-Aware Domain Generalized Segmentation", "abstract": "Deep models trained on source domain lack generalization when evaluated on unseen target domains with different data distributions. The problem becomes even more pronounced when we have no access to target domain samples for adaptation. In this paper, we address domain generalized semantic segmentation, where a segmentation model is trained to be domain-invariant without using any target domain data. Existing approaches to tackle this problem standardize data into a unified distribution. We argue that while such a standardization promotes global normalization, the resulting features are not discriminative enough to get clear segmentation boundaries. To enhance separation between categories while simultaneously promoting domain invariance, we propose a framework including two novel modules: Semantic-Aware Normalization (SAN) and Semantic-Aware Whitening (SAW). Specifically, SAN focuses on category-level center alignment between features from different image styles, while SAW enforces distributed alignment for the already center-aligned features. With the help of SAN and SAW, we encourage both intra-category compactness and inter-category separability. We validate our approach through extensive experiments on widely-used datasets (i.e. GTAV, SYNTHIA, Cityscapes, Mapillary and BDDS). Our approach shows significant improvements over existing state-of-the-art on various backbone networks. Code is available at https://github.com/leolyj/SAN-SAW", "venue": "arXiv", "keywords": []}
{"id": "pengSemanticAwareDomainGeneralized2022a", "title": "Semantic-Aware Domain Generalized Segmentation", "abstract": "", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": ["domain generalization", "feature whitening", "feature-level augmentation", "instance normalization", "semantic segmentation"]}
{"id": "perezEffectivenessDataAugmentation2017", "title": "The Effectiveness of Data Augmentation in Image Classification Using Deep Learning", "abstract": "In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.", "venue": "arXiv", "keywords": []}
{"id": "perezEffectivenessDataAugmentation2017a", "title": "The Effectiveness of Data Augmentation in Image Classification Using Deep Learning", "abstract": "In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.", "venue": "arXiv", "keywords": ["gans", "surveys"]}
{"id": "PersistentHomology2024", "title": "Persistent Homology", "abstract": "See homology for an introduction to the notation. Persistent homology is a method for computing topological features of a space at different spatial resolutions. More persistent features are detected over a wide range of spatial scales and are deemed more likely to represent true features of the underlying space rather than artifacts of sampling, noise, or particular choice of parameters. To find the persistent homology of a space, the space must first be represented as a simplicial complex. A distance function on the underlying space corresponds to a filtration of the simplicial complex, that is a nested sequence of increasing subsets. One common method of doing this is via taking the sublevel filtration of the distance to a point cloud, or equivalently, the offset filtration on the point cloud and taking its nerve in order to get the simplicial filtration known as Cech filtration. A similar construction uses a nested sequence of Vietoris--Rips complexes known as the Vietoris--Rips filtration.", "venue": "Wikipedia", "keywords": []}
{"id": "peteiro-barralSurveyMethodsDistributed2013", "title": "A Survey of Methods for Distributed Machine Learning", "abstract": "Traditionally, a bottleneck preventing the development of more intelligent systems was the limited amount of data available. Nowadays, the total amount of information is almost incalculable and automatic data analyzers are even more needed. However, the limiting factor is the inability of learning algorithms to use all the data to learn within a reasonable time. In order to handle this problem, a new field in machine learning has emerged: large-scale learning. In this context, distributed learning seems to be a promising line of research since allocating the learning process among several workstations is a natural way of scaling up learning algorithms. Moreover, it allows to deal with data sets that are naturally distributed, a frequent situation in many real applications. This study provides some background regarding the advantages of distributed environments as well as an overview of distributed learning for dealing with ``very large'' data sets.", "venue": "Progress in Artificial Intelligence", "keywords": []}
{"id": "pfeufferADUULMDatasetSemanticSegmentation", "title": "The ADUULM-Dataset - A Semantic Segmentation Dataset for Sensor Fusion", "abstract": "One of the key challenges of today's semantic segmentation approaches is to obtain robust and reliable segmentation results not only in good weather conditions, but also in adverse weather conditions such as darkness, fog or heavy rain. For this purpose, multiple sensor data of several sensor types such as camera and lidar are required to compensate the weather sensitivity of individual sensors. Hence, a semantic segmentation dataset is necessary, which contains camera and lidar data, but until recently, no such dataset exists. Therefore, the ADUULM dataset was created, a semantic segmentation dataset which consists of fine-annotated camera data and pixel-wise labeled lidar data recorded in diverse weather conditions. Additionally, the corresponding GPS, IMU and stereo information are provided, and for each annotated data sample, a short video-sequence is available, too. Furthermore, state-of-the-art semantic segmentation and drivable area detection approaches are evaluated on the proposed dataset, and it turned out that new methods are required to obtain robust and reliable results in adverse weather conditions. The ADUULM-dataset will be available online at https: //www.uni-ulm.de/in/iui-drive-u/projekte/aduulm-dataset/.", "venue": "", "keywords": []}
{"id": "pfeufferRobustEnvironmentPerception2023", "title": "Robust Environment Perception in Adverse Weather Conditions", "abstract": "A robust and reliable environment perception in all weather conditions is an important premise for autonomous vehicles. While current environment perception systems based on neural networks such as semantic segmentation approaches deliver good results in good weather conditions, being confronted with adverse weather conditions such as rain, fog or snow is still a challenging task today. The reason is that they are disturbed by adverse weather effects which may cause sensor disturbances or bad visibility. This thesis addresses these difficulties coming up with adverse weather effects. Two different approaches are presented how to overcome this problem and to increase robustness and performance of environment perception systems in adverse weather conditions in the case of semantic segmentation. One possibility to improve the segmentation accuracy is to consider image information of previous video frames as well as the current video frame. This reduces the number of ghost objects and the number of flickering pixels within a video sequence. For instance, temporal image information can be captured by means of recurrent neural networks. Therefore, a state-of-the-art segmentation approach is extended with recurrent units. The drawback of this video-segmentation method is that it is time-intensive so that its network architecture and the recurrent units are optimized further to accelerate the proposed segmentation approach with only a slight loss of performance. The second approach how to increase the robustness in adverse weather conditions is to use not only camera information but also additional environment information from other sensors, e.g. lidar sensor. The motivation is that different sensor types are disturbed differently in diverse adverse weather conditions. The failure of one sensor may be still compensated by the environment data recorded by the other sensors. In this thesis, a semantic segmentation approach is developed, which uses camera and lidar data to increase the overall performance and robustness in adverse weather conditions. More concretely, the sensor data are fused at proper positions by means of a fusion unit, which weights each modality by means of its noise level. Noiseless sensor data are weighted more strongly than disturbed sensor data, so that the neural network is affected by noisy data to a smaller extent. Finally, both approaches are evaluated on different datasets and are compared with current state-of-the-art segmentation approaches. Special attention is paid to their performance and robustness in various adverse weather conditions. It turns out that the use of temporal information and the use of different sensor data increase the robustness and performance of current segmentation approaches in these conditions.", "venue": "Universit\\\"at Ulm", "keywords": []}
{"id": "pfeufferRobustEnvironmentPerception2023a", "title": "Robust Environment Perception in Adverse Weather Conditions", "abstract": "A robust and reliable environment perception in all weather conditions is an important premise for autonomous vehicles. While current environment perception systems based on neural networks such as semantic segmentation approaches deliver good results in good weather conditions, being confronted with adverse weather conditions such as rain, fog or snow is still a challenging task today. The reason is that they are disturbed by adverse weather effects which may cause sensor disturbances or bad visibility. This thesis addresses these difficulties coming up with adverse weather effects. Two different approaches are presented how to overcome this problem and to increase robustness and performance of environment perception systems in adverse weather conditions in the case of semantic segmentation. One possibility to improve the segmentation accuracy is to consider image information of previous video frames as well as the current video frame. This reduces the number of ghost objects and the number of flickering pixels within a video sequence. For instance, temporal image information can be captured by means of recurrent neural networks. Therefore, a state-of-the-art segmentation approach is extended with recurrent units. The drawback of this video-segmentation method is that it is time-intensive so that its network architecture and the recurrent units are optimized further to accelerate the proposed segmentation approach with only a slight loss of performance. The second approach how to increase the robustness in adverse weather conditions is to use not only camera information but also additional environment information from other sensors, e.g. lidar sensor. The motivation is that different sensor types are disturbed differently in diverse adverse weather conditions. The failure of one sensor may be still compensated by the environment data recorded by the other sensors. In this thesis, a semantic segmentation approach is developed, which uses camera and lidar data to increase the overall performance and robustness in adverse weather conditions. More concretely, the sensor data are fused at proper positions by means of a fusion unit, which weights each modality by means of its noise level. Noiseless sensor data are weighted more strongly than disturbed sensor data, so that the neural network is affected by noisy data to a smaller extent. Finally, both approaches are evaluated on different datasets and are compared with current state-of-the-art segmentation approaches. Special attention is paid to their performance and robustness in various adverse weather conditions. It turns out that the use of temporal information and the use of different sensor data increase the robustness and performance of current segmentation approaches in these conditions.", "venue": "", "keywords": ["adverse weather", "dissertations", "semantic segmentation"]}
{"id": "pfeufferRobustSemanticSegmentation2019", "title": "Robust Semantic Segmentation in Adverse Weather Conditions by Means of Sensor Data Fusion", "abstract": "A robust and reliable semantic segmentation in adverse weather conditions is very important for autonomous cars, but most state-of-the-art approaches only achieve high accuracy rates in optimal weather conditions. The reason is that they are only optimized for good weather conditions and given noise models. However, most of them fail, if data with unknown disturbances occur, and their performance decrease enormously. One possibility to still obtain reliable results is to observe the environment with different sensor types, such as camera and lidar, and to fuse the sensor data by means of neural networks, since different sensors behave differently in diverse weather conditions. Hence, the sensors can complement each other by means of an appropriate sensor data fusion. Nevertheless, the fusion-based approaches are still susceptible to disturbances and fail to classify disturbed image areas correctly. This problem can be solved by means of a special training method, the so called Robust Learning Method (RLM), a method by which the neural network learns to handle unknown noise. In this work, two different sensor fusion architectures for semantic segmentation are compared and evaluated on several datasets. Furthermore, it is shown that the RLM increases the robustness in adverse weather conditions enormously, and achieve good results although no disturbance model has been learned by the neural network.", "venue": "arXiv", "keywords": ["adverse weather", "automotive occlusion", "sensor fusion"]}
{"id": "pfeufferRobustSemanticSegmentation2020", "title": "Robust Semantic Segmentation in Adverse Weather Conditions by Means of Fast Video-Sequence Segmentation", "abstract": "Computer vision tasks such as semantic segmentation perform very well in good weather conditions, but if the weather turns bad, they have problems to achieve this performance in these conditions. One possibility to obtain more robust and reliable results in adverse weather conditions is to use video-segmentation approaches instead of commonly used single-image segmentation methods. Video-segmentation approaches capture temporal information of the previous video-frames in addition to current image information, and hence, they are more robust against disturbances, especially if they occur in only a few frames of the video-sequence. However, video-segmentation approaches, which are often based on recurrent neural networks, cannot be applied in real-time applications anymore, since their recurrent structures in the network are computational expensive. For instance, the inference time of the LSTM-ICNet, in which recurrent units are placed at proper positions in the single-segmentation approach ICNet, increases up to 61 percent compared to the basic ICNet. Hence, in this work, the LSTM-ICNet is sped up by modifying the recurrent units of the network so that it becomes real-time capable again. Experiments on different datasets and various weather conditions show that the inference time can be decreased by about 23 percent by these modifications, while they achieve similar performance than the LSTM-ICNet and outperform the single-segmentation approach enormously in adverse weather conditions.", "venue": "arXiv", "keywords": []}
{"id": "phelanUpdatedAcademicWorkflow", "title": "An Updated Academic Workflow: Zotero & Obsidian", "abstract": "", "venue": "", "keywords": []}
{"id": "PIPAL", "title": "PIPAL", "abstract": "", "venue": "", "keywords": []}
{"id": "pivaEmpiricalGeneralizationStudy2023", "title": "Empirical Generalization Study: Unsupervised Domain Adaptation vs. Domain Generalization Methods for Semantic Segmentation in the Wild", "abstract": "", "venue": "2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)", "keywords": ["domain generalization", "generalization quantification", "surveys", "unsupervised da"]}
{"id": "ponimatkinSimplePowerfulGlobal2022", "title": "A Simple and Powerful Global Optimization for Unsupervised Video Object Segmentation", "abstract": "We propose a simple, yet powerful approach for unsupervised object segmentation in videos. We introduce an objective function whose minimum represents the mask of the main salient object over the input sequence. It only relies on independent image features and optical flows, which can be obtained using off-the-shelf self-supervised methods. It scales with the length of the sequence with no need for superpixels or sparsification, and it generalizes to different datasets without any specific training. This objective function can actually be derived from a form of spectral clustering applied to the entire video. Our method achieves on-par performance with the state of the art on standard benchmarks (DAVIS2016, SegTrack-v2, FBMS59), while being conceptually and practically much simpler.", "venue": "arXiv", "keywords": ["loss functions", "unsupervised learning"]}
{"id": "ponnIdentificationExplanationChallenging2020", "title": "Identification and Explanation of Challenging Conditions for Camera-Based Object Detection of Automated Vehicles", "abstract": "For a safe market launch of automated vehicles, the risks of the overall system as well as the sub-components must be efficiently identified and evaluated. This also includes camera-based object detection using artificial intelligence algorithms. It is trivial and explainable that due to the principle of the camera, performance depends highly on the environmental conditions and can be poor, for example in heavy fog. However, there are other factors influencing the performance of camera-based object detection, which will be comprehensively investigated for the first time in this paper. Furthermore, a precise modeling of the detection performance and the explanation of individual detection results is not possible due to the artificial intelligence based algorithms used. Therefore, a modeling approach based on the investigated influence factors is proposed and the newly developed SHapley Additive exPlanations (SHAP) approach is adopted to analyze and explain the detection performance of different object detection algorithms. The results show that many influence factors such as the relative rotation of an object towards the camera or the position of an object on the image have basically the same influence on the detection performance regardless of the detection algorithm used. In particular, the revealed weaknesses of the tested object detectors can be used to derive challenging and critical scenarios for the testing and type approval of automated vehicles.", "venue": "Sensors (Basel, Switzerland)", "keywords": ["adverse weather", "automotive occlusion", "critical citation"]}
{"id": "ponomarevMultiagentSystemsDecentralized2017", "title": "Multi-Agent Systems and Decentralized Artificial Superintelligence", "abstract": "Multi-agents systems communication is a technology, which provides a way for multiple interacting intelligent agents to communicate with each other and with environment. Multiple-agent systems are used to solve problems that are difficult for solving by individual agent. Multiple-agent communication technologies can be used for management and organization of computing fog and act as a global, distributed operating system. In present publication we suggest technology, which combines decentralized P2P BOINC general-purpose computing tasks distribution, multiple-agents communication protocol and smart-contract based rewards, powered by Ethereum blockchain. Such system can be used as distributed P2P computing power market, protected from any central authority. Such decentralized market can further be updated to system, which learns the most efficient way for software-hardware combinations usage and optimization. Once system learns to optimize software-hardware efficiency it can be updated to general-purpose distributed intelligence, which acts as combination of single-purpose AI.", "venue": "arXiv", "keywords": ["multi-task learning"]}
{"id": "prabhuAUGCOAugmentationConsistencyguided2021", "title": "AUGCO: Augmentation Consistency-guided Self-training for Source-free Domain Adaptive Semantic Segmentation", "abstract": "Most modern approaches for domain adaptive semantic segmentation rely on continued access to source data during adaptation, which may be infeasible due to computational or privacy constraints. We focus on source-free domain adaptation for semantic segmentation, wherein a source model must adapt itself to a new target domain given only unlabeled target data. We propose Augmentation Consistency-guided Self-training (AUGCO), a source-free adaptation algorithm that uses the model's pixel-level predictive consistency across diverse, automatically generated views of each target image along with model confidence to identify reliable pixel predictions, and selectively self-trains on those. AUGCO achieves state-of-the-art results for source-free adaptation on 3 standard benchmarks for semantic segmentation, all within a simple to implement and fast to converge method.", "venue": "arXiv.org", "keywords": ["consistency training", "self-training", "semantic segmentation"]}
{"id": "prasadReferenceGuidedReflection2022", "title": "Reference Guided Reflection Removal Using Deep Visual Attribute Cues", "abstract": "Reflections in images are typically caused due to presence of glass like reflective objects or surfaces that affect the overall visual appeal and hence undesirable. There has been extensive interest in the past to use data driven approaches for both single image as well as multi image reflection removal. However, recently there has been only minor incremental improvements in single image reflection removal given the challenging ill-posed nature of the problem. Reference based methods has yielded state of the art performance in areas such as super resolution, however has been unexplored for reflection removal. In this paper, we propose a novel multi-stage deep learning based method for reference based reflection removal. We also propose a novel visual attribute cue that represents the reflection free semantic content of the input image. This cue is generated using the reference image while maintaining the geometric structure of the input image. We formulate the reference based reflection removal problem as extraction of visual attribute cues followed by a guided image restoration. We perform qualitative and quantitative evaluation to demonstrate the superiority of the proposed approach over the existing state of the art single image reflection removal methods.", "venue": "2022 IEEE International Conference on Image Processing (ICIP)", "keywords": ["image-to-image"]}
{"id": "PrimerEnforcingLipschitz2019", "title": "Primer: Enforcing Lipschitz Constraints for Neural Networks", "abstract": "", "venue": "@broadinstitute", "keywords": ["lipschitz-constraints"]}
{"id": "pumarolaCFlowConditionalGenerative2020", "title": "C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds", "abstract": "Flow-based generative models have highly desirable properties like exact log-likelihood evaluation and exact latent-variable inference, however they are still in their infancy and have not received as much attention as alternative generative models. In this paper, we introduce C-Flow, a novel conditioning scheme that brings normalizing flows to an entirely new scenario with great possibilities for multimodal data modeling. C-Flow is based on a parallel sequence of invertible mappings in which a source flow guides the target flow at every step, enabling fine-grained control over the generation process. We also devise a new strategy to model unordered 3D point clouds that, in combination with the conditioning scheme, makes it possible to address 3D reconstruction from a single image and its inverse problem of rendering an image given a point cloud. We demonstrate our conditioning method to be very adaptable, being also applicable to image manipulation, style transfer and multi-modal image-to-image mapping in a diversity of domains, including RGB images, segmentation maps and edge masks.", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["image synthesis", "normalizing flows"]}
{"id": "qiaoTopologyawareRobustOptimization2023", "title": "Topology-Aware Robust Optimization for Out-of-distribution Generalization", "abstract": "Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.", "venue": "[object Object]", "keywords": ["domain generalization", "highly-analytical", "manifold learning", "topological analysis"]}
{"id": "qinLearningUnlearnableAdversarial2023", "title": "Learning the Unlearnable: Adversarial Augmentations Suppress Unlearnable Example Attacks", "abstract": "Unlearnable example attacks are data poisoning techniques that can be used to safeguard public data against unauthorized use for training deep learning models. These methods add stealthy perturbations to the original image, thereby making it difficult for deep learning models to learn from these training data effectively. Current research suggests that adversarial training can, to a certain degree, mitigate the impact of unlearnable example attacks, while common data augmentation methods are not effective against such poisons. Adversarial training, however, demands considerable computational resources and can result in non-trivial accuracy loss. In this paper, we introduce the UEraser method, which outperforms current defenses against different types of state-of-the-art unlearnable example attacks through a combination of effective data augmentation policies and loss-maximizing adversarial augmentations. In stark contrast to the current SOTA adversarial training methods, UEraser uses adversarial augmentations, which extends beyond the confines of \\ perturbation budget assumed by current unlearning attacks and defenses. It also helps to improve the model's generalization ability, thus protecting against accuracy loss. UEraser wipes out the unlearning effect with error-maximizing data augmentations, thus restoring trained model accuracies. Interestingly, UEraser-Lite, a fast variant without adversarial augmentations, is also highly effective in preserving clean accuracies. On challenging unlearnable CIFAR-10, CIFAR-100, SVHN, and ImageNet-subset datasets produced with various attacks, it achieves results that are comparable to those obtained during clean training. We also demonstrate its efficacy against possible adaptive attacks. Our code is open source and available to the deep learning community: https://github.com/lafeat/ueraser.", "venue": "arXiv", "keywords": ["adversarial learning"]}
{"id": "qiOccludedVideoInstance", "title": "Occluded Video Instance Segmentation: Dataset and ICCV 2021 Challenge", "abstract": "Although deep learning methods have achieved advanced video object recognition performance in recent years, perceiving heavily occluded objects in a video is still a very challenging task. To promote the development of occlusion understanding, we collect a large-scale dataset called OVIS for video instance segmentation in the occluded scenario. OVIS consists of 296k high-quality instance masks and 901 occluded scenes. While our human vision systems can perceive those occluded objects by contextual reasoning and association, our experiments suggest that current video understanding systems cannot. On the OVIS dataset, all baseline methods encounter a significant performance degradation of about 80% in the heavily occluded object group, which demonstrates that there is still a long way to go in understanding obscured objects and videos in a complex real-world scenario. To facilitate the research on new paradigms for video understanding systems, we launched a challenge based on the OVIS dataset. The submitted top-performing algorithms have achieved much higher performance than our baselines. In this paper, we will introduce the OVIS dataset and further dissect it by analyzing the results of baselines and submitted methods. The OVIS dataset and challenge information can be found at http://songbai.site/ovis.", "venue": "", "keywords": ["challenges", "dataset debut"]}
{"id": "qiuColoristaNetPhotorealisticVideo2022", "title": "ColoristaNet for Photorealistic Video Style Transfer", "abstract": "Photorealistic style transfer aims to transfer the artistic style of an image onto an input image or video while keeping photorealism. In this paper, we think it's the summary statistics matching scheme in existing algorithms that leads to unrealistic stylization. To avoid employing the popular Gram loss, we propose a self-supervised style transfer framework, which contains a style removal part and a style restoration part. The style removal network removes the original image styles, and the style restoration network recovers image styles in a supervised manner. Meanwhile, to address the problems in current feature transformation methods, we propose decoupled instance normalization to decompose feature transformation into style whitening and restylization. It works quite well in ColoristaNet and can transfer image styles efficiently while keeping photorealism. To ensure temporal coherency, we also incorporate optical flow methods and ConvLSTM to embed contextual information. Experiments demonstrates that ColoristaNet can achieve better stylization effects when compared with state-of-the-art algorithms.", "venue": "arXiv", "keywords": ["favorite", "style transfer"]}
{"id": "rabinRegularizationTransportationMaps2010", "title": "Regularization of Transportation Maps for Color and Contrast Transfer", "abstract": "In this paper, we take interest in the process of assigning a given color distribution to a given image. Two examples of such an image modification are histogram equalization (or specification) and color transfer, in which the color palette of a style image is assigned to a source image. Classical methods for gray level specification, as well as more recent methods for color transfer, can be defined as optimal transportation problems. The corresponding image modifications are known to produce visually unpleasing effects such as the removal of details and texture, as well as the enhancement of noise or compression patterns. In this paper, a new method is proposed for the suppression of these artifacts. The method relies on a non local regularization of the transportation map, defined as the difference between the original image and the modified one. The interest of using this method is demonstrated on the aforementioned applications: contrast adjustment and color transfer.", "venue": "2010 IEEE International Conference on Image Processing", "keywords": ["optimal transport", "style transfer"]}
{"id": "radfordUnsupervisedRepresentationLearning2016", "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.", "venue": "arXiv", "keywords": ["adversarial learning", "foundational", "gans", "image-to-image", "representation learning", "unsupervised learning"]}
{"id": "rafiDomainGeneralizationSemantic2024", "title": "Domain Generalization for Semantic Segmentation: A Survey", "abstract": "Deep neural networks (DNNs) have proven explicit contributions in making autonomous driving cars and related tasks such as semantic segmentation, motion tracking, object detection, sensor fusion, and planning. However, in challenging situations, DNNs are not generalizable because of the inherent domain shift due to the nature of training under the i.i.d. assumption. The goal of semantic segmentation is to preserve information from a given image into multiple meaningful categories for visual understanding. Particularly for semantic segmentation, pixel-wise annotation is extremely costly and not always feasible. Domain generalization for semantic segmentation aims to learn pixel-level semantic labels from multiple source domains and generalize to predict pixel-level semantic labels on multiple unseen target domains. In this survey, for the first time, we present a comprehensive review of DG for semantic segmentation. we present a comprehensive summary of recent works related to domain generalization in semantic segmentation, which establishes the importance of generalizing to new environments of segmentation models. Although domain adaptation has gained more attention in segmentation tasks than domain generalization, it is still worth unveiling new trends that are adopted from domain generalization methods in semantic segmentation. We cover most of the recent and dominant DG methods in the context of semantic segmentation and also provide some other related applications. We conclude this survey by highlighting the future directions in this area.", "venue": "Artificial Intelligence Review", "keywords": []}
{"id": "rafiDomainGeneralizationSemantic2024a", "title": "Domain Generalization for Semantic Segmentation: A Survey", "abstract": "Deep neural networks (DNNs) have proven explicit contributions in making autonomous driving cars and related tasks such as semantic segmentation, motion tracking, object detection, sensor fusion, and planning. However, in challenging situations, DNNs are not generalizable because of the inherent domain shift due to the nature of training under the i.i.d. assumption. The goal of semantic segmentation is to preserve information from a given image into multiple meaningful categories for visual understanding. Particularly for semantic segmentation, pixel-wise annotation is extremely costly and not always feasible. Domain generalization for semantic segmentation aims to learn pixel-level semantic labels from multiple source domains and generalize to predict pixel-level semantic labels on multiple unseen target domains. In this survey, for the first time, we present a comprehensive review of DG for semantic segmentation. we present a comprehensive summary of recent works related to domain generalization in semantic segmentation, which establishes the importance of generalizing to new environments of segmentation models. Although domain adaptation has gained more attention in segmentation tasks than domain generalization, it is still worth unveiling new trends that are adopted from domain generalization methods in semantic segmentation. We cover most of the recent and dominant DG methods in the context of semantic segmentation and also provide some other related applications. We conclude this survey by highlighting the future directions in this area.", "venue": "Artificial Intelligence Review", "keywords": ["domain generalization", "semantic segmentation", "surveys"]}
{"id": "raghuVisionTransformersSee", "title": "Do Vision Transformers See Like Convolutional Neural Networks?", "abstract": "Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer.", "venue": "", "keywords": ["transformers"]}
{"id": "rahmanAdversarialPolicyOptimization2023", "title": "Adversarial Policy Optimization in Deep Reinforcement Learning", "abstract": "The policy represented by the deep neural network can overfit the spurious features in observations, which hamper a reinforcement learning agent from learning effective policy. This issue becomes severe in high-dimensional state, where the agent struggles to learn a useful policy. Data augmentation can provide a performance boost to RL agents by mitigating the effect of overfitting. However, such data augmentation is a form of prior knowledge, and naively applying them in environments might worsen an agent's performance. In this paper, we propose a novel RL algorithm to mitigate the above issue and improve the efficiency of the learned policy. Our approach consists of a max-min game theoretic objective where a perturber network modifies the state to maximize the agent's probability of taking a different action while minimizing the distortion in the state. In contrast, the policy network updates its parameters to minimize the effect of perturbation while maximizing the expected future reward. Based on this objective, we propose a practical deep reinforcement learning algorithm, Adversarial Policy Optimization (APO). Our method is agnostic to the type of policy optimization, and thus data augmentation can be incorporated to harness the benefit. We evaluated our approaches on several DeepMind Control robotic environments with high-dimensional and noisy state settings. Empirical results demonstrate that our method APO consistently outperforms the state-of-the-art on-policy PPO agent. We further compare our method with state-of-the-art data augmentation, RAD, and regularization-based approach DRAC. Our agent APO shows better performance compared to these baselines.", "venue": "arXiv", "keywords": ["adversarial learning", "ppo", "reinforcement learning"]}
{"id": "rahmanSinkhornKnoppUnraveling2023", "title": "Sinkhorn Knopp: Unraveling Optimal Transport for Data Alignment", "abstract": "In the realm of data alignment and optimal transport, the Sinkhorn Knopp algorithm has emerged as a powerful tool for solving", "venue": "Medium", "keywords": ["optimal transport"]}
{"id": "rajagopalUnderstandingVisualizingGeneralization2021", "title": "Understanding and Visualizing Generalization in UNets", "abstract": "Fully-convolutional neural networks, such as the 2D or 3D UNet, are now pervasive in medical imaging for semantic segmentation, classification, image denoising, domain translation, and reconstruction. However, evaluation of UNet performance, as with most CNNs, has mostly been relegated to evaluation of a few performance metrics (e.g. accuracy, IoU, SSIM, etc.) using the network's final predictions, which provides little insight into important issues such as dataset shift that occur in clinical application. In this paper, we propose techniques for understanding and visualizing the generalization performance of UNets in image classification and regression tasks, giving rise to metrics that are indicative of performance on a withheld test-set without the need for groundtruth annotations.", "venue": "Proceedings of the Fourth Conference on Medical Imaging with Deep Learning", "keywords": ["clustering", "domain generalization", "feature visualization", "semantic segmentation"]}
{"id": "ramaseshEffectScaleCatastrophic2021", "title": "Effect of Scale on Catastrophic Forgetting in Neural Networks", "abstract": "Catastrophic forgetting presents a challenge in developing deep learning models capable of continual learning, i.e. learning tasks sequentially. Recently, both computer vision and natural-language processing have witnessed great progress through the use of large-scale pretrained models. In this work, we present an empirical study of catastrophic forgetting in this pretraining paradigm. Our experiments indicate that large, pretrained ResNets and Transformers are significantly more resistant to forgetting than randomly-initialized, trained-from-scratch models; this robustness systematically improves with scale of both model and pretraining dataset size. We take initial steps towards characterizing what aspect of model representations allows them to perform continual learning so well, finding that in the pretrained models, distinct class representations grow more orthogonal with scale. Our results suggest that, when possible, scale and a diverse pretraining dataset can be useful ingredients in mitigating catastrophic forgetting.", "venue": "International Conference on Learning Representations", "keywords": ["bias sources"]}
{"id": "ratnerLearningComposeDomainSpecific2017", "title": "Learning to Compose Domain-Specific Transformations for Data Augmentation", "abstract": "Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "ravikumarFisheyeDistanceNetSelfSupervisedFisheye2021", "title": "FisheyeDistanceNet++: Self-Supervised Fisheye Distance Estimation with Self-Attention, Robust Loss Function and Camera View Generalization", "abstract": "FisheyeDistanceNet proposed a self-supervised monocular depth estimation method for fisheye cameras with a large field of view ( 180 ). To achieve scale-invariant depth estimation, FisheyeDistanceNet supervises depth map predictions over multiple scales during training. To overcome this bottleneck, we incorporate self-attention layers and robust loss function to FisheyeDistanceNet. A general adaptive robust loss function helps obtain sharp depth maps without a need to train over multiple scales and allows us to learn hyperparameters in loss function to aid in better optimization in terms of convergence speed and accuracy. We also ablate the importance of Instance Normalization over Batch Normalization in the network architecture. Finally, we generalize the network to be invariant to camera views by training multiple perspectives using front, rear, and side cameras. Proposed algorithm improvements, FisheyeDistanceNet++, result in 30% relative improvement in RMSE while reducing the training time by 25% on the WoodScape dataset. We also obtain state-of-the-art results on the KITTI dataset, in comparison to other self-supervised monocular methods.", "venue": "Electronic Imaging", "keywords": ["fisheye distortion", "homography estimation", "loss functions", "self-supervised learning"]}
{"id": "ravindranRandMSAugmentMixedSampleAugmentation2023", "title": "RandMSAugment: A Mixed-Sample Augmentation for Limited-Data Scenarios", "abstract": "The high costs of annotating large datasets suggests a need for effectively training CNNs with limited data, and data augmentation is a promising direction. We study foundational augmentation techniques, including Mixed Sample Data Augmentations (MSDAs) and a no-parameter variant of RandAugment termed Preset-RandAugment, in the fully supervised scenario. We observe that Preset-RandAugment excels in limited-data contexts while MSDAs are moderately effective. We show that low-level feature transforms play a pivotal role in this performance difference, postulate a new property of augmentations related to their data efficiency, and propose new ways to measure the diversity and realism of augmentations. Building on these insights, we introduce a novel augmentation technique called RandMSAugment that integrates complementary strengths of existing methods. RandMSAugment significantly outperforms the competition on CIFAR-100, STL-10, and Tiny-Imagenet. With very small training sets (4, 25, 100 samples/class), RandMSAugment achieves compelling performance gains between 4.1% and 6.75%. Even with more training data (500 samples/class) we improve performance by 1.03% to 2.47%. RandMSAugment does not require hyperparameter tuning, extra validation data, or cumbersome optimizations.", "venue": "arXiv", "keywords": ["mixture augmentations"]}
{"id": "rebolFrameToFrameConsistentSemantic", "title": "Frame-To-Frame Consistent Semantic Segmentation", "abstract": "In this work, we aim for temporally consistent semantic segmentation throughout frames in a video. Many semantic segmentation algorithms process images individually which leads to an inconsistent scene interpretation due to illumination changes, occlusions and other variations over time. To achieve a temporally consistent prediction, we train a convolutional neural network (CNN) which propagates features through consecutive frames in a video using a convolutional long short term memory (ConvLSTM) cell. Besides the temporal feature propagation, we penalize inconsistencies in our loss function. We show in our experiments that the performance improves when utilizing video information compared to single frame prediction. The mean intersection over union (mIoU) metric on the Cityscapes validation set increases from 45.2 % for the single frames to 57.9 % for video data after implementing the ConvLSTM to propagate features trough time on the ESPNet. Most importantly, inconsistency decreases from 4.5 % to 1.3 % which is a reduction by 71.1 %. Our results indicate that the added temporal information produces a frame-to-frame consistent and more accurate image understanding compared to single frame processing. Code and videos are available at https://github.com/mrebol/f2f-consistent-semantic-segmentation", "venue": "arXiv", "keywords": ["semantic segmentation", "temporal consistency"]}
{"id": "reddyMasterAllSimultaneous2022", "title": "Master of~All: Simultaneous Generalization of~Urban-Scene Segmentation to~All Adverse Weather Conditions", "abstract": "Computer vision systems for autonomous navigation must generalize well in adverse weather and illumination conditions expected in the real world. However, semantic segmentation of images captured in such conditions remains a challenging task for current state-of-the-art (SOTA) methods trained on broad daylight images, due to the associated distribution shift. On the other hand, domain adaptation techniques developed for the purpose rely on the availability of the source data, (un)labeled target data and/or its auxiliary information (e.g., GPS). Even then, they typically adapt to a single(specific) target domain(s). To remedy this, we propose a novel, fully test time, adaptation technique, named Master of ALL (MALL), for simultaneous generalization to multiple target domains. MALL learns to generalize on unseen adverse weather images from multiple target domains directly at the inference time. More specifically, given a pre-trained model and its parameters, MALL enforces edge consistency prior at the inference stage and updates the model based on (a) a single test sample at a time (MALL-sample), or (b) continuously for the whole test domain (MALL-domain). Not only the target data, MALL also does not need access to the source data and thus, can be used with any pre-trained model. Using a simple model pre-trained on daylight images, MALL outperforms specially designed adverse weather semantic segmentation methods, both in domain generalization and test-time adaptation settings. Our experiments on foggy, snow, night, cloudy, overcast, and rainy conditions demonstrate the target domain-agnostic effectiveness of our approach. We further show that MALL can improve the performance of a model on an adverse weather condition, even when the model is already pre-trained for the specific condition.", "venue": "Computer Vision -- ECCV 2022", "keywords": ["adverse weather", "domain adaptation", "test-time da"]}
{"id": "renNewDistributedMethod2021", "title": "A New Distributed Method for Training Generative Adversarial Networks", "abstract": "Generative adversarial networks (GANs) are emerging machine learning models for generating synthesized data similar to real data by jointly training a generator and a discriminator. In many applications, data and computational resources are distributed over many devices, so centralized computation with all data in one location is infeasible due to privacy and/or communication constraints. This paper proposes a new framework for training GANs in a distributed fashion: Each device computes a local discriminator using local data; a single server aggregates their results and computes a global GAN. Specifically, in each iteration, the server sends the global GAN to the devices, which then update their local discriminators; the devices send their results to the server, which then computes their average as the global discriminator and updates the global generator accordingly. Two different update schedules are designed with different levels of parallelism between the devices and the server. Numerical results obtained using three popular datasets demonstrate that the proposed framework can outperform a state-of-the-art framework in terms of convergence speed.", "venue": "arXiv", "keywords": []}
{"id": "renProbabilisticMixtureofExpertsEfficient2021", "title": "Probabilistic Mixture-of-Experts for Efficient Deep Reinforcement Learning", "abstract": "Deep reinforcement learning (DRL) has successfully solved various problems recently, typically with a unimodal policy representation. However, grasping distinguishable skills for some tasks with non-unique optima can be essential for further improving its learning efficiency and performance, which may lead to a multimodal policy represented as a mixture-of-experts (MOE). To our best knowledge, present DRL algorithms for general utility do not deploy this method as policy function approximators due to the potential challenge in its differentiability for policy learning. In this work, we propose a probabilistic mixture-of-experts (PMOE) implemented with a Gaussian mixture model (GMM) for multimodal policy, together with a novel gradient estimator for the indifferentiability problem, which can be applied in generic off-policy and on-policy DRL algorithms using stochastic policies, e.g., Soft Actor-Critic (SAC) and Proximal Policy Optimisation (PPO). Experimental results testify the advantage of our method over unimodal polices and two different MOE methods, as well as a method of option frameworks, based on the above two types of DRL algorithms, on six MuJoCo tasks. Different gradient estimations for GMM like the reparameterisation trick (Gumbel-Softmax) and the score-ratio trick are also compared with our method. We further empirically demonstrate the distinguishable primitives learned with PMOE and show the benefits of our method in terms of exploration.", "venue": "arXiv", "keywords": []}
{"id": "renTorchOptEfficientLibrary2022", "title": "TorchOpt: An Efficient Library for Differentiable Optimization", "abstract": "Recent years have witnessed the booming of various differentiable optimization algorithms. These algorithms exhibit different execution patterns, and their execution needs massive computational resources that go beyond a single CPU and GPU. Existing differentiable optimization libraries, however, cannot support efficient algorithm development and multi-CPU/GPU execution, making the development of differentiable optimization algorithms often cumbersome and expensive. This paper introduces TorchOpt, a PyTorch-based efficient library for differentiable optimization. TorchOpt provides a unified and expressive differentiable optimization programming abstraction. This abstraction allows users to efficiently declare and analyze various differentiable optimization programs with explicit gradients, implicit gradients, and zero-order gradients. TorchOpt further provides a high-performance distributed execution runtime. This runtime can fully parallelize computation-intensive differentiation operations (e.g. tensor tree flattening) on CPUs / GPUs and automatically distribute computation to distributed devices. Experimental results show that TorchOpt achieves \\ training time speedup on an 8-GPU server. TorchOpt is available at: https://github.com/metaopt/torchopt/.", "venue": "arXiv", "keywords": []}
{"id": "ResearchGate", "title": "ResearchGate", "abstract": "", "venue": "", "keywords": []}
{"id": "resoOcclusionAwareMethodTemporally2019", "title": "Occlusion-Aware Method for Temporally Consistent Superpixels", "abstract": "A wide variety of computer vision applications rely on superpixel or supervoxel algorithms as a preprocessing step. This underlines the overall importance that these approaches have gained in recent years. However, most methods show a lack of temporal consistency or fail in producing temporally stable superpixels. In this paper, we present an approach to generate temporally consistent superpixels for video content. Our method is formulated as a contour-evolving expectation-maximization framework, which utilizes an efficient label propagation scheme to encourage the preservation of superpixel shapes and their relative positioning over time. By explicitly detecting the occlusion of superpixels and the disocclusion of new image regions, our framework is able to terminate and create superpixels whose corresponding image region becomes hidden or newly appears. Additionally, the occluded parts of superpixels are incorporated in the further optimization. This increases the compliance of the superpixel flow with the optical flow present in the scene. Using established benchmark suites, we show that our approach produces highly competitive results in comparison to state-of-the-art streaming-capable supervoxel and superpixel algorithms for video content. This is further shown by comparing the streaming-capable approaches as basis for the task of interactive video segmentation where the proposed approach provides the lowest overall misclassification rate.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "keywords": ["automotive occlusion"]}
{"id": "rezendeVariationalInferenceNormalizing2016", "title": "Variational Inference with Normalizing Flows", "abstract": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.", "venue": "arXiv", "keywords": ["distribution estimation", "foundational", "normalizing flows", "vaes", "variational inference"]}
{"id": "ribaKorniaOpenSource2019", "title": "Kornia: An Open Source Differentiable Computer Vision Library for PyTorch", "abstract": "This work presents Kornia -- an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. The package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.", "venue": "arXiv", "keywords": []}
{"id": "richterPlayingDataGround2016", "title": "Playing for Data: Ground Truth from Computer Games", "abstract": "Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just 1/3 of the CamVid training set outperform models trained on the complete CamVid training set.", "venue": "arXiv", "keywords": []}
{"id": "richterSemanticEvidentialGrid2021", "title": "Semantic Evidential Grid Mapping Using Monocular and Stereo Cameras", "abstract": "Accurately estimating the current state of local traffic scenes is one of the key problems in the development of software components for automated vehicles. In addition to details on free space and drivability, static and dynamic traffic participants and information on the semantics may also be included in the desired representation. Multi-layer grid maps allow the inclusion of all of this information in a common representation. However, most existing grid mapping approaches only process range sensor measurements such as Lidar and Radar and solely model occupancy without semantic states. In order to add sensor redundancy and diversity, it is desired to add visionbased sensor setups in a common grid map representation. In this work, we present a semantic evidential grid mapping pipeline, including estimates for eight semantic classes, that is designed for straightforward fusion with range sensor data. Unlike other publications, our representation explicitly models uncertainties in the evidential model. We present results of our grid mapping pipeline based on a monocular vision setup and a stereo vision setup. Our mapping results are accurate and dense mapping due to the incorporation of a disparity- or depth-based ground surface estimation in the inverse perspective mapping. We conclude this paper by providing a detailed quantitative evaluation based on real traffic scenarios in the KITTI odometry benchmark dataset and demonstrating the advantages compared to other semantic grid mapping approaches.", "venue": "", "keywords": []}
{"id": "rieckNeuralPersistenceComplexity2019", "title": "Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology", "abstract": "While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data. Measures for characterizing and monitoring structural properties, however, have not been developed. In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs. To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization. Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss.", "venue": "arXiv", "keywords": ["persistent homology", "topological data analysis", "topological ml"]}
{"id": "ringerTextureBiasCNNs2019", "title": "Texture Bias Of CNNs Limits Few-Shot Classification Performance", "abstract": "Accurate image classification given small amounts of labelled data (few-shot classification) remains an open problem in computer vision. In this work we examine how the known texture bias of Convolutional Neural Networks (CNNs) affects few-shot classification performance. Although texture bias can help in standard image classification, in this work we show it significantly harms few-shot classification performance. After correcting this bias we demonstrate state-of-the-art performance on the competitive miniImageNet task using a method far simpler than the current best performing few-shot learning approaches.", "venue": "arXiv", "keywords": []}
{"id": "risserStableControllableNeural2017", "title": "Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses", "abstract": "Recently, methods have been proposed that perform texture synthesis and style transfer by using convolutional neural networks (e.g. Gatys et al. ). These methods are exciting because they can in some cases create results with state-of-the-art quality. However, in this paper, we show these methods also have limitations in texture quality, stability, requisite parameter tuning, and lack of user controls. This paper presents a multiscale synthesis pipeline based on convolutional neural networks that ameliorates these issues. We first give a mathematical explanation of the source of instabilities in many previous approaches. We then improve these instabilities by using histogram losses to synthesize textures that better statistically match the exemplar. We also show how to integrate localized style losses in our multiscale framework. These losses can improve the quality of large features, improve the separation of content and style, and offer artistic controls such as paint by numbers. We demonstrate that our approach offers improved quality, convergence in fewer iterations, and more stability over the optimization.", "venue": "arXiv", "keywords": ["augmentation stability", "style transfer", "texture transfer"]}
{"id": "robbianoBayesianLearningOccupancy2020", "title": "Bayesian Learning of Occupancy Grids", "abstract": "Occupancy grids encode for hot spots on a map that is represented by a two dimensional grid of disjoint cells. The problem is to recursively update the probability that each cell in the grid is occupied, based on a sequence of sensor measurements from a moving platform. In this paper, we provide a new Bayesian framework for generating these probabilities that does not assume statistical independence between the occupancy state of grid cells. This approach is made analytically tractable through the use of binary asymmetric channel models that capture the errors associated with observing the occupancy state of a grid cell. Binary-valued measurement vectors are the thresholded output of a sensor in a radar, sonar, or other sensory system. We compare the performance of the proposed framework to that of the classical formulation for occupancy grids. The results show that the proposed framework identifies occupancy grids with lower false alarm and miss detection rates, and requires fewer observations of the surrounding area, to generate an accurate estimate of occupancy probabilities when compared to conventional formulations.", "venue": "arXiv", "keywords": []}
{"id": "rosSYNTHIADatasetLarge2016", "title": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes", "abstract": "Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images; thus, having a sufficient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for semantic segmentation -- in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show how the inclusion of SYNTHIA in the training stage significantly improves performance on the semantic segmentation task.", "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": []}
{"id": "rothDataCentricAugmentationApproach2021", "title": "A Data-Centric Augmentation Approach for Disturbed Sensor Image Segmentation", "abstract": "In the context of sensor-based data analysis, the compensation of image artifacts is a challenge. When the structures of interest are not clearly visible in an image, algorithms that can cope with artifacts are crucial for obtaining the desired information. Thereby, the high variation of artifacts, the combination of different types of artifacts, and their similarity to signals of interest are specific issues that have to be considered in the analysis. Despite the high generalization capability of deep learning-based approaches, their recent success was driven by the availability of large amounts of labeled data. Therefore, the provision of comprehensive labeled image data with different characteristics of image artifacts is of importance. At the same time, applying deep neural networks to problems with low availability of labeled data remains a challenge. This work presents a data-centric augmentation approach based on generative adversarial networks that augments the existing labeled data with synthetic artifacts generated from data not present in the training set. In our experiments, this augmentation leads to a more robust generalization in segmentation. Our method does not need additional labeling and does not lead to additional memory or time consumption during inference. Further, we find it to be more effective than comparable augmentations based on procedurally generated artifacts and the direct use of real artifacts. Building upon the improved segmentation results, we observe that our approach leads to improvements of 22% in the F1-score for an evaluated detection problem. Having achieved these results with an example sensor, we expect increased robustness against artifacts in future applications.", "venue": "Journal of Imaging", "keywords": ["adversarial learning", "gans", "noise injection"]}
{"id": "royEffectsDegradationsDeep2023", "title": "Effects of Degradations on Deep Neural Network Architectures", "abstract": "Deep convolutional neural networks (CNN) have massively influenced recent advances in large-scale image classification. More recently, a dynamic routing algorithm with capsules (groups of neurons) has shown state-of-the-art recognition performance. However, the behavior of such networks in the presence of a degrading signal (noise) is mostly unexplored. An analytical study on different network architectures toward noise robustness is essential for selecting the appropriate model in a specific application scenario. This paper presents an extensive performance analysis of six deep architectures for image classification on six most common image degradation models. In this study, we have compared VGG-16, VGG-19, ResNet-50, Inception-v3, MobileNet and CapsuleNet architectures on Gaussian white, Gaussian color, salt-and-pepper, Gaussian blur, motion blur and JPEG compression noise models.", "venue": "arXiv", "keywords": ["filter augmentations", "noise injection"]}
{"id": "rubensteinLatentSpaceWasserstein2018", "title": "On the Latent Space of Wasserstein Auto-Encoders", "abstract": "We study the role of latent space dimensionality in Wasserstein auto-encoders (WAEs). Through experimentation on synthetic and real datasets, we argue that random encoders should be preferred over deterministic encoders. We highlight the potential of WAEs for representation learning with promising results on a benchmark disentanglement task.", "venue": "arXiv", "keywords": ["manifold learning", "vaes"]}
{"id": "ruderOverviewMultiTaskLearning2017", "title": "An Overview of Multi-Task Learning in Deep Neural Networks", "abstract": "Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.", "venue": "arXiv", "keywords": []}
{"id": "rutaDIFFNSTDiffusionInterleaving2023", "title": "DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer", "abstract": "Neural Style Transfer (NST) is the field of study applying neural techniques to modify the artistic appearance of a content image to match the style of a reference style image. Traditionally, NST methods have focused on texture-based image edits, affecting mostly low level information and keeping most image structures the same. However, style-based deformation of the content is desirable for some styles, especially in cases where the style is abstract or the primary concept of the style is in its deformed rendition of some content. With the recent introduction of diffusion models, such as Stable Diffusion, we can access far more powerful image generation techniques, enabling new possibilities. In our work, we propose using this new class of models to perform style transfer while enabling deformable style transfer, an elusive capability in previous models. We show how leveraging the priors of these models can expose new artistic controls at inference time, and we document our findings in exploring this new direction for the field of style transfer.", "venue": "arXiv", "keywords": ["diffusion models", "style transfer"]}
{"id": "rutaNeATNeuralArtistic2023", "title": "NeAT: Neural Artistic Tracing for Beautiful Style Transfer", "abstract": "Style transfer is the task of reproducing the semantic contents of a source image in the artistic style of a second target image. In this paper, we present NeAT, a new state-of-the art feed-forward style transfer method. We re-formulate feed-forward style transfer as image editing, rather than image generation, resulting in a model which improves over the state-of-the-art in both preserving the source content and matching the target style. An important component of our model's success is identifying and fixing \"style halos\", a commonly occurring artefact across many style transfer techniques. In addition to training and testing on standard datasets, we introduce the BBST-4M dataset, a new, large scale, high resolution dataset of 4M images. As a component of curating this data, we present a novel model able to classify if an image is stylistic. We use BBST-4M to improve and measure the generalization of NeAT across a huge variety of styles. Not only does NeAT offer state-of-the-art quality and generalization, it is designed and trained for fast inference at high resolution.", "venue": "arXiv", "keywords": ["style transfer"]}
{"id": "saeedFederatedSelfSupervisedLearning2021", "title": "Federated Self-Supervised Learning of Multi-Sensor Representations for Embedded Intelligence", "abstract": "Smartphones, wearables, and Internet of Things (IoT) devices produce a wealth of data that cannot be accumulated in a centralized repository for learning supervised models due to privacy, bandwidth limitations, and the prohibitive cost of annotations. Federated learning provides a compelling framework for learning models from decentralized data, but conventionally, it assumes the availability of labeled samples, whereas on-device data are generally either unlabeled or cannot be annotated readily through user interaction. To address these issues, we propose a self-supervised approach termed -signal correspondence learning\\ based on wavelet transform to learn useful representations from unlabeled sensor inputs, such as electroencephalography, blood volume pulse, accelerometer, and WiFi channel state information. Our auxiliary task requires a deep temporal neural network to determine if a given pair of a signal and its complementary viewpoint (i.e., a scalogram generated with a wavelet transform) align with each other or not through optimizing a contrastive objective. We extensively assess the quality of learned features with our multi-view strategy on diverse public datasets, achieving strong performance in all domains. We demonstrate the effectiveness of representations learned from an unlabeled input collection on downstream tasks with training a linear classifier over pretrained network, usefulness in low-data regime, transfer learning, and cross-validation. Our methodology achieves competitive performance with fully-supervised networks, and it outperforms pre-training with autoencoders in both central and federated contexts. Notably, it improves the generalization in a semi-supervised setting as it reduces the volume of labeled data required through leveraging self-supervised learning.", "venue": "IEEE Internet of Things Journal", "keywords": ["edge computing", "federated learning", "self-supervised learning"]}
{"id": "saenkoDomainAdaptationDeep", "title": "Domain Adaptation for Deep Learning", "abstract": "", "venue": "", "keywords": ["surveys", "unsupervised da"]}
{"id": "sahaReIQAUnsupervisedLearning2023", "title": "Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild", "abstract": "Automatic Perceptual Image Quality Assessment is a challenging problem that impacts billions of internet, and social media users daily. To advance research in this field, we propose a Mixture of Experts approach to train two separate encoders to learn high-level content and low-level image quality features in an unsupervised setting. The unique novelty of our approach is its ability to generate low-level representations of image quality that are complementary to high-level features representing image content. We refer to the framework used to train the two encoders as Re-IQA. For Image Quality Assessment in the Wild, we deploy the complementary low and high-level image representations obtained from the Re-IQA framework to train a linear regression model, which is used to map the image representations to the ground truth quality scores, refer Figure 1. Our method achieves state-of-the-art performance on multiple large-scale image quality assessment databases containing both real and synthetic distortions, demonstrating how deep neural networks can be trained in an unsupervised setting to produce perceptually relevant representations. We conclude from our experiments that the low and high-level features obtained are indeed complementary and positively impact the performance of the linear regressor. A public release of all the codes associated with this work will be made available on GitHub.", "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["image quality assessment", "unsupervised learning"]}
{"id": "sahariaPaletteImagetoImageDiffusion2022", "title": "Palette: Image-to-Image Diffusion Models", "abstract": "This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results.", "venue": "arXiv", "keywords": ["diffusion models", "image inpainting", "image-to-image"]}
{"id": "sahuSalientKeypointDetection2015", "title": "Salient Keypoint Detection Using Entropy Map for Iris Biometric", "abstract": "This paper presents a novel approach to detect salient keypoints from the textured region in iris of an individual for efficient recognition. Salient regions are visually pre-attentive distinct portions in an image. Entropy from local segments within an image is used as the significant measure of saliency. To know the entropy value of such portions, an entropy map is generated. Local feature extraction is achieved from the segmented annular iris. Salient keypoint detection is performed using the proposed method, and subsequently each point is represented using Speeded-Up Robust Feature (SURF) descriptor. Finally, matching is performed and the recognition accuracy is calculated from the Receiver Operating Characteristic (ROC) curve. All experiments are done on publicly available BATH and CASIAv3 databases.", "venue": "Proceedings of the 2nd International Conference on Perception and Machine Intelligence", "keywords": []}
{"id": "sakaridisACDCAdverseConditions2021", "title": "ACDC: The Adverse Conditions Dataset with Correspondences for Semantic Driving Scene Understanding", "abstract": "Level 5 autonomy for self-driving cars requires a robust visual perception system that can parse input images under any visual condition. However, existing semantic segmentation datasets are either dominated by images captured under normal conditions or are small in scale. To address this, we introduce ACDC, the Adverse Conditions Dataset with Correspondences for training and testing semantic segmentation methods on adverse visual conditions. ACDC consists of a large set of 4006 images which are equally distributed between four common adverse conditions: fog, nighttime, rain, and snow. Each adverse-condition image comes with a high-quality fine pixel-level semantic annotation, a corresponding image of the same scene taken under normal conditions, and a binary mask that distinguishes between intra-image regions of clear and uncertain semantic content. Thus, ACDC supports both standard semantic segmentation and the newly introduced uncertainty-aware semantic segmentation. A detailed empirical study demonstrates the challenges that the adverse domains of ACDC pose to state-of-the-art supervised and unsupervised approaches and indicates the value of our dataset in steering future progress in the field. Our dataset and benchmark are publicly available.", "venue": "arXiv", "keywords": ["adverse weather", "dataset debut", "semantic segmentation"]}
{"id": "sakaridisSemanticFoggyScene2018", "title": "Semantic Foggy Scene Understanding with Synthetic Data", "abstract": "", "venue": "International Journal of Computer Vision", "keywords": []}
{"id": "sanakoyeuStyleAwareContentLoss2018", "title": "A Style-Aware Content Loss for Real-time HD Style Transfer", "abstract": "Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but previous work is limited to only a single instance of a style or shows no benefit from more images. Moreover, previous work has relied on a direct comparison of art in the domain of RGB images or on CNNs pre-trained on ImageNet, which requires millions of labeled object bounding boxes and can introduce an extra bias, since it has been assembled without artistic consideration. To circumvent these issues, we propose a style-aware content loss, which is trained jointly with a deep encoder-decoder network for real-time, high-resolution stylization of images and videos. We propose a quantitative measure for evaluating the quality of a stylized image and also have art historians rank patches from our approach against those from previous work. These and our qualitative results ranging from small image patches to megapixel stylistic images and videos show that our approach better captures the subtle nature in which a style affects content.", "venue": "arXiv", "keywords": []}
{"id": "ScaleinvariantFeatureOperator2023", "title": "Scale-Invariant Feature Operator", "abstract": "In the fields of computer vision and image analysis, the scale-invariant feature operator (or SFOP) is an algorithm to detect local features in images. The algorithm was published by F\\\"orstner et al. in 2009.", "venue": "Wikipedia", "keywords": []}
{"id": "schmidSelfSupervisedTraversabilityPrediction2022", "title": "Self-Supervised Traversability Prediction by Learning to Reconstruct Safe Terrain", "abstract": "Navigating off-road with a fast autonomous vehicle depends on a robust perception system that differentiates traversable from non-traversable terrain. Typically, this depends on a semantic understanding which is based on supervised learning from images annotated by a human expert. This requires a significant investment in human time, assumes correct expert classification, and small details can lead to misclassification. To address these challenges, we propose a method for predicting high- and low-risk terrains from only past vehicle experience in a self-supervised fashion. First, we develop a tool that projects the vehicle trajectory into the front camera image. Second, occlusions in the 3D representation of the terrain are filtered out. Third, an autoencoder trained on masked vehicle trajectory regions identifies low- and highrisk terrains based on the reconstruction error. We evaluated our approach with two models and different bottleneck sizes with two different training and testing sites with a fourwheeled off-road vehicle. Comparison with two independent test sets of semantic labels from similar terrain as training sites demonstrates the ability to separate the ground as low-risk and the vegetation as high-risk with 81.1% and 85.1% accuracy.", "venue": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "keywords": ["self-supervised learning"]}
{"id": "schmidSelfSupervisedTraversabilityPrediction2022a", "title": "Self-Supervised Traversability Prediction by Learning to Reconstruct Safe Terrain", "abstract": "Navigating off-road with a fast autonomous vehicle depends on a robust perception system that differentiates traversable from non-traversable terrain. Typically, this depends on a semantic understanding which is based on supervised learning from images annotated by a human expert. This requires a significant investment in human time, assumes correct expert classification, and small details can lead to misclassification. To address these challenges, we propose a method for predicting high- and low-risk terrains from only past vehicle experience in a self-supervised fashion. First, we develop a tool that projects the vehicle trajectory into the front camera image. Second, occlusions in the 3D representation of the terrain are filtered out. Third, an autoencoder trained on masked vehicle trajectory regions identifies low- and high-risk terrains based on the reconstruction error. We evaluated our approach with two models and different bottleneck sizes with two different training and testing sites with a fourwheeled off-road vehicle. Comparison with two independent test sets of semantic labels from similar terrain as training sites demonstrates the ability to separate the ground as low-risk and the vegetation as high-risk with 81.1% and 85.1% accuracy.", "venue": "arXiv", "keywords": []}
{"id": "schneiderImprovingRobustnessCommon2020", "title": "Improving Robustness against Common Corruptions by Covariate Shift Adaptation", "abstract": "Today's state-of-the-art machine vision models are vulnerable to image corruptions like blurring or compression artefacts, limiting their performance in many realworld applications. We here argue that popular benchmarks to measure model robustness against common corruptions (like ImageNet-C) underestimate model robustness in many (but not all) application scenarios. The key insight is that in many scenarios, multiple unlabeled examples of the corruptions are available and can be used for unsupervised online adaptation. Replacing the activation statistics estimated by batch normalization on the training set with the statistics of the corrupted images consistently improves the robustness across 25 different popular computer vision models. Using the corrected statistics, ResNet-50 reaches 62.2% mCE on ImageNet-C compared to 76.7% without adaptation. With the more robust DeepAugment+AugMix model, we improve the state of the art achieved by a ResNet50 model up to date from 53.6% mCE to 45.4% mCE. Even adapting to a single sample improves robustness for the ResNet-50 and AugMix models, and 32 samples are sufficient to improve the current state of the art for a ResNet50 architecture. We argue that results with adapted statistics should be included whenever reporting scores in corruption benchmarks and other out-of-distribution generalization settings.", "venue": "arXiv", "keywords": ["adversarial robustness", "robustness analysis"]}
{"id": "schulmanProximalPolicyOptimization2017", "title": "Proximal Policy Optimization Algorithms", "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.", "venue": "arXiv", "keywords": ["foundational", "ppo", "reinforcement learning"]}
{"id": "schwonbergAugmentationbasedDomainGeneralization2023", "title": "Augmentation-Based Domain Generalization for Semantic Segmentation", "abstract": "Unsupervised Domain Adaptation (UDA) and domain generalization (DG) are two research areas that aim to tackle the lack of generalization of Deep Neural Networks (DNNs) towards unseen domains. While UDA methods have access to unlabeled target images, domain generalization does not involve any target data and only learns generalized features from a source domain. Image-style randomization or augmentation is a popular approach to improve network generalization without access to the target domain. Complex methods are often proposed that disregard the potential of simple image augmentations for out-of-domain generalization. For this reason, we systematically study the in- and out-of-domain generalization capabilities of simple, rule-based image augmentations like blur, noise, color jitter and many more. Based on a full factorial design of experiment design we provide a systematic statistical evaluation of augmentations and their interactions. Our analysis provides both, expected and unexpected, outcomes. Expected, because our experiments confirm the common scientific standard that combination of multiple different augmentations out-performs single augmentations. Unexpected, because combined augmentations perform competitive to state-of-the-art domain generalization approaches, while being significantly simpler and without training overhead. On the challenging synthetic-to-real domain shift between Synthia and Cityscapes we reach 39.5% mIoU compared to 40.9% mIoU of the best previous work. When additionally employing the recent vision transformer architecture DAFormer we outperform these benchmarks with a performance of 44.2% mIoU", "venue": "arXiv", "keywords": ["domain generalization", "semantic segmentation", "surveys"]}
{"id": "schwonbergSurveyUnsupervisedDomain2023", "title": "Survey on Unsupervised Domain Adaptation for Semantic Segmentation for Visual Perception in Automated Driving", "abstract": "Deep neural networks (DNNs) have proven their capabilities in the past years and play a significant role in environment perception for the challenging application of automated driving. They are employed for tasks such as detection, semantic segmentation, and sensor fusion. Despite tremendous research efforts, several issues still need to be addressed that limit the applicability of DNNs in automated driving. The bad generalization of DNNs to unseen domains is a major problem on the way to a safe, large-scale application, because manual annotation of new domains is costly, particularly for semantic segmentation. For this reason, methods are required to adapt DNNs to new domains without labeling effort. This task is termed unsupervised domain adaptation (UDA). While several different domain shifts challenge DNNs, the shift between synthetic and real data is of particular importance for automated driving, as it allows the use of simulation environments for DNN training. We present an overview of the current state of the art in this research field. We categorize and explain the different approaches for UDA. The number of considered publications is larger than any other survey on this topic. We also go far beyond the description of the UDA state-of-the-art, as we present a quantitative comparison of approaches and point out the latest trends in this field. We conduct a critical analysis of the state-of-the-art and highlight promising future research directions. With this survey, we aim to facilitate UDA research further and encourage scientists to exploit novel research directions.", "venue": "IEEE Access", "keywords": ["semantic segmentation", "surveys", "unsupervised da"]}
{"id": "schwonbergSurveyUnsupervisedDomain2023a", "title": "Survey on Unsupervised Domain Adaptation for Semantic Segmentation for Visual Perception in Automated Driving", "abstract": "Deep neural networks (DNNs) have proven their capabilities in many areas in the past years, such as robotics, or automated driving, enabling technological breakthroughs. DNNs play a significant role in environment perception for the challenging application of automated driving and are employed for tasks such as detection, semantic segmentation, and sensor fusion. Despite this progress and tremendous research efforts, several issues still need to be addressed that limit the applicability of DNNs in automated driving. The bad generalization of DNNs to new, unseen domains is a major problem on the way to a safe, large-scale application, because manual annotation of new domains is costly, particularly for semantic segmentation. For this reason, methods are required to adapt DNNs to new domains without labeling effort. The task, which these methods aim to solve is termed unsupervised domain adaptation (UDA). While several different domain shifts can challenge DNNs, the shift between synthetic and real data is of particular importance for automated driving, as it allows the use of simulation environments for DNN training. In this work, we present an overview of the current state of the art in this field of research. We categorize and explain the different approaches for UDA. The number of considered publications is larger than any other survey on this topic. The scope of this survey goes far beyond the description of the UDA state-of-the-art. Based on our large data and knowledge base, we present a quantitative comparison of the approaches and use the observations to point out the latest trends in this field. In the following, we conduct a critical analysis of the state-of-the-art and highlight promising future research directions. With this survey, we aim to facilitate UDA research further and encourage scientists to exploit novel research directions to generalize DNNs better.", "venue": "arXiv", "keywords": ["semantic segmentation", "surveys", "unsupervised da"]}
{"id": "sekkatSynWoodScapeSyntheticSurroundView2022", "title": "SynWoodScape: Synthetic Surround-View Fisheye Camera Dataset for Autonomous Driving", "abstract": "", "venue": "IEEE Robotics and Automation Letters", "keywords": ["dataset debut", "fisheye distortion"]}
{"id": "sheenyRADIATERadarDataset2021", "title": "RADIATE: A Radar Dataset for Automotive Perception in Bad Weather", "abstract": "Datasets for autonomous cars are essential for the development and benchmarking of perception systems. However, most existing datasets are captured with camera and LiDAR sensors in good weather conditions. In this paper, we present the RAdar Dataset In Adverse weaThEr (RADIATE), aiming to facilitate research on object detection, tracking and scene understanding using radar sensing for safe autonomous driving. RADIATE includes 3 hours of annotated radar images with more than 200K labelled road actors in total, on average about 4.6 instances per radar image. It covers 8 different categories of actors in a variety of weather conditions (e.g., sun, night, rain, fog and snow) and driving scenarios (e.g., parked, urban, motorway and suburban), representing different levels of challenge. To the best of our knowledge, this is the first public radar dataset which provides high-resolution radar images on public roads with a large amount of road actors labelled. The data collected in adverse weather, e.g., fog and snowfall, is unique. Some baseline results of radar based object detection and recognition are given to show that the use of radar data is promising for automotive applications in bad weather, where vision and LiDAR can fail. RADIATE also has stereo images, 32-channel LiDAR and GPS data, directed at other applications such as sensor fusion, localisation and mapping. The public dataset can be accessed at http://pro.hw.ac.uk/radiate/.", "venue": "arXiv", "keywords": ["adverse weather", "automotive occlusion", "dataset debut"]}
{"id": "sheikholeslamiAblationProgrammingMachine2019", "title": "Ablation Programming for Machine Learning", "abstract": "DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.", "venue": "", "keywords": ["dissertations", "experimental design", "surveys"]}
{"id": "shengAvatarNetMultiscaleZeroshot2018", "title": "Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration", "abstract": "Zero-shot artistic style transfer is an important image synthesis problem aiming at transferring arbitrary style into content images. However, the trade-off between the generalization and efficiency in existing methods impedes a high quality zero-shot style transfer in real-time. In this paper, we resolve this dilemma and propose an efficient yet effective Avatar-Net that enables visually plausible multi-scale transfer for arbitrary style. The key ingredient of our method is a style decorator that makes up the content features by semantically aligned style features from an arbitrary style image, which does not only holistically match their feature distributions but also preserve detailed style patterns in the decorated features. By embedding this module into an image reconstruction network that fuses multi-scale style abstractions, the Avatar-Net renders multi-scale stylization for any style image in one feed-forward pass. We demonstrate the state-of-the-art effectiveness and efficiency of the proposed method in generating high-quality stylized images, with a series of applications include multiple style integration, video stylization and etc.", "venue": "arXiv", "keywords": ["feature-level augmentation", "style transfer"]}
{"id": "shenMixedSampleAugmentation2023", "title": "Mixed Sample Augmentation for Online Distillation", "abstract": "Mixed Sample Regularization (MSR), such as MixUp or CutMix, is a powerful data augmentation strategy to generalize convolutional neural networks. Previous empirical analysis has illustrated an orthogonal performance gain between MSR and conventional offline Knowledge Distillation (KD). To be more specific, student networks can be enhanced with the involvement of MSR in the training stage of sequential distillation. Yet, the interplay between MSR and online knowledge distillation, where an ensemble of peer students learn mutually from each other, remains unexplored. To bridge the gap, we make the first attempt at incorporating CutMix into online distillation, where we empirically observe a significant improvement. Encouraged by this fact, we propose an even stronger MSR specifically for online distillation, named as CutnMix. Furthermore, a novel online distillation framework is designed upon CutnMix, to enhance the distillation with feature level mutual learning and a self-ensemble teacher. Comprehensive evaluations on CIFAR10 and CIFAR100 with six network architectures show that our approach can consistently outperform state-of-the-art distillation methods.", "venue": "arXiv", "keywords": ["knowledge distillation", "mixture augmentations", "online da"]}
{"id": "shenTagLLMRepurposingGeneralPurpose2024", "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains", "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from task functions, our method enables zero-shot generalization to unseen problems through diverse combinations of the input tags. It also boosts LLM's performance in various specialized domains, such as predicting protein or chemical properties and modeling drug-target interactions, outperforming expert models tailored to these tasks.", "venue": "arXiv", "keywords": []}
{"id": "shettyNotUsingCar2018", "title": "Not Using the Car to See the Sidewalk: Quantifying and Controlling the Effects of Context in Classification and Segmentation", "abstract": "Importance of visual context in scene understanding tasks is well recognized in the computer vision community. However, to what extent the computer vision models for image classification and semantic segmentation are dependent on the context to make their predictions is unclear. A model overly relying on context will fail when encountering objects in context distributions different from training data and hence it is important to identify these dependencies before we can deploy the models in the real-world. We propose a method to quantify the sensitivity of black-box vision models to visual context by editing images to remove selected objects and measuring the response of the target models. We apply this methodology on two tasks, image classification and semantic segmentation, and discover undesirable dependency between objects and context, for example that \"sidewalk\" segmentation relies heavily on \"cars\" being present in the image. We propose an object removal based data augmentation solution to mitigate this dependency and increase the robustness of classification and segmentation models to contextual variations. Our experiments show that the proposed data augmentation helps these models improve the performance in out-of-context scenarios, while preserving the performance on regular data.", "venue": "arXiv.org", "keywords": []}
{"id": "shiAutomaticDataAugmentation2019", "title": "Automatic Data Augmentation by Learning the Deterministic Policy", "abstract": "Aiming to produce sufficient and diverse training samples, data augmentation has been demonstrated for its effectiveness in training deep models. Regarding that the criterion of the best augmentation is challenging to define, we in this paper present a novel learning-based augmentation method termed as DeepAugNet, which formulates the final augmented data as a collection of several sequentially augmented subsets. Specifically, the current augmented subset is required to maximize the performance improvement compared with the last augmented subset by learning the deterministic augmentation policy using deep reinforcement learning. By introducing an unified optimization goal, DeepAugNet intends to combine the data augmentation and the deep model training in an end-to-end training manner which is realized by simultaneously training a hybrid architecture of dueling deep Q-learning algorithm and a surrogate deep model. We extensively evaluated our proposed DeepAugNet on various benchmark datasets including Fashion MNIST, CUB, CIFAR-100 and WebCaricature. Compared with the current state-of-the-arts, our method can achieve a significant improvement in small-scale datasets, and a comparable performance in large-scale datasets. Code will be available soon.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "shiDistributedAdaptiveClustering2021", "title": "Distributed Adaptive Clustering Learning over Time-Varying Multitask Networks", "abstract": "With increasing research on distributed processing in networks, adaptive learning strategies have gradually attracted researchers' attention. The traditional adaptive learning strategy mainly aims at a single task unchanged over time, while real networks often entail multitasks scenarios with tasks that change over time. Furthermore, although cooperation among agents is beneficial for single task time-invariant networks, agents' indiscriminate cooperation in time-varying multitask networks may cause undesirable effects. In this paper, an adaptive clustering learning approach based on an event-triggered scheme over time-varying multitask networks is proposed. With this process, agents are enabled to distinguish their clusters to determine cooperative neighbors and improve the accuracy of estimation over networks. The mean-square of the proposed algorithm is analyzed in detail, and the error probability of false alarms and false detections of the clustering mechanism is evaluated. An extensive simulation is given to validate the analytical performance of the distributed learning strategy over time-varying multitasks.", "venue": "Information Sciences", "keywords": ["adaptive learning", "multi-task learning"]}
{"id": "shiFishDreamerFisheyeSemantic2023", "title": "FishDreamer: Towards Fisheye Semantic Completion via Unified Image Outpainting and Segmentation", "abstract": "This paper raises the new task of Fisheye Semantic Completion (FSC), where dense texture, structure, and semantics of a fisheye image are inferred even beyond the sensor field-of-view (FoV). Fisheye cameras have larger FoV than ordinary pinhole cameras, yet its unique special imaging model naturally leads to a blind area at the edge of the image plane. This is suboptimal for safety-critical applications since important perception tasks, such as semantic segmentation, become very challenging within the blind zone. Previous works considered the out-FoV outpainting and in-FoV segmentation separately. However, we observe that these two tasks are actually closely coupled. To jointly estimate the tightly intertwined complete fisheye image and scene semantics, we introduce the new FishDreamer which relies on successful ViTs enhanced with a novel Polar-aware Cross Attention module (PCA) to leverage dense context and guide semantically-consistent content generation while considering different polar distributions. In addition to the contribution of the novel task and architecture, we also derive Cityscapes-BF and KITTI360-BF datasets to facilitate training and evaluation of this new track. Our experiments demonstrate that the proposed FishDreamer outperforms methods solving each task in isolation and surpasses alternative approaches on the Fisheye Semantic Completion. Code and datasets are publicly available at https://github.com/MasterHow/FishDreamer.", "venue": "arXiv", "keywords": ["fisheye distortion", "semantic segmentation"]}
{"id": "shiTransformerScaleGate2022", "title": "Transformer Scale Gate for Semantic Segmentation", "abstract": "Effectively encoding multi-scale contextual information is crucial for accurate semantic segmentation. Existing transformer-based segmentation models combine features across scales without any selection, where features on sub-optimal scales may degrade segmentation outcomes. Leveraging from the inherent properties of Vision Transformers, we propose a simple yet effective module, Transformer Scale Gate (TSG), to optimally combine multi-scale features.TSG exploits cues in self and cross attentions in Vision Transformers for the scale selection. TSG is a highly flexible plug-and-play module, and can easily be incorporated with any encoder-decoder-based hierarchical vision Transformer architecture. Extensive experiments on the Pascal Context and ADE20K datasets demonstrate that our feature selection strategy achieves consistent gains.", "venue": "arXiv", "keywords": ["promising", "segformer", "semantic segmentation", "transformers"]}
{"id": "shiVisualizationComparisonVision2024", "title": "Visualization Comparison of Vision Transformers and Convolutional Neural Networks", "abstract": "Recent research has demonstrated that Vision Transformers (ViTs) are capable of comparable or even better performance than convolutional neural network (CNN) baselines. The differences in their structural designs are obvious, but our understanding of the differences in their feature representations remains limited. In this work, we propose several techniques to achieve high-quality visualization of representations in ViTs. Both qualitative and quantitative experiments show that our technical improvements can observably improve ViT visualization quality compared to previous studies. Furthermore, we conduct visualizations to explore the disparities between ViTs and CNNs pre-trained on ImageNet1K, revealing three intriguing properties of ViTs: a) ViT feature propagation retains image detail information with minimal loss, whereas CNNs discard most image details for class discrimination. b) Different from CNNs, objectrelated features do not show in ViT higher layers, suggesting that class-discriminative features may not be required for ViT classification. c) Our visualization-assisted texture-bias experiment reveals that both ViTs and CNNs exhibit texture bias, of which ViTs seem to be more biased towards local textures.", "venue": "IEEE Transactions on Multimedia", "keywords": ["bias sources", "cnns", "feature visualization", "representation learning", "transformers", "visualizations"]}
{"id": "shortenSurveyImageData2019", "title": "A Survey on Image Data Augmentation for Deep Learning", "abstract": "Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.", "venue": "Journal of Big Data", "keywords": ["surveys"]}
{"id": "shortenSurveyImageData2019a", "title": "A Survey on Image Data Augmentation for Deep Learning", "abstract": "Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.", "venue": "Journal of Big Data", "keywords": ["gans", "image data"]}
{"id": "shuiBenefitsRepresentationRegularization2021", "title": "On the Benefits of Representation Regularization in Invariance Based Domain Generalization", "abstract": "A crucial aspect in reliable machine learning is to design a deployable system in generalizing new related but unobserved environments. Domain generalization aims to alleviate such a prediction gap between the observed and unseen environments. Previous approaches commonly incorporated learning invariant representation for achieving good empirical performance. In this paper, we reveal that merely learning invariant representation is vulnerable to the unseen environment. To this end, we derive novel theoretical analysis to control the unseen test environment error in the representation learning, which highlights the importance of controlling the smoothness of representation. In practice, our analysis further inspires an efficient regularization method to improve the robustness in domain generalization. Our regularization is orthogonal to and can be straightforwardly adopted in existing domain generalization algorithms for invariant representation learning. Empirical results show that our algorithm outperforms the base versions in various dataset and invariance criteria.", "venue": "arXiv", "keywords": ["domain generalization", "representation learning"]}
{"id": "silbermanIndoorSceneSegmentation2011", "title": "Indoor Scene Segmentation Using a Structured Light Sensor", "abstract": "In this paper we explore how a structured light depth sensor, in the form of the Microsoft Kinect, can assist with indoor scene segmentation. We use a CRF-based model to evaluate a range of different representations for depth information and propose a novel prior on 3D location. We introduce a new and challenging indoor scene dataset, complete with accurate depth maps and dense label coverage. Evaluating our model on this dataset reveals that the combination of depth and intensity images gives dramatic performance gains over intensity images alone. Our results clearly demonstrate the utility of structured light sensors for scene understanding.", "venue": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)", "keywords": []}
{"id": "silverLecture9Exploration", "title": "Lecture 9: Exploration and Exploitation", "abstract": "", "venue": "", "keywords": []}
{"id": "simonyanVeryDeepConvolutional2015", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.", "venue": "arXiv", "keywords": ["cnns", "foundational", "semantic segmentation"]}
{"id": "SimplicialHomology2024", "title": "Simplicial Homology", "abstract": "In algebraic topology, simplicial homology is the sequence of homology groups of a simplicial complex. It formalizes the idea of the number of holes of a given dimension in the complex. This generalizes the number of connected components (the case of dimension 0). Simplicial homology arose as a way to study topological spaces whose building blocks are n-simplices, the n-dimensional analogs of triangles. This includes a point (0-simplex), a line segment (1-simplex), a triangle (2-simplex) and a tetrahedron (3-simplex). By definition, such a space is homeomorphic to a simplicial complex (more precisely, the geometric realization of an abstract simplicial complex). Such a homeomorphism is referred to as a triangulation of the given space. Many topological spaces of interest can be triangulated, including every smooth manifold (Cairns and Whitehead).: sec.5.3.2 Simplicial homology is defined by a simple recipe for any abstract simplicial complex. It is a remarkable fact that simplicial homology only depends on the associated topological space.: sec.8.6 As a result, it gives a computable way to distinguish one space from another.", "venue": "Wikipedia", "keywords": []}
{"id": "singhNeuralStyleTransfer2021", "title": "Neural Style Transfer: A Critical Review", "abstract": "Neural Style Transfer (NST) is a class of software algorithms that allows us to transform scenes, change/edit the environment of a media with the help of a Neural Network. NST finds use in image and video editing software allowing image stylization based on a general model, unlike traditional methods. This made NST a trending topic in the entertainment industry as professional editors/media producers create media faster and offer the general public recreational use. In this paper, the current progress in Neural Style Transfer with all related aspects such as still images and videos is presented critically. The authors looked at the different architectures used and compared their advantages and limitations. Multiple literature reviews focus on either the Neural Style Transfer (of images) or cover Generative Adversarial Networks (GANs) that generate video. As per the authors' knowledge, this is the only research article that looks at image and video style transfer, particularly mobile devices with high potential usage. This article also reviewed the challenges faced in applyingvideo neural style transfer in real-time on mobile devices and presents research gaps with future research directions. NST, a fascinating deep learning application, has considerable research and application potential in the coming years.", "venue": "IEEE Access", "keywords": ["style transfer", "surveys"]}
{"id": "singhSAFINArbitraryStyle2021", "title": "SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization", "abstract": "Artistic style transfer aims to transfer the style characteristics of one image onto another image while retaining its content. Existing approaches commonly leverage various normalization techniques, although these face limitations in adequately transferring diverse textures to different spatial locations. Self-Attention based approaches have tackled this issue with partial success but suffer from unwanted artifacts. Motivated by these observations, this paper aims to combine the best of both worlds: self-attention and normalization. That yields a new plug-and-play module that we name Self-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially a spatially-adaptive normalization module whose parameters are inferred through attention on the content and style image. We demonstrate that plugging SAFIN into the base network of another state-of-the-art method results in enhanced stylization. We also develop a novel base network composed of Wavelet Transform for multi-scale style transfer, which when combined with SAFIN, produces visually appealing results with lesser unwanted textures.", "venue": "arXiv", "keywords": ["instance normalization", "self-attention", "style transfer"]}
{"id": "skartadosTrickVOSBagTricks2023", "title": "TrickVOS: A Bag of Tricks for Video Object Segmentation", "abstract": "Space-time memory (STM) network methods have been dominant in semi-supervised video object segmentation (SVOS) due to their remarkable performance. In this work, we identify three key aspects where we can improve such methods; i) supervisory signal, ii) pretraining and iii) spatial awareness. We then propose TrickVOS; a generic, method-agnostic bag of tricks addressing each aspect with i) a structure-aware hybrid loss, ii) a simple decoder pretraining regime and iii) a cheap tracker that imposes spatial constraints in model predictions. Finally, we propose a lightweight network and show that when trained with TrickVOS, it achieves competitive results to state-of-the-art methods on DAVIS and YouTube benchmarks, while being one of the first STM-based SVOS methods that can run in real-time on a mobile device.", "venue": "arXiv", "keywords": ["loss functions", "temporal consistency"]}
{"id": "skorokhodovAligningLatentImage2021", "title": "Aligning Latent and Image Spaces to Connect the Unconnectable", "abstract": "In this work, we develop a method to generate infinite high-resolution images with diverse and complex content. It is based on a perfectly equivariant generator with synchronous interpolations in the image and latent spaces. Latent codes, when sampled, are positioned on the coordinate grid, and each pixel is computed from an interpolation of the nearby style codes. We modify the AdaIN mechanism to work in such a setup and train the generator in an adversarial setting to produce images positioned between any two latent vectors. At test time, this allows for generating complex and diverse infinite images and connecting any two unrelated scenes into a single arbitrarily large panorama. Apart from that, we introduce LHQ: a new dataset of high-resolution nature landscapes. We test the approach on LHQ, LSUN Tower and LSUN Bridge and outperform the baselines by at least 4 times in terms of quality and diversity of the produced infinite images. The project page is located at https://universome.github.io/alis.", "venue": "arXiv", "keywords": ["image synthesis", "instance normalization"]}
{"id": "skorokhodovAligningLatentImage2021a", "title": "Aligning Latent and Image Spaces to Connect the Unconnectable", "abstract": "In this work, we develop a method to generate infinite high-resolution images with diverse and complex content. It is based on a perfectly equivariant generator with synchronous interpolations in the image and latent spaces. Latent codes, when sampled, are positioned on the coordinate grid, and each pixel is computed from an interpolation of the nearby style codes. We modify the AdaIN mechanism to work in such a setup and train the generator in an adversarial setting to produce images positioned between any two latent vectors. At test time, this allows for generating complex and diverse infinite images and connecting any two unrelated scenes into a single arbitrarily large panorama. Apart from that, we introduce LHQ: a new dataset of 90khigh-resolution nature landscapes. We test the approach on LHQ, LSUN Tower and LSUN Bridge and outperform the baselines by at least 4 times in terms of quality and diversity of the produced infinite images. The project website is located at https://universome.github.io/alis.", "venue": "arXiv", "keywords": []}
{"id": "smithFederatedMultiTaskLearning2017", "title": "Federated Multi-Task Learning", "abstract": "Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["federated learning", "multi-task learning"]}
{"id": "sojkaTechnicalReportICCV2023", "title": "Technical Report for ICCV 2023 Visual Continual Learning Challenge: Continuous Test-time Adaptation for Semantic Segmentation", "abstract": "The goal of the challenge is to develop a test-time adaptation (TTA) method, which could adapt the model to gradually changing domains in video sequences for semantic segmentation task. It is based on a synthetic driving video dataset - SHIFT. The source model is trained on images taken during daytime in clear weather. Domain changes at test-time are mainly caused by varying weather conditions and times of day. The TTA methods are evaluated in each image sequence (video) separately, meaning the model is reset to the source model state before the next sequence. Images come one by one and a prediction has to be made at the arrival of each frame. Each sequence is composed of 401 images and starts with the source domain, then gradually drifts to a different one (changing weather or time of day) until the middle of the sequence. In the second half of the sequence, the domain gradually shifts back to the source one. Ground truth data is available only for the validation split of the SHIFT dataset, in which there are only six sequences that start and end with the source domain. We conduct an analysis specifically on those sequences. Ground truth data for test split, on which the developed TTA methods are evaluated for leader board ranking, are not publicly available. The proposed solution secured a 3rd place in a challenge and received an innovation award. Contrary to the solutions that scored better, we did not use any external pretrained models or specialized data augmentations, to keep the solutions as general as possible. We have focused on analyzing the distributional shift and developing a method that could adapt to changing data dynamics and generalize across different scenarios.", "venue": "arXiv", "keywords": ["challenges"]}
{"id": "sokolicRobustnessInvarianceGeneralization2017", "title": "Robustness and Invariance in the Generalization Error of Deep Neural Networks", "abstract": "", "venue": "", "keywords": ["domain generalization", "generalization quantification", "robustness analysis"]}
{"id": "songESMAMLSimpleHessianFree2020", "title": "ES-MAML: Simple Hessian-Free Meta Learning", "abstract": "We introduce ES-MAML, a new framework for solving the model agnostic meta learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms for MAML are based on policy gradients, and incur significant difficulties when attempting to estimate second derivatives using backpropagation on stochastic policies. We show how ES can be applied to MAML to obtain an algorithm which avoids the problem of estimating second derivatives, and is also conceptually simple and easy to implement. Moreover, ES-MAML can handle new types of non-smooth adaptation operators, and other techniques for improving performance and estimation of ES methods become applicable. We show empirically that ES-MAML is competitive with existing methods and often yields better adaptation with fewer queries.", "venue": "arXiv", "keywords": ["meta-learning"]}
{"id": "songPowerInferFastLarge2023", "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU", "abstract": "This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.", "venue": "arXiv", "keywords": []}
{"id": "sooryaMODIFIEDGUIDEDIMAGE2014", "title": "MODIFIED GUIDED IMAGE FILTER", "abstract": "Filtering is widely used in image and video processing for various applications. In this paper a explicit image filter called guided filter is proposed to remove noise in images smoothing and sharpening of images. The guided filter is derived from the local linear model compute the filtering output by considering the content of guided image which can be the input image or another different image. The guided filter can be used as an edge preserving smoothing operator. Transfer the structure of guided image to the filtering output enabling and filtering application like dehazing and guided feathering. Computer vision and computer graphics applications, including edge-aware smoothing, detail enhancement, HDR compression, image matting.", "venue": "", "keywords": ["filter augmentations", "guided filters"]}
{"id": "sorzanoSurveyDimensionalityReduction", "title": "A Survey of Dimensionality Reduction Techniques", "abstract": "Experimental life sciences like biology or chemistry have seen in the recent decades an explosion of the data available from experiments. Laboratory instruments become more and more complex and report hundreds or thousands measurements for a single experiment and therefore the statistical methods face challenging tasks when dealing with such high-dimensional data. However, much of the data is highly redundant and can be efficiently brought down to a much smaller number of variables without a significant loss of information. The mathematical procedures making possible this reduction are called dimensionality reduction techniques; they have widely been developed by fields like Statistics or Machine Learning, and are currently a hot research topic. In this review we categorize the plethora of dimension reduction techniques available and give the mathematical insight behind them.", "venue": "", "keywords": []}
{"id": "sorzanoSurveyDimensionalityReductiona", "title": "A Survey of Dimensionality Reduction Techniques", "abstract": "Experimental life sciences like biology or chemistry have seen in the recent decades an explosion of the data available from experiments. Laboratory instruments become more and more complex and report hundreds or thousands measurements for a single experiment and therefore the statistical methods face challenging tasks when dealing with such high-dimensional data. However, much of the data is highly redundant and can be efficiently brought down to a much smaller number of variables without a significant loss of information. The mathematical procedures making possible this reduction are called dimensionality reduction techniques; they have widely been developed by fields like Statistics or Machine Learning, and are currently a hot research topic. In this review we categorize the plethora of dimension reduction techniques available and give the mathematical insight behind them.", "venue": "", "keywords": []}
{"id": "srivastavaComprehensiveReviewSoil2021", "title": "A Comprehensive Review on Soil Classification Using Deep Learning and Computer Vision Techniques", "abstract": "Soil classification is one of the major affairs and emanating topics in a large number of countries. The population of the world is rising at a majorly rapid pace and along with the increase in population, the demand for food surges actively. Typical techniques employed by the farmers are not adequate enough to fulfill the increasing requirements and therefore they have to hinder the cultivating soil. For proper crop yield, farmers should be aware of the correct soil type for a particular crop, which affects the increased demand for food. There are various laboratory and field methods to classify soil, but these have limitations like time and labor-consuming. There is a requirement of computer-based soil classification techniques which will help farmers in the field and won't take a lot of time. This paper talks about different computer-based soil classification practices divided into two streams. First is image processing and computer vision-based soil classification approaches which include the conventional image processing algorithms and methods to classify soil using different features like texture, color, and particle size. Second is deep learning and machine learning-based soil classification approaches, such as CNN, which yields state-of-the-art results. Deep learning applications mostly diminish the dependency on spatial-form designs and preprocessing techniques by facilitating the end-to-end process. This paper also presents some databases created by the researchers according to the objective of the study. Databases are created under different environmental and illumination conditions, using different appliances such as digital cameras, digital camcorder, and a smartphone camera. Also, evaluation metrics are briefly discussed to layout some graded measures for differentiation. This review serves as a brief guide to new researchers in the field of soil classification, it provides fundamental understanding and general knowledge of the modern state-of-the-art researches, in addition to skillful researchers considering some dynamic trends for future work.", "venue": "Multimedia Tools and Applications", "keywords": ["surveys"]}
{"id": "staibDistributionallyRobustOptimization2019", "title": "Distributionally Robust Optimization and Generalization in Kernel Methods", "abstract": "Distributionally robust optimization (DRO) has attracted attention in machine learning due to its connections to regularization, generalization, and robustness. Existing work has considered uncertainty sets based on -divergences and Wasserstein distances, each of which have drawbacks. In this paper, we study DRO with uncertainty sets measured via maximum mean discrepancy (MMD). We show that MMD DRO is roughly equivalent to regularization by the Hilbert norm and, as a byproduct, reveal deep connections to classic results in statistical learning. In particular, we obtain an alternative proof of a generalization bound for Gaussian kernel ridge regression via a DRO lense. The proof also suggests a new regularizer. Our results apply beyond kernel methods: we derive a generically applicable approximation of MMD DRO, and show that it generalizes recent work on variance-based regularization.", "venue": "arXiv", "keywords": ["domain generalization", "kernel methods"]}
{"id": "StandardsPreparingTheses", "title": "Standards for Preparing Theses and Dissertations", "abstract": "", "venue": "", "keywords": []}
{"id": "stimperMultidimensionalContrastLimited2019", "title": "Multidimensional Contrast Limited Adaptive Histogram Equalization", "abstract": "Contrast enhancement is an important preprocessing technique for improving the performance of downstream tasks in image processing and computer vision. Among the existing approaches based on nonlinear histogram transformations, contrast limited adaptive histogram equalization (CLAHE) is a popular choice for dealing with 2D images obtained in natural and scientific settings. The recent hardware upgrade in data acquisition systems results in significant increase in data complexity, including their sizes and dimensions. Measurements of densely sampled data higher than three dimensions, usually composed of 3D data as a function of external parameters, are becoming commonplace in various applications in the natural sciences and engineering. The initial understanding of these complex multidimensional datasets often requires human intervention through visual examination, which may be hampered by the varying levels of contrast permeating through the dimensions. We show both qualitatively and quantitatively that using our multidimensional extension of CLAHE (MCLAHE) simultaneously on all dimensions of the datasets allows better visualization and discernment of multidimensional image features, as demonstrated using cases from 4D photoemission spectroscopy and fluorescence microscopy. Our implementation of multidimensional CLAHE in Tensorflow is publicly accessible and supports parallelization with multiple CPUs and various other hardware accelerators, including GPUs.", "venue": "IEEE Access", "keywords": ["histogram equalization"]}
{"id": "StyleTransferbasedAugmentation2023", "title": "A Style Transfer-based Augmentation Framework for Improving Segmentation and Classification Performance across Different Sources in Ultrasound Images", "abstract": "Paper Info Reviews Meta-review Author Feedback Post-Rebuttal Meta-reviews Authors Bin Huang, Ziyue Xu, Shing-Chow Chan, Zhong Liu, Huiying Wen, Chao Hou, Qicai Huang, Meiqin Jiang, Changfeng Dong, Jie Zeng, Ruhai Zou, Bingsheng Huang, Xin Chen, Shuo Li Abstract Ultrasound imaging can vary in style/appearance due to differences in scanning equipment and other factors, resulting in degraded segmentation and classification performance of deep learning models for ultrasound image analysis. Previous studies have attempted to solve this problem by using style transfer and augmentation techniques, but these methods usually require a large amount of data from multiple sources and source-specific discriminators, which are not feasible for medical datasets with limited samples. Moreover, finding suitable augmentation methods for ultrasound data can be difficult. To address these challenges, we propose a novel style transfer-based augmentation framework that consists of three components: mixed style augmentation (MixStyleAug), feature augmentation (FeatAug), and mask-based style augmentation (MaskAug). MixStyleAug uses a style transfer network to transform the style of a training image into various reference styles, which enriches the information from different sources for the network. FeatAug augments the styles at the feature level to compensate for possible style variations, especially for small-size datasets with limited styles. MaskAug leverages segmentation masks to highlight the key regions in the images, which enhances the model's generalizability. We evaluate our framework on five ultrasound datasets collected from different scanners and centers. Our framework outperforms previous methods on both segmentation and classification tasks, especially on small-size datasets. Our results suggest that our framework can effectively improve the performance of deep learning models across different ultrasound sources with limited data. Link to paper DOI: https://doi.org/10.1007/978-3-031-43987-2_5 SharedIt: https://rdcu.be/dnwJn Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper In this paper, the authors propose a data augmentation method for ultrasound images. By using style transformations, the authors propose mixed stye augmentation, feature augmentation, and mask-based stye augmentation. Experiments on five datasets demonstrate the effectiveness of the proposed methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The strengths of this paper are as follows: This paper examines data augmentation from three aspects to train CNNs on ultrasound images. The paper demonstrates the effectiveness of data augmentation based on style transformation for training multi-task networks of segmentation and classification. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Many questions are raised due to the poor organization of the paper. In the introduction, the authors describe that different imaging processes degrade performance. It is unclear whether this is for different modalities or the same modality with different equipment. It would be understandable if the same modality and the same equipment could produce differences in medical imaging. The authors should explain carefully. In the introduction, the authors describe a general problem in medical imaging. Since the authors propose data augmentation for ultrasound images in this paper, the focus should be on ultrasound. If the discussion begins with general medical images, the authors should explain why the focus is on ultrasound images. This paper deals with data augmentation of multi-task networks for segmentation and classification in ultrasound images. This should be explained in the introduction. The reviewer cannot understand the description of MaskAug in Sect. 2.3. Please rate the clarity and organization of this paper Poor Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for~instance, providing code and data is a plus, but not a requirement for acceptance The followings are the detailed comments to the authors. In Fig. 2, there are no inputs or outputs specified. Data augmentation generally produces variation in the training data. Fig. 2, on the other hand, contains a training source and a testing source. What do they indicate? Does \\ in Eq. (1) denote the feature map obtained after global average pooling? Also, in Eq. (1), is the same dynamic range noise given to the mean and variance? Experiments to evaluate accuracy should be conducted using not only private datasets, but also public datasets. Each dataset is divided into training and test. In general, we divide each dataset into training, validation, and test. What is the reason for not setting validation? In addition to evaluation within the dataset, experiments should also be conducted on cross DB to demonstrate the generality of the data augmentation. Cross-validation should also be performed to reduce data bias. Table II is referenced, but there is no Table II. Since AutoAug is cited in the paper, a comparison with AutoAug would be helpful. In the results of Table 1, why are LD1 and TD1 results for the training data? Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer's guide on what makes a good review: https://conferences.miccai.org/2023/en/REVIEWER-GUIDELINES.html The architecture of the multi-task netowork is described in the supplemental material, so it is possible to implement it. On the other hand, the parameters (e.g., \\ ) of the proposed data augmentation method are unknown, so the code must be publicly available in order to reproduce it. PyTorch is used for the implementation, but the version is not available. The experimental environment is described. Rate the paper on a scale of 1-8, 8 being the strongest (8-5: accept; 4-1: reject). Spreading the score helps create a distribution for decision-making 3 Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It is difficult to understand this paper correctly because of the poor organization of the paper, the lack of explanation of the proposed method in many parts, and the lack of clarity in the experimental conditions and results. So, I have rated this paper as reject. Reviewer confidence Very confident [Post rebuttal] After reading the author's rebuttal, state your overall opinion of the paper if it has been changed N/A [Post rebuttal] Please justify your decision N/A Review #2 Please describe the contribution of the paper The key contributions of the paper include: 1) A combined style augmentation approach that incorporates information from various sources to enhance the training set, thereby improving the model's performance. 2) Instead of augmenting in the original image domain, we suggested a feature-based augmentation method that adjusts the style at the feature level for more effective compensation of potential source variations. 3) To minimize the influence of irrelevant style information on ultrasound images during the style transfer process, we introduced a mask-based style augmentation strategy Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The authors have provided a thorough and detailed explanation of the proposed method, ensuring that readers can fully comprehend the underlying concepts and implementation. This clarity in presentation demonstrates the authors' deep understanding of the subject matter and promotes accessibility for the target audience. A significant strength of the paper is its comparison with previous methods in the field. By critically evaluating and contrasting the proposed method with existing approaches, the authors effectively demonstrate the improvements and advantages offered by their work, thereby highlighting its potential impact on the field. The paper concludes with a well-organized and concise summary of the main findings, contributions, and implications of the study. This section effectively synthesizes the key points of the paper, allowing readers to appreciate the significance of the work and its potential to advance the current state of knowledge in the field. The paper is complemented by well-designed flowcharts and figures that effectively illustrate the discussed concepts and methodologies. These visual aids not only enhance the readers' grasp of the content but also contribute to the overall professional appearance of the paper. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Insufficient Dataset Description: The authors have not provided an adequate description of the dataset utilized in the study, which may hinder the reader's understanding of the context and the applicability of the proposed method. Furthermore, the lack of information regarding the preprocessing steps implemented in the research could adversely affect the reproducibility of the method, thereby limiting its potential adoption by other researchers. Disorganized Introduction Section: The Introduction section of the paper appears to be lacking in organization and clarity, making it difficult for the reader to grasp the importance and motivation behind the proposed method. A well-structured Introduction that clearly outlines the research problem, its significance, and the authors' approach to addressing it is crucial for setting the context and engaging the readers. Inadequate Description of Previous Methods: While the paper compares the proposed method with earlier approaches in the field, the authors have not provided sufficient information about these previous methods. A brief but informative overview of the earlier approaches would have been valuable in helping the reader understand the limitations of existing methods and the reasons behind the development of the proposed method. This additional context would further emphasize the novelty and potential benefits of the authors' work. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for~instance, providing code and data is a plus, but not a requirement for acceptance The authors have not provided an adequate description of the dataset utilized in the study, which may hinder the reader's understanding of the context and the applicability of the proposed method. Furthermore, the lack of information regarding the preprocessing steps implemented in the research could adversely affect the reproducibility of the method, thereby limiting its potential adoption by other researchers Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer's guide on what makes a good review: https://conferences.miccai.org/2023/en/REVIEWER-GUIDELINES.html Insufficient Dataset Description: The authors have not provided an adequate description of the dataset utilized in the study, which may hinder the reader's understanding of the context and the applicability of the proposed method. Furthermore, the lack of information regarding the preprocessing steps implemented in the research could adversely affect the reproducibility of the method, thereby limiting its potential adoption by other researchers. Disorganized Introduction Section: The Introduction section of the paper appears to be lacking in organization and clarity, making it difficult for the reader to grasp the importance and motivation behind the proposed method. A well-structured Introduction that clearly outlines the research problem, its significance, and the authors' approach to addressing it is crucial for setting the context and engaging the readers. Inadequate Description of Previous Methods: While the paper compares the proposed method with earlier approaches in the field, the authors have not provided sufficient information about these previous methods. A brief but informative overview of the earlier approaches would have been valuable in helping the reader understand the limitations of existing methods and the reasons behind the development of the proposed method. This additional context would further emphasize the novelty and potential benefits of the authors' work. Addressing these weaknesses in the paper would greatly enhance its overall quality and increase its potential to make a meaningful contribution to the existing body of literature in the field Rate the paper on a scale of 1-8, 8 being the strongest (8-5: accept; 4-1: reject). Spreading the score helps create a distribution for decision-making 7 Please justify your recommendation. What were the major factors that led you to your overall score for this paper? - Reviewer confidence Very confident [Post rebuttal] After reading the author's rebuttal, state your overall opinion of the paper if it has been changed N/A [Post rebuttal] Please justify your decision N/A Review #3 Please describe the contribution of the paper The paper presents an augmentation based on style transfer for improving segmentation and classification. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. well-written Good ablation experiment evaluated on multiple datasets collected by diverse imaging setting. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The p-values could be reported for Table 1. The method is computationally expensive containing style network and classification/segmentation network but dice score improvement is very negligible (I was wondering if it is even statistically significant ) Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for~instance, providing code and data is a plus, but not a requirement for acceptance The network architecture and training is explained but dataset is not available online Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer's guide on what makes a good review: https://conferences.miccai.org/2023/en/REVIEWER-GUIDELINES.html It is not clear how part B (feature augmentation) is used. The features are translated but how the translated feature were used for the final network? Rate the paper on a scale of 1-8, 8 being the strongest (8-5: accept; 4-1: reject). Spreading the score helps create a distribution for decision-making 5 Please justify your recommendation. What were the major factors that led you to your overall score for this paper? They have good contributions and the method is evaluated on diverse datasets. Reviewer confidence Very confident [Post rebuttal] After reading the author's rebuttal, state your overall opinion of the paper if it has been changed N/A [Post rebuttal] Please justify your decision N/A Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers' recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The proposed data augmentation schemes would be useful for the ultrasound imaging community. More clarifications to the key sections of methodology would be needed. Author Feedback Thanks to the meta-reviewer and reviewers for your valuable comments of our work. Especially, thanks to R2 and R3 for accepting the paper directly. We appreciate your recognition of: Our contributions (Meta-Reviewer: ``useful for the ultrasound imaging community''); Sufficient experiments (R3: ``Good ablation experiment'' R2: ``demonstrate the improvements and advantages''); Well-organization (R2: ``detailed explanation of the proposed method'' R2: ``well-designed flowcharts and figures'' R3: ``well-written''). Q1: Why focus on ultrasound (US) (R1). A1: US presents unique challenges compared to other modalities. 1) US is affected by speckle noise, which differs from common Gaussian noise. 2) Acoustic shadow in US can cause missing information in the image. These characteristics can lead to degraded performance in similar studies. Q2: Clarifications of description in Sect.2.3 MaskAug (R1). A2: To provide clearer explanations, Fig. 2C illustrates the pipeline of MaskAug and the simplified steps are as follows: 1) Select content/style images from training/testing sources. 2) Use a trained network to generate ROIs of these images. 3) Feed the content image, style image and ROIs into style transfer network. 4) Translate the intensity distribution of ROIs in the content image to that of the style image. Q3: Clarifications about Eq.1 (R1). A3: \\ is not the feature maps after GAP, as shown in Fig.S1 of supplementary materials. Dynamic range noise for the mean and variance is the same. Q4: Clarifications about the use of training/testing sources, validation set, and the input/output in Fig.2 (R1). A4: As stated in the 3rd sentence of Sect.3, our datasets are divided into training and testing sources. Images from training and testing sources are available during training, but only labels from training source are used for loss computation. We randomly selected 20% of training set as validation set during training. In Fig.2, inputs are the images from training and testing sources, and outputs are the segmentation and classification results. Q5: Why are LD1 and TD1 results for the training data in Table.1 (R1)? A5: They are not the results for training data. As mentioned in the caption of Table.1 and the 4th sentence of Sect.3, they represent the performance of the testing set from training sources. Q6: Lack of information on preprocessing steps (R2). A6: Preprocessing steps are described in the 5th sentence of Sect.3. Q7: Use of part B in final network (R3). A7: Part B (FeatAug) is not used in final network, as stated in the 1st sentence of Section 2.2. Q8: Conduct experiments using public datasets (R1). A8: We conducted experiments on a public thyroid dataset (DDTI). The DSC/AUROC values for BigAug, Hesse et al., UDA and our method are 0.526/0.488, 0.645/0.537, 0.470/0.565 and 0.680/0.614, respectively. These experiments indicate that our method achieves the best performance on both public and private datasets, further demonstrating its generality. Q9: A comparison with AutoAug would be helpful (R1). A9: To address your concern, we applied AutoAug to our datasets and found that it further highlights the superiority of our method. The DSC/AUROC values for LD1, LD2, LD3, TD1 and TD2 using AutoAug are 0.939/0.871, 0.876/0.653, 0.914/0.678, 0.687/0.769 and 0.535/0.392, respectively. The results indicate that AutoAug performs worse than our method and support our conclusions. Q10: p could be reported in Table.1 and question about negligible improvement of dice score (R3). A10: To compare our method with traditional augmentation, we conducted statistical analysis and observed significant improvements in DSC for TD1 and TD2 (p 0.05), as well as significant improvements in AUROC for LD1, LD2, LD3 and TD1 (p 0.05). The DSC shows no significant improvement in LDs, as the liver is easy to segment, resulting in a high baseline. Although the increase in DSC is slight, our method improves both AUROC and DSC across all sources. Post-rebuttal Meta-Reviews Meta-review # 1 (Primary) Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The author's feedback has addressed most of the comments from the reviewer, the method description is much more clarified now. Meta-review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper is interesting and the task clinically relevant. However, in its current state the paper needs significant improvement regarding its organization and clarity, lacks enough information about the dataset used and enough discussion of related works. Meta-review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper proposes a data augmentation strategy for ultrasound images based on style transfer. The method is interesting and tested on five different ultrasound datasets. The reviews were very mixed (strong accept to reject). The main criticism is about missing details and inadequacies in data and method description. I agree with this. The data is not described at all and the tasks of segmentation and classification are not specified. What is classified here? Why is a multi-task network necessary? Unfortunately, the rebuttal is convincing and opens even more questions (FeatAug is not used in final network? Training and testing split is unclear. It seems that the test data is also used for validation, ). Therefore, I recommend rejection of the paper. back to top", "venue": "MICCAI 2023 - Accepted Papers, Reviews, Author Feedback", "keywords": []}
{"id": "suConsistencyLearningBased2022", "title": "Consistency Learning Based on Class-Aware Style Variation for Domain Generalizable Semantic Segmentation", "abstract": "", "venue": "Proceedings of the 30th ACM International Conference on Multimedia", "keywords": ["consistency training", "domain generalization", "semantic segmentation", "style transfer"]}
{"id": "suFVAEsImproveVAEs2018", "title": "F-VAEs: Improve VAEs with Conditional Flows", "abstract": "In this paper, we integrate VAEs and flow-based generative models successfully and get f-VAEs. Compared with VAEs, f-VAEs generate more vivid images, solved the blurred-image problem of VAEs. Compared with flow-based models such as Glow, f-VAE is more lightweight and converges faster, achieving the same performance under smaller-size architecture.", "venue": "arXiv", "keywords": ["normalizing flows", "vaes"]}
{"id": "sunC2DANImprovedDeep2020", "title": "C2DAN: An Improved Deep Adaptation Network with Domain Confusion and Classifier Adaptation", "abstract": "Deep neural networks have been successfully applied in domain adaptation which uses the labeled data of source domain to supplement useful information for target domain. Deep Adaptation Network (DAN) is one of these efficient frameworks, it utilizes Multi-Kernel Maximum Mean Discrepancy (MK-MMD) to align the feature distribution in a reproducing kernel Hilbert space. However, DAN does not perform very well in feature level transfer, and the assumption that source and target domain share classifiers is too strict in different adaptation scenarios. In this paper, we further improve the adaptability of DAN by incorporating Domain Confusion (DC) and Classifier Adaptation (CA). To achieve this, we propose a novel domain adaptation method named C2DAN. Our approach first enables Domain Confusion (DC) by using a domain discriminator for adversarial training. For Classifier Adaptation (CA), a residual block is added to the source domain classifier in order to learn the difference between source classifier and target classifier. Beyond validating our framework on the standard domain adaptation dataset office-31, we also introduce and evaluate on the Comprehensive Cars (CompCars) dataset, and the experiment results demonstrate the effectiveness of the proposed framework C2DAN.", "venue": "Sensors", "keywords": ["distribution estimation", "entropic optimization", "loss functions", "model adaptation"]}
{"id": "sunComparisonCognitiveDifferences2022", "title": "Comparison of Cognitive Differences of Artworks between Artist and Artistic Style Transfer", "abstract": "This study explores how audiences responded to perceiving and distinguishing the paintings created by AI or human artists. The stimuli were six paintings which were completed by AI and human artists. A total of 750 subjects participated to identify which ones were completed by human artists or by AI. Results revealed that most participants could correctly distinguish between paintings made by AI or human artists and that accuracy was higher for those who used ``intuition'' as the criterion for judgment. The participants preferred the paintings created by human artists. Furthermore, there were big differences in the perception of the denotation and connotation of paintings between audiences of different backgrounds. The reasons for this will be analyzed in subsequent research.", "venue": "Applied Sciences", "keywords": ["style transfer"]}
{"id": "sunIBAFormerIntrabatchAttention2023", "title": "IBAFormer: Intra-batch Attention Transformer for Domain Generalized Semantic Segmentation", "abstract": "Domain generalized semantic segmentation (DGSS) is a critical yet challenging task, where the model is trained only on source data without access to any target data. Despite the proposal of numerous DGSS strategies, the generalization capability remains limited in CNN architectures. Though some Transformer-based segmentation models show promising performance, they primarily focus on capturing intrasample attentive relationships, disregarding inter-sample correlations which can potentially benefit DGSS. To this end, we enhance the attention modules in Transformer networks for improving DGSS by incorporating information from other independent samples in the same batch, enriching contextual information, and diversifying the training data for each attention block. Specifically, we propose two alternative intra-batch attention mechanisms, namely mean-based intrabatch attention (MIBA) and element-wise intra-batch attention (EIBA), to capture correlations between different samples, enhancing feature representation and generalization capabilities. Building upon intra-batch attention, we introduce IBAFormer, which integrates self-attention modules with the proposed intra-batch attention for DGSS. Extensive experiments demonstrate that IBAFormer achieves SOTA performance in DGSS, and ablation studies further confirm the effectiveness of each introduced component.", "venue": "arXiv", "keywords": ["ablation candidates", "domain generalization", "feature fusion", "segformer", "semantic segmentation", "transformers"]}
{"id": "sunLoFTRDetectorFreeLocal2021", "title": "LoFTR: Detector-Free Local Feature Matching with Transformers", "abstract": "We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. In contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive field provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. LoFTR also ranks first on two public benchmarks of visual localization among the published methods.", "venue": "arXiv", "keywords": ["feature pyramids", "transformers"]}
{"id": "sunRevisitingUnreasonableEffectiveness2017", "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era", "abstract": "The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10 or 100 ? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pretraining) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-theart results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.", "venue": "2017 IEEE International Conference on Computer Vision (ICCV)", "keywords": ["representation learning", "semantic segmentation", "surveys"]}
{"id": "suPixelAdaptiveConvolutionalNeural2019", "title": "Pixel-Adaptive Convolutional Neural Networks", "abstract": "Convolutions are the fundamental building blocks of CNNs. The fact that their weights are spatially shared is one of the main reasons for their widespread use, but it is also a major limitation, as it makes convolutions contentagnostic. We propose a pixel-adaptive convolution (PAC) operation, a simple yet effective modification of standard convolutions, in which the filter weights are multiplied with a spatially varying kernel that depends on learnable, local pixel features. PAC is a generalization of several popular filtering techniques and thus can be used for a wide range of use cases. Specifically, we demonstrate state-ofthe-art performance when PAC is used for deep joint image upsampling. PAC also offers an effective alternative to fully-connected CRF (Full-CRF), called PAC-CRF, which performs competitively compared to Full-CRF, while being considerably faster. In addition, we also demonstrate that PAC can be used as a drop-in replacement for convolution layers in pre-trained networks, resulting in consistent performance improvements.", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["cnns", "crfs"]}
{"id": "suSASFormerTransformersSparsely2023", "title": "SASFormer: Transformers for Sparsely Annotated Semantic Segmentation", "abstract": "Semantic segmentation based on sparse annotation has advanced in recent years. It labels only part of each object in the image, leaving the remainder unlabeled. Most of the existing approaches are time-consuming and often necessitate a multi-stage training strategy. In this work, we propose a simple yet effective sparse annotated semantic segmentation framework based on segformer, dubbed SASFormer, that achieves remarkable performance. Specifically, the framework first generates hierarchical patch attention maps, which are then multiplied by the network predictions to produce correlated regions separated by valid labels. Besides, we also introduce the affinity loss to ensure consistency between the features of correlation results and network predictions. Extensive experiments showcase that our proposed approach is superior to existing methods and achieves cutting-edge performance. The source code is available at ://github.com/su-hui-zz/SASFormer\\.", "venue": "arXiv", "keywords": ["affinity modeling", "segformer", "semantic segmentation", "transformers"]}
{"id": "suVariationalInferenceUnified2018", "title": "Variational Inference: A Unified Framework of Generative Models and Some Revelations", "abstract": "We reinterpreting the variational inference in a new perspective. Via this way, we can easily prove that EM algorithm, VAE, GAN, AAE, ALI(BiGAN) are all special cases of variational inference. The proof also reveals the loss of standard GAN is incomplete and it explains why we need to train GAN cautiously. From that, we find out a regularization term to improve stability of GAN training.", "venue": "arXiv", "keywords": []}
{"id": "szegedyRethinkingInceptionArchitecture2015", "title": "Rethinking the Inception Architecture for Computer Vision", "abstract": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.", "venue": "arXiv", "keywords": ["foundational", "loss functions"]}
{"id": "t.dinhFedUUnifiedFramework2021", "title": "FedU: A Unified Framework for Federated Multi-Task Learning with Laplacian Regularization", "abstract": "Federated multi-task learning (FMTL) has emerged as a natural choice to capture the statistical diversity among the clients in federated learning. To unleash the potential of FMTL beyond statistical diversity, we formulate a new FMTL problem FedU using Laplacian regularization, which can explicitly leverage relationships among the clients for multi-task learning. We first show that FedU provides a unified framework covering a wide range of problems such as conventional federated learning, personalized federated learning, few-shot learning, and stratified model learning. We then propose algorithms including both communication-centralized and decentralized schemes to learn optimal models of FedU. Theoretically, we show that the convergence rates of both FedU's algorithms achieve linear speedup for strongly convex and sublinear speedup of order \\ for nonconvex objectives. While the analysis of FedU is applicable to both strongly convex and nonconvex loss functions, the conventional FMTL algorithm MOCHA, which is based on CoCoA framework, is only applicable to convex case. Experimentally, we verify that FedU outperforms the vanilla FedAvg, MOCHA, as well as pFedMe and Per-FedAvg in personalized federated learning.", "venue": "", "keywords": ["federated learning", "multi-task learning"]}
{"id": "tainImprovingSemiSupervisedSemantic2023", "title": "Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network", "abstract": "Semi-supervised semantic segmentation (SSS) is an important task that utilizes both labeled and unlabeled data to reduce expenses on labeling training examples. However, the effectiveness of SSS algorithms is limited by the difficulty of fully exploiting the potential of unlabeled data. To address this, we propose a dual-level Siamese structure network (DSSN) for pixel-wise contrastive learning. By aligning positive pairs with a pixel-wise contrastive loss using strong augmented views in both low-level image space and high-level feature space, the proposed DSSN is designed to maximize the utilization of available unlabeled data. Additionally, we introduce a novel class-aware pseudo-label selection strategy for weak-to-strong supervision, which addresses the limitations of most existing methods that do not perform selection or apply a predefined threshold for all classes. Specifically, our strategy selects the top high-confidence prediction of the weak view for each class to generate pseudo labels that supervise the strong augmented views. This strategy is capable of taking into account the class imbalance and improving the performance of long-tailed classes. Our proposed method achieves state-of-the-art results on two datasets, PASCAL VOC 2012 and Cityscapes, outperforming other SSS algorithms by a significant margin.", "venue": "arXiv", "keywords": ["contrastive learning", "semantic segmentation", "semi-supervised learning", "siamese networks"]}
{"id": "takaseSelfpacedDataAugmentation2021", "title": "Self-Paced Data Augmentation for Training Neural Networks", "abstract": "Data augmentation is widely used for machine learning; however, an effective method to apply data augmentation has not been established even though it includes several factors that should be tuned carefully. One such factor is sample suitability, which involves selecting samples that are suitable for data augmentation. A typical method that applies data augmentation to all training samples disregards sample suitability, which may reduce classifier performance. To address this problem, we propose the self-paced augmentation (SPA) to automatically and dynamically select suitable samples for data augmentation when training a neural network. The proposed method mitigates the deterioration of generalization performance caused by ineffective data augmentation. We discuss two reasons the proposed SPA works relative to curriculum learning and desirable changes to loss function instability. Experimental results demonstrate that the proposed SPA can improve the generalization performance, particularly when the number of training samples is small. In addition, the proposed SPA outperforms the state-of-the-art RandAugment method.", "venue": "Neurocomputing", "keywords": ["auto-augmentation policies", "curriculum learning"]}
{"id": "tancikBlockNeRFScalableLarge2022", "title": "Block-NeRF: Scalable Large Scene Neural View Synthesis", "abstract": "We present Block-NeRF, a variant of Neural Radiance Fields that can represent large-scale environments. Specifically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental conditions. We add appearance embeddings, learned pose refinement, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco.", "venue": "arXiv.org", "keywords": ["image synthesis", "neural radiance fields"]}
{"id": "tanExposureBasedMultiHistogram2019", "title": "Exposure Based Multi-Histogram Equalization Contrast Enhancement for Non-Uniform Illumination Images", "abstract": "Non-uniform illuminated images have poses challenges in contrast enhancement due to the existence of different exposure region caused by the uneven illumination. Although Histogram Equalization (HE) is a well-known method for contrast improvement, however, existing HE-based enhancement methods for non-illumination often generated unnatural images, introduced unwanted artefacts and washed out effect because they do not utilize the information from the different exposure regions in performing equalization. Therefore, this study proposes a modified HE-based contrast enhancement technique for non-uniform illuminated images namely Exposure Region-Based Multi-Histogram Equalization (ERMHE). ERMHE uses exposure region-based histogram segmentation thresholds to segment the original histogram into subhistograms. With the thresholded sub-histograms, ERMHE then uses an entropy-controlled gray level allocation scheme to allocate new output gray level range and to obtain new thresholds that will be used to repartition the histogram prior to HE process. 154 non-uniform illuminated sample images are used to evaluate the application of the proposed ERMHE. By comparing ERMHE to four existing HE-based contrast enhancement namely, Global HE, Mean Preserving Bi-Histogram Equalization (BBHE), Dualistic Sub-Image Histogram Equalization (DSIHE) and Contrast Limited Adaptive Histogram Equalization (CLAHE), qualitatively, ERMHE produces enhanced images with a natural appearance, appealing contrast, less degradation and reasonable detail preservation. Quantitatively, ERMHE achieves the highest Peak Signal to Noise Ratio (PSNR), lowest Absolute Mean Brightness Error (AMBE) and second best in Discrete Entropy (DE) scores. From the analyses, ERMHE has shown its capability in enhancing different exposure regions exist in a non-uniform illuminated images.", "venue": "IEEE Access", "keywords": ["histogram equalization"]}
{"id": "tanielianApproximatingLipschitzContinuous2021", "title": "Approximating Lipschitz Continuous Functions with GroupSort Neural Networks", "abstract": "Recent advances in adversarial attacks and Wasserstein GANs have advocated for use of neural networks with restricted Lipschitz constants. Motivated by these observations, we study the recently introduced GroupSort neural networks, with constraints on the weights, and make a theoretical step towards a better understanding of their expressive power. We show in particular how these networks can represent any Lipschitz continuous piecewise linear functions. We also prove that they are well-suited for approximating Lipschitz continuous functions and exhibit upper bounds on both the depth and size. To conclude, the efficiency of GroupSort networks compared with more standard ReLU networks is illustrated in a set of synthetic experiments.", "venue": "Proceedings of The 24th International Conference on Artificial Intelligence and Statistics", "keywords": ["lipschitz-constraints"]}
{"id": "tanRethinkingMultidomainGeneralization2024", "title": "Rethinking Multi-domain Generalization with A General Learning Objective", "abstract": "Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a \\ -mapping to relax the constraint. We rethink the learning objective for mDG and design a new learning objective\\ to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of domain-independent conditional features, disclosing that many previous mDG endeavors actually partially the objective\\ and thus lead to limited performance. As such, our study distills a general learning objective into four practical components, providing a general, robust, and flexible mechanism to handle complex domain shifts. Extensive empirical results indicate that the proposed objective with \\ -mapping leads to substantially better mDG performance in various downstream tasks, including regression, segmentation, and classification.", "venue": "arXiv", "keywords": []}
{"id": "tasdighiProbabilisticActorCriticLearning2024", "title": "Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty", "abstract": "We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement learning. We report empirical evaluations demonstrating PAC's enhanced stability and improved performance over the state of the art in diverse continuous control problems.", "venue": "arXiv", "keywords": ["actor-critic", "pac learning", "reinforcement learning"]}
{"id": "tatuNovelActiveContour2013", "title": "A Novel Active Contour Model for Texture Segmentation", "abstract": "Texture is intuitively defined as a repeated arrangement of a basic pattern or object in an image. There is no mathematical definition of a texture though. The human visual system is able to identify and segment different textures in a given image. Automating this task for a computer is far from trivial.", "venue": "arXiv", "keywords": ["texture transfer"]}
{"id": "taylorImprovingDeepLearning2017", "title": "Improving Deep Learning Using Generic Data Augmentation", "abstract": "Deep artificial neural networks require a large corpus of training data in order to effectively learn, where collection of such training data is often expensive and laborious. Data augmentation overcomes this issue by artificially inflating the training set with label preserving transformations. Recently there has been extensive use of generic data augmentation to improve Convolutional Neural Network (CNN) task performance. This study benchmarks various popular data augmentation schemes to allow researchers to make informed decisions as to which training methods are most appropriate for their data sets. Various geometric and photometric schemes are evaluated on a coarse-grained data set using a relatively simple CNN. Experimental results, run using 4-fold cross-validation and reported in terms of Top-1 and Top-5 accuracy, indicate that cropping in geometric augmentation significantly increases CNN task performance.", "venue": "arXiv", "keywords": []}
{"id": "temelCUREORChallengingUnreal2018", "title": "CURE-OR: Challenging Unreal and Real Environments for Object Recognition", "abstract": "In this paper, we introduce a large-scale, controlled, and multi-platform object recognition dataset denoted as Challenging Unreal and Real Environments for Object Recognition (CURE-OR). In this dataset, there are 1,000,000 images of 100 objects with varying size, color, and texture that are positioned in five different orientations and captured using five devices including a webcam, a DSLR, and three smartphone cameras in real-world (real) and studio (unreal) environments. The controlled challenging conditions include underexposure, overexposure, blur, contrast, dirty lens, image noise, resizing, and loss of color information. We utilize CURE-OR dataset to test recognition APIs-Amazon Rekognition and Microsoft Azure Computer Vision- and show that their performance significantly degrades under challenging conditions. Moreover, we investigate the relationship between object recognition and image quality and show that objective quality algorithms can estimate recognition performance under certain photometric challenging conditions. The dataset is publicly available at https://ghassanalregib.com/cure-or/.", "venue": "arXiv", "keywords": ["dataset debut", "occluded images"]}
{"id": "temelObjectRecognitionMultifarious2019", "title": "Object Recognition under Multifarious Conditions: A Reliability Analysis and A Feature Similarity-based Performance Estimation", "abstract": "In this paper, we investigate the reliability of online recognition platforms, Amazon Rekognition and Microsoft Azure, with respect to changes in background, acquisition device, and object orientation. We focus on platforms that are commonly used by the public to better understand their real-world performances. To assess the variation in recognition performance, we perform a controlled experiment by changing the acquisition conditions one at a time. We use three smartphones, one DSLR, and one webcam to capture side views and overhead views of objects in a living room, an office, and photo studio setups. Moreover, we introduce a framework to estimate the recognition performance with respect to backgrounds and orientations. In this framework, we utilize both handcrafted features based on color, texture, and shape characteristics and data-driven features obtained from deep neural networks. Experimental results show that deep learning-based image representations can estimate the recognition performance variation with a Spearman's rank-order correlation of 0.94 under multifarious acquisition conditions.", "venue": "arXiv", "keywords": ["dataset debut", "occluded images"]}
{"id": "teshimaCouplingbasedInvertibleNeural2020", "title": "Coupling-Based Invertible Neural Networks Are Universal Diffeomorphism Approximators", "abstract": "Invertible neural networks based on coupling flows (CF-INNs) have various machine learning applications such as image synthesis and representation learning. However, their desirable characteristics such as analytic invertibility come at the cost of restricting the functional forms. This poses a question on their representation power: are CF-INNs universal approximators for invertible functions? Without a universality, there could be a well-behaved invertible transformation that the CF-INN can never approximate, hence it would render the model class unreliable. We answer this question by showing a convenient criterion: a CF-INN is universal if its layers contain affine coupling and invertible linear functions as special cases. As its corollary, we can affirmatively resolve a previously unsolved problem: whether normalizing flow models based on affine coupling can be universal distributional approximators. In the course of proving the universality, we prove a general theorem to show the equivalence of the universality for certain diffeomorphism classes, a theoretical insight that is of interest by itself.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["highly-analytical", "normalizing flows"]}
{"id": "tewariAdvancesNeuralRendering2021", "title": "Advances in Neural Rendering", "abstract": "Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...", "venue": "arXiv.org", "keywords": ["3d scenes", "generative augmentation", "neural fields", "neural rendering", "surveys"]}
{"id": "tewariStateArtNeural2020", "title": "State of the Art on Neural Rendering", "abstract": "Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. This state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.", "venue": "arXiv.org", "keywords": ["3d scenes", "neural fields", "neural rendering", "surveys"]}
{"id": "theodoridisTrappedTextureBias2022", "title": "Trapped in Texture Bias? A Large Scale Comparison of Deep Instance Segmentation", "abstract": "Do deep learning models for instance segmentation generalize to novel objects in a systematic way? For classification, such behavior has been questioned. In this study, we aim to understand if certain design decisions such as framework, architecture or pre-training contribute to the semantic understanding of instance segmentation. To answer this question, we consider a special case of robustness and compare pre-trained models on a challenging benchmark for object-centric, out-of-distribution texture. We do not introduce another method in this work. Instead, we take a step back and evaluate a broad range of existing literature. This includes Cascade and Mask R-CNN, Swin Transformer, BMask, YOLACT(++), DETR, BCNet, SOTR and SOLOv2. We find that YOLACT++, SOTR and SOLOv2 are significantly more robust to out-of-distribution texture than other frameworks. In addition, we show that deeper and dynamic architectures improve robustness whereas training schedules, data augmentation and pre-training have only a minor impact. In summary we evaluate 68 models on 61 versions of MS COCO for a total of 4148 evaluations.", "venue": "arXiv", "keywords": ["bias sources", "semantic segmentation"]}
{"id": "TheoreticalIssuesDeep", "title": "Theoretical Issues in Deep Networks", "abstract": "", "venue": "", "keywords": []}
{"id": "ThesisProposalPresentations2023", "title": "Thesis Proposal Presentations Sample.Pdf.Pptx", "abstract": "Thesis Proposal Presentations Sample.pdf.pptx - Download as a PDF or view online for free", "venue": "SlideShare", "keywords": []}
{"id": "tianImprovingAutoAugmentAugmentationWise2020", "title": "Improving Auto-Augment via Augmentation-Wise Weight Sharing", "abstract": "The recent progress on automatically searching augmentation policies has boosted the performance substantially for various tasks. A key component of automatic augmentation search is the evaluation process for a particular augmentation policy, which is utilized to return reward and usually runs thousands of times. A plain evaluation process, which includes full model training and validation, would be time-consuming. To achieve efficiency, many choose to sacrifice evaluation reliability for speed. In this paper, we dive into the dynamics of augmented training of the model. This inspires us to design a powerful and efficient proxy task based on the Augmentation-Wise Weight Sharing (AWS) to form a fast yet accurate evaluation process in an elegant way. Comprehensive analysis verifies the superiority of this approach in terms of effectiveness and efficiency. The augmentation policies found by our method achieve superior accuracies compared with existing auto-augmentation search methods. On CIFAR-10, we achieve a top-1 error rate of 1.24%, which is currently the best performing single model without extra training data. On ImageNet, we get a top-1 error rate of 20.36% for ResNet-50, which leads to 3.34% absolute error rate reduction over the baseline augmentation.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "tianStrikingRightBalance2022", "title": "Striking the Right Balance: Recall Loss for Semantic Segmentation", "abstract": "Class imbalance is a fundamental problem in computer vision applications such as semantic segmentation. Specifically, uneven class distributions in a training dataset often result in unsatisfactory performance on under-represented classes. Many works have proposed to weight the standard cross entropy loss function with pre-computed weights based on class statistics, such as the number of samples and class margins. There are two major drawbacks to these methods: 1) constantly up-weighting minority classes can introduce excessive false positives in semantic segmentation; 2) a minority class is not necessarily a hard class. The consequence is low precision due to excessive false positives. In this regard, we propose a hardclass mining loss by reshaping the vanilla cross entropy loss such that it weights the loss for each class dynamically based on instantaneous recall performance. We show that the novel recall loss changes gradually between the standard cross entropy loss and the inverse frequency weighted loss. Recall loss also leads to improved mean accuracy while offering competitive mean Intersection over Union (IoU) performance. On Synthia dataset1, recall loss achieves 9% relative improvement on mean accuracy with competitive mean IoU using DeepLab-ResNet18 compared to the cross entropy loss. Code available at https: //github.com/PotatoTian/recall-semseg.", "venue": "arXiv", "keywords": ["loss functions", "semantic segmentation"]}
{"id": "tjioAdversarialSemanticHallucination2022", "title": "Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation", "abstract": "", "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision", "keywords": ["adversarial learning", "auto-augmentation policies", "cnns", "domain generalization", "semantic segmentation"]}
{"id": "tjioDualStageStylization2023", "title": "Dual Stage Stylization Modulation for Domain Generalized Semantic Segmentation", "abstract": "Obtaining sufficient labeled data for training deep models is often challenging in real-life applications. To address this issue, we propose a novel solution for single-source domain generalized semantic segmentation. Recent approaches have explored data diversity enhancement using hallucination techniques. However, excessive hallucination can degrade performance, particularly for imbalanced datasets. As shown in our experiments, minority classes are more susceptible to performance reduction due to hallucination compared to majority classes. To tackle this challenge, we introduce a dual-stage Feature Transform (dF T ) layer within the Adversarial Semantic Hallucination+ (ASH+) framework. The ASH+ framework performs a dual-stage manipulation of hallucination strength. By leveraging semantic information for each pixel, our approach adaptively adjusts the pixel-wise hallucination strength, thus providing fine-grained control over hallucination. We validate the effectiveness of our proposed method through comprehensive experiments on publicly available semantic segmentation benchmark datasets (Cityscapes and SYNTHIA). Quantitative and qualitative comparisons demonstrate that our approach is competitive with state-of-the-art methods for the Cityscapes dataset and surpasses existing solutions for the SYNTHIA dataset. Code for our framework will be made readily available to the research community.", "venue": "arXiv", "keywords": ["adversarial learning", "domain generalization", "semantic segmentation", "siamese networks", "style transfer"]}
{"id": "toldoUnsupervisedDomainAdaptation2020", "title": "Unsupervised Domain Adaptation in Semantic Segmentation: A Review", "abstract": "The aim of this paper is to give an overview of the recent advancements in the Unsupervised Domain Adaptation (UDA) of deep networks for semantic segmentation. This task is attracting a wide interest since semantic segmentation models require a huge amount of labeled data and the lack of data fitting specific requirements is the main limitation in the deployment of these techniques. This field has been recently explored and has rapidly grown with a large number of ad-hoc approaches. This motivates us to build a comprehensive overview of the proposed methodologies and to provide a clear categorization. In this paper, we start by introducing the problem, its formulation and the various scenarios that can be considered. Then, we introduce the different levels at which adaptation strategies may be applied: namely, at the input (image) level, at the internal features representation and at the output level. Furthermore, we present a detailed overview of the literature in the field, dividing previous methods based on the following (non mutually exclusive) categories: adversarial learning, generative-based, analysis of the classifier discrepancies, self-teaching, entropy minimization, curriculum learning and multi-task learning. Novel research directions are also briefly introduced to give a hint of interesting open problems in the field. Finally, a comparison of the performance of the various methods in the widely used autonomous driving scenario is presented.", "venue": "Technologies", "keywords": ["model adaptation", "semantic segmentation", "sim-to-real", "surveys", "unsupervised da"]}
{"id": "toldoUnsupervisedDomainAdaptation2021", "title": "Unsupervised Domain Adaptation in Semantic Segmentation via Orthogonal and Clustered Embeddings", "abstract": "Deep learning frameworks allowed for a remarkable advancement in semantic segmentation, but the data hungry nature of convolutional networks has rapidly raised the demand for adaptation techniques able to transfer learned knowledge from label-abundant domains to unlabeled ones. In this paper we propose an effective Unsupervised Domain Adaptation (UDA) strategy, based on a feature clustering method that captures the different semantic modes of the feature distribution and groups features of the same class into tight and well-separated clusters. Furthermore, we introduce two novel learning objectives to enhance the discriminative clustering performance: an orthogonality loss forces spaced out individual representations to be orthogonal, while a sparsity loss reduces class-wise the number of active feature channels. The joint effect of these modules is to regularize the structure of the feature space. Extensive evaluations in the synthetic-to-real scenario show that we achieve state-of-the-art performance.", "venue": "2021 IEEE Winter Conference on Applications of Computer Vision (WACV)", "keywords": ["clustering", "feature engineering", "loss functions", "semantic segmentation", "sim-to-real", "unsupervised da"]}
{"id": "tolstikhinWassersteinAutoEncoders2019", "title": "Wasserstein Auto-Encoders", "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.", "venue": "arXiv", "keywords": []}
{"id": "tomarSelfSupervisedGenerativeStyle2022", "title": "Self-Supervised Generative Style Transfer for One-Shot Medical Image Segmentation", "abstract": "", "venue": "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)", "keywords": ["self-supervised learning", "style transfer"]}
{"id": "tomasiImageCorrelationConvolution", "title": "Image Correlation, Convolution and Filtering", "abstract": "", "venue": "", "keywords": []}
{"id": "TopologicalVectorSpace2024", "title": "Topological Vector Space", "abstract": "In mathematics, a topological vector space (also called a linear topological space and commonly abbreviated TVS or t.v.s.) is one of the basic structures investigated in functional analysis. A topological vector space is a vector space that is also a topological space with the property that the vector space operations (vector addition and scalar multiplication) are also continuous functions. Such a topology is called a vector topology and every topological vector space has a uniform topological structure, allowing a notion of uniform convergence and completeness. Some authors also require that the space is a Hausdorff space (although this article does not). One of the most widely studied categories of TVSs are locally convex topological vector spaces. This article focuses on TVSs that are not necessarily locally convex. Banach spaces, Hilbert spaces and Sobolev spaces are other well-known examples of TVSs. Many topological vector spaces are spaces of functions, or linear operators acting on topological vector spaces, and the topology is often defined so as to capture a particular notion of convergence of sequences of functions. In this article, the scalar field of a topological vector space will be assumed to be either the complex numbers C \\ \\ \\ or the real numbers R , \\ \\ ,\\ unless clearly stated otherwise.", "venue": "Wikipedia", "keywords": []}
{"id": "torralbaUnbiasedLookDataset2011", "title": "Unbiased Look at Dataset Bias", "abstract": "Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.", "venue": "CVPR 2011", "keywords": ["bias sources", "domain generalization", "surveys"]}
{"id": "tranMultiAugmentationEfficientVisual2022", "title": "Multi-Augmentation for Efficient Visual Representation Learning for Self-supervised Pre-training", "abstract": "In recent years, self-supervised learning has been studied to deal with the limitation of available labeled-dataset. Among the major components of self-supervised learning, the data augmentation pipeline is one key factor in enhancing the resulting performance. However, most researchers manually designed the augmentation pipeline, and the limited collections of transformation may cause the lack of robustness of the learned feature representation. In this work, we proposed Multi-Augmentations for Self-Supervised Representation Learning (MA-SSRL), which fully searched for various augmentation policies to build the entire pipeline to improve the robustness of the learned feature representation. MA-SSRL successfully learns the invariant feature representation and presents an efficient, effective, and adaptable data augmentation pipeline for self-supervised pre-training on different distribution and domain datasets. MA-SSRL outperforms the previous state-of-the-art methods on transfer and semi-supervised benchmarks while requiring fewer training epochs.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "representation learning", "self-supervised learning"]}
{"id": "trespFeatureSpacesManifolds", "title": "Feature Spaces, Manifolds, and Deep Generative Models", "abstract": "", "venue": "", "keywords": ["feature engineering", "manifold learning", "topological data analysis"]}
{"id": "trockmanOrthogonalizingConvolutionalLayers2021", "title": "Orthogonalizing Convolutional Layers with the Cayley Transform", "abstract": "Recent work has highlighted several advantages of enforcing orthogonality in the weight layers of deep networks, such as maintaining the stability of activations, preserving gradient norms, and enhancing adversarial robustness by enforcing low Lipschitz constants. Although numerous methods exist for enforcing the orthogonality of fully-connected layers, those for convolutional layers are more heuristic in nature, often focusing on penalty methods or limited classes of convolutions. In this work, we propose and evaluate an alternative approach to directly parameterize convolutional layers that are constrained to be orthogonal. Specifically, we propose to apply the Cayley transform to a skew-symmetric convolution in the Fourier domain, so that the inverse convolution needed by the Cayley transform can be computed efficiently. We compare our method to previous Lipschitz-constrained and orthogonal convolutional layers and show that it indeed preserves orthogonality to a high degree even for large convolutions. Applied to the problem of certified adversarial robustness, we show that networks incorporating the layer outperform existing deterministic methods for certified defense against \\ -norm-bounded adversaries, while scaling to larger architectures than previously investigated. Code is available at https://github.com/locuslab/orthogonal-convolutions.", "venue": "arXiv", "keywords": ["lipschitz-constraints"]}
{"id": "truongFastFlowReconstruction2022", "title": "Fast Flow Reconstruction via Robust Invertible Nxn Convolution", "abstract": "Flow-based generative models have recently become one of the most efficient approaches to model data generation. Indeed, they are constructed with a sequence of invertible and tractable transformations. Glow first introduced a simple type of generative flow using an invertible \\ convolution. However, the \\ convolution suffers from limited flexibility compared to the standard convolutions. In this paper, we propose a novel invertible \\ convolution approach that overcomes the limitations of the invertible \\ convolution. In addition, our proposed network is not only tractable and invertible but also uses fewer parameters than standard convolutions. The experiments on CIFAR-10, ImageNet and Celeb-HQ datasets, have shown that our invertible \\ convolution helps to improve the performance of generative models significantly.", "venue": "arXiv", "keywords": ["normalizing flows"]}
{"id": "tsaiLearningAdaptStructured2020", "title": "Learning to Adapt Structured Output Space for Semantic Segmentation", "abstract": "Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.", "venue": "arXiv", "keywords": ["adversarial learning", "domain adaptation", "semantic segmentation"]}
{"id": "tsaregorodtsevParticleAugmentSamplingBasedData2021", "title": "ParticleAugment: Sampling-Based Data Augmentation", "abstract": "We present an automated data augmentation approach for image classification. We formulate the problem as Monte Carlo sampling where our goal is to approximate the optimal augmentation policies. We propose a particle filtering formulation to find optimal augmentation policies and their schedules during model training. Our performance measurement procedure relies on a validation subset of our training set, while the policy transition model depends on a Gaussian prior and an optional augmentation velocity parameter. In our experiments, we show that our formulation for automated augmentation reaches promising results on CIFAR-10, CIFAR-100, and ImageNet datasets using the standard network architectures for this problem. By comparing with the related work, we also show that our method reaches a balance between the computational cost of policy search and the model performance.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "tsaregorodtsevParticleAugmentSamplingBasedData2021a", "title": "ParticleAugment: Sampling-Based Data Augmentation", "abstract": "We present an automated data augmentation approach for image classification. We formulate the problem as Monte Carlo sampling where our goal is to approximate the optimal augmentation policies. We propose a particle filtering scheme for the policy search where the probability of applying a set of augmentation operations forms the state of the filter. We measure the policy performance based on the loss function difference between a reference and the actual model, which we afterwards use to re-weight the particles and finally update the policy. In our experiments, we show that our formulation for automated augmentation reaches promising results on CIFAR-10, CIFAR-100, and ImageNet datasets using the standard network architectures for this problem. By comparing with the related work, our method reaches a balance between the computational cost of policy search and the model performance. Our code will be made publicly available.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "tsitsulinShapeDataIntrinsic2020", "title": "The Shape of Data: Intrinsic Distance for Data Distributions", "abstract": "The ability to represent and compare machine learning models is crucial in order to quantify subtle model changes, evaluate generative models, and gather insights on neural network architectures. Existing techniques for comparing data distributions focus on global data properties such as mean and covariance; in that sense, they are extrinsic and uni-scale. We develop a first-of-its-kind intrinsic and multi-scale method for characterizing and comparing data manifolds, using a lower-bound of the spectral Gromov-Wasserstein inter-manifold distance, which compares all data moments. In a thorough experimental study, we demonstrate that our method effectively discerns the structure of data manifolds even on unaligned data of different dimensionality, and showcase its efficacy in evaluating the quality of generative models.", "venue": "arXiv", "keywords": ["highly-analytical", "manifold learning"]}
{"id": "tuliAreConvolutionalNeural2021", "title": "Are Convolutional Neural Networks or Transformers More like Human Vision?", "abstract": "Modern machine learning models for computer vision exceed humans in accuracy on specific visual recognition tasks, notably on datasets like ImageNet. However, high accuracy can be achieved in many ways. The particular decision function found by a machine learning system is determined not only by the data to which the system is exposed, but also the inductive biases of the model, which are typically harder to characterize. In this work, we follow a recent trend of in-depth behavioral analyses of neural network models that go beyond accuracy as an evaluation metric by looking at patterns of errors. Our focus is on comparing a suite of standard Convolutional Neural Networks (CNNs) and a recently-proposed attention-based network, the Vision Transformer (ViT), which relaxes the translation-invariance constraint of CNNs and therefore represents a model with a weaker set of inductive biases. Attention-based networks have previously been shown to achieve higher accuracy than CNNs on vision tasks, and we demonstrate, using new metrics for examining error consistency with more granularity, that their errors are also more consistent with those of humans. These results have implications both for building more human-like vision models, as well as for understanding visual object recognition in humans.", "venue": "arXiv", "keywords": ["bias sources", "cnns", "critical citation", "human vision", "transformers"]}
{"id": "tzengAdversarialDiscriminativeDomain2017", "title": "Adversarial Discriminative Domain Adaptation", "abstract": "Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They also can improve recognition despite the presence of domain shift or dataset bias: several adversarial approaches to unsupervised domain adaptation have recently been introduced, which reduce the difference between the training and test domain distributions and thus improve generalization performance. Prior generative approaches show compelling visualizations, but are not optimal on discriminative tasks and can be limited to smaller shifts. Prior discriminative approaches could handle larger domain shifts, but imposed tied weights on the model and did not exploit a GAN-based loss. We first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and we use this generalized view to better relate the prior approaches. We propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard cross-domain digit classification tasks and a new more difficult cross-modality object classification task.", "venue": "arXiv", "keywords": ["adversarial learning", "domain adaptation"]}
{"id": "ullmanAtomsRecognitionHuman2016", "title": "Atoms of Recognition in Human and Computer Vision", "abstract": "Discovering the visual features and representations used by the brain to recognize objects is a central problem in the study of vision. Recently, neural network models of visual object recognition, including biological and deep network models, have shown remarkable progress and have begun to rival human performance in some challenging tasks. These models are trained on image examples and learn to extract features and representations and to use them for categorization. It remains unclear, however, whether the representations and learning processes discovered by current models are similar to those used by the human visual system. Here we show, by introducing and using minimal recognizable images, that the human visual system uses features and processes that are not used by current models and that are critical for recognition. We found by psychophysical studies that at the level of minimal recognizable images a minute change in the image can have a drastic effect on recognition, thus identifying features that are critical for the task. Simulations then showed that current models cannot explain this sensitivity to precise feature configurations and, more generally, do not learn to recognize minimal images at a human level. The role of the features shown here is revealed uniquely at the minimal level, where the contribution of each feature is essential. A full understanding of the learning and use of such features will extend our understanding of visual recognition and its cortical mechanisms and will enhance the capacity of computational models to learn from visual experience and to deal with recognition and detailed image interpretation.", "venue": "Proceedings of the National Academy of Sciences", "keywords": ["bias sources", "human vision", "representation learning"]}
{"id": "ulyanovImprovedTextureNetworks2017", "title": "Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis", "abstract": "The recent work of Gatys et al., who characterized the style of an image by the statistics of convolutional neural network filters, ignited a renewed interest in the texture generation and image stylization problems. While their image generation technique uses a slow optimization process, recently several authors have proposed to learn generator neural networks that can produce similar outputs in one quick forward pass. While generator networks are promising, they are still inferior in visual quality and diversity compared to generation-by-optimization. In this work, we advance them in two significant ways. First, we introduce an instance normalization module to replace batch normalization with significant improvements to the quality of image stylization. Second, we improve diversity by introducing a new learning formulation that encourages generators to sample unbiasedly from the Julesz texture ensemble, which is the equivalence class of all images characterized by certain filter responses. Together, these two improvements take feed forward texture synthesis and image stylization much closer to the quality of generation-via-optimization, while retaining the speed advantage.", "venue": "arXiv", "keywords": ["image synthesis", "style transfer", "texture transfer"]}
{"id": "ulyanovInstanceNormalizationMissing2017", "title": "Instance Normalization: The Missing Ingredient for Fast Stylization", "abstract": "It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture_nets. Full paper can be found at arXiv:1701.02096.", "venue": "arXiv", "keywords": ["foundational", "instance normalization", "style transfer"]}
{"id": "UnseenImageSynthesis2023", "title": "Unseen Image Synthesis with Diffusion Models", "abstract": "While the current trend in the generative field is scaling up towards larger models and more training data for generalized domain representations, we go the opposite direction in this work by synthesizing unseen domain images without additional training. We do so via latent sampling and geometric optimization using pre-trained and frozen Denoising Diffusion Probabilistic Models (DDPMs) on single-domain datasets. Our key observation is that DDPMs pre-trained even just on single-domain images are already equipped with sufficient representation abilities to reconstruct arbitrary images from the inverted latent encoding following bi-directional deterministic diffusion and denoising trajectories. This motivates us to investigate the statistical and geometric behaviors of the Out-Of-Distribution (OOD) samples from unseen image domains in the latent spaces along the denoising chain. Notably, we theoretically and empirically show that the inverted OOD samples also establish Gaussians that are distinguishable from the original In-Domain (ID) samples in the intermediate latent spaces, which allows us to sample from them directly. Geometrical domain-specific information of the unseen subspace (e.g., sample-wise distance and angles) is used to further optimize the sampled OOD latent encodings from the estimated Gaussian prior. We conduct extensive analysis and experiments using pre-trained diffusion models (DDPM, iDDPM) on different datasets (AFHQ, CelebA-HQ, LSUN-Church, and LSUN-Bedroom), proving the effectiveness of this novel perspective to explore and re-think the diffusion models' data synthesis generalization ability.", "venue": "", "keywords": ["diffusion models", "image synthesis"]}
{"id": "uricarChallengesDesigningDatasets2019", "title": "Challenges in Designing Datasets and Validation for Autonomous Driving", "abstract": "Autonomous driving is getting a lot of attention in the last decade and will be the hot topic at least until the first successful certification of a car with Level 5 autonomy. There are many public datasets in the academic community. However, they are far away from what a robust industrial production system needs. There is a large gap between academic and industrial setting and a substantial way from a research prototype, built on public datasets, to a deployable solution which is a challenging task. In this paper, we focus on bad practices that often happen in the autonomous driving from an industrial deployment perspective. Data design deserves at least the same amount of attention as the model design. There is very little attention paid to these issues in the scientific community, and we hope this paper encourages better formalization of dataset design. More specifically, we focus on the datasets design and validation scheme for autonomous driving, where we would like to highlight the common problems, wrong assumptions, and steps towards avoiding them, as well as some open problems.", "venue": "arXiv", "keywords": ["surveys"]}
{"id": "uricarDesoilingDatasetRestoring2019", "title": "Desoiling Dataset: Restoring Soiled Areas on Automotive Fisheye Cameras", "abstract": "Surround-view cameras became an integral part of autonomous driving setup. Being directly exposed to harsh environmental settings, they can get soiled easily. When cameras get soiled, the degradation of performance is usually more dramatic compared to other sensors. Having this on mind, we decided to design a dataset for measuring the performance degradation as well as to help constructing classifiers for soiling detection, or for trying to restore the soiled images, so we can increase the performance of the off-the-shelf classifiers. The proposed dataset contains 40+ approximately 1 minute long video sequences with paired image information of both clean and soiled nature. The dataset will be released as a companion to our recently published dataset to encourage further research in this area. We constructed a CycleGAN architecture to produce de-soiled images and demonstrate 5% improvement in road detection and 3% improvement in detection of lanes and curbs.", "venue": "2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)", "keywords": ["automotive occlusion", "fisheye distortion"]}
{"id": "uricarLetGetDirty2020", "title": "Let's Get Dirty: GAN Based Data Augmentation for Camera Lens Soiling Detection in Autonomous Driving", "abstract": "Wide-angle fisheye cameras are commonly used in automated driving for parking and low-speed navigation tasks. Four of such cameras form a surround-view system that provides a complete and detailed view of the vehicle. These cameras are directly exposed to harsh environmental settings and can get soiled very easily by mud, dust, water, frost. Soiling on the camera lens can severely degrade the visual perception algorithms, and a camera cleaning system triggered by a soiling detection algorithm is increasingly being deployed. While adverse weather conditions, such as rain, are getting attention recently, there is only limited work on general soiling. The main reason is the difficulty in collecting a diverse dataset as it is a relatively rare event.", "venue": "arXiv", "keywords": ["automotive occlusion", "dataset debut", "gans"]}
{"id": "uricarSoilingNetSoilingDetection2019", "title": "SoilingNet: Soiling Detection on Automotive Surround-View Cameras", "abstract": "Cameras are an essential part of sensor suite in autonomous driving. Surround-view cameras are directly exposed to external environment and are vulnerable to get soiled. Cameras have a much higher degradation in performance due to soiling compared to other sensors. Thus it is critical to accurately detect soiling on the cameras, particularly for higher levels of autonomous driving. We created a new dataset having multiple types of soiling namely opaque and transparent. As there is no public dataset available for this task, we will release a public dataset to encourage further research. We demonstrate high accuracy using a Convolutional Neural Network (CNN) based architecture. We also show that it can be combined with the existing object detection task in a multitask learning framework. Finally, we make use of Generative Adversarial Networks (GANs) to generate more images for data augmentation and show that it works successfully similar to the style transfer.", "venue": "2019 IEEE Intelligent Transportation Systems Conference (ITSC)", "keywords": ["automotive occlusion"]}
{"id": "uricarVisibilityNetCameraVisibility2020", "title": "VisibilityNet: Camera Visibility Detection and Image Restoration for Autonomous Driving", "abstract": "Cameras sensors are crucial for autonomous driving as they are the only sensing modality that provide measured color information of the surrounding scene. Cameras are directly exposed to external weather conditions where visibility is dramatically affected due to various reasons such as rain, ice, fog, soil, ..etc. Hence, it is crucial to detect and remove the visibility degradation caused by the harsh weather conditions. In this paper, we focus mainly on soiling degradation. We provide methods for classification of the soiled parts as well as methods for estimating the scene behind the soiled parts. A new dataset is created providing manually annotated soiled masks knows as WoodScape dataset to encourage research in that area.", "venue": "Electronic Imaging", "keywords": ["automotive occlusion"]}
{"id": "uricarVisibilityNetCameraVisibility2020a", "title": "VisibilityNet: Camera Visibility Detection and Image Restoration for Autonomous Driving", "abstract": "Camera sensors are crucial for autonomous driving as they are the only sensing modality that provide measured color information of the surrounding scene. Cameras are directly exposed to external weather conditions where visibility is dramatically affected due to various reasons such as rain, ice, fog, soil, ..etc. Hence, it is crucial to detect and remove the visibility degradation caused by the harsh weather conditions. In this paper, we focus mainly on soiling degradation. We provide methods for classification of the soiled parts as well as methods for estimating the scene behind the soiled parts. A new dataset is created providing manually annotated soiled masks knows as WoodScape dataset to encourage research in that area.", "venue": "", "keywords": []}
{"id": "uricarYesWeGAN2019", "title": "Yes, We GAN: Applying Adversarial Techniques for Autonomous Driving", "abstract": "Generative Adversarial Networks (GAN) have gained a lot of popularity from their introduction in 2014 till present. Research on GAN is rapidly growing and there are many variants of the original GAN focusing on various aspects of deep learning. GAN are perceived as the most impactful direction of machine learning in the last decade. This paper focuses on the application of GAN in autonomous driving including topics such as advanced data augmentation, loss function learning, semi-supervised learning, etc. We formalize and review key applications of adversarial techniques and discuss challenges and open problems to be addressed.", "venue": "Electronic Imaging", "keywords": ["adversarial learning", "dataset debut", "gans", "generative augmentation"]}
{"id": "UVQMeasuringYouTubes", "title": "UVQ: Measuring YouTube's Perceptual Video Quality", "abstract": "Posted by Yilin Wang, Staff Software Engineer, YouTube and Feng Yang, Senior Staff Software Engineer, Google Research Update --- 2022/11/08: This p...", "venue": "", "keywords": []}
{"id": "vakanskiEVALUATIONCOMPLEXITYMEASURES2021", "title": "EVALUATION OF COMPLEXITY MEASURES FOR DEEP LEARNING GENERALIZATION IN MEDICAL IMAGE ANALYSIS", "abstract": "The generalization error of deep learning models for medical image analysis often increases on images collected with different devices for data acquisition, device settings, or patient population. A better understanding of the generalization capacity on new images is crucial for clinicians' trustworthiness. Although significant efforts have been recently directed toward establishing generalization bounds and complexity measures, there is still a significant discrepancy between the predicted and actual generalization performance. As well, related large empirical studies have been primarily based on validation with general-purpose image datasets. This paper presents an empirical study that investigates the correlation between 25 complexity measures and the generalization abilities of deep learning classifiers for breast ultrasound images. The results indicate that PAC-Bayes flatness and path norm measures produce the most consistent explanation for the combination of models and data. We also report that multi-task classification and segmentation approach for breast images is conducive toward improved generalization.", "venue": "IEEE International Workshop on Machine Learning for Signal Processing : [proceedings]. IEEE International Workshop on Machine Learning for Signal Processing", "keywords": ["generalization quantification", "measure theory", "surveys"]}
{"id": "vandermeulenRobustKernelDensity2014", "title": "Robust Kernel Density Estimation by Scaling and Projection in Hilbert Space", "abstract": "", "venue": "Advances in Neural Information Processing Systems", "keywords": []}
{"id": "vandermeulenRobustKernelDensity2014a", "title": "Robust Kernel Density Estimation by Scaling and Projection in Hilbert Space", "abstract": "", "venue": "Advances in Neural Information Processing Systems", "keywords": []}
{"id": "vandersluijsExploringImageAugmentations2023", "title": "Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays", "abstract": "Image augmentations are quintessential for effective visual representation learning across self-supervised learning techniques. While augmentation strategies for natural imaging have been studied extensively, medical images are vastly different from their natural counterparts. Thus, it is unknown whether common augmentation strategies employed in Siamese representation learning generalize to medical images and to what extent. To address this challenge, in this study, we systematically assess the effect of various augmentations on the quality and robustness of the learned representations. We train and evaluate Siamese Networks for abnormality detection on chest X-Rays across three large datasets (MIMIC-CXR, CheXpert and VinDR-CXR). We investigate the efficacy of the learned representations through experiments involving linear probing, fine-tuning, zero-shot transfer, and data efficiency. Finally, we identify a set of augmentations that yield robust representations that generalize well to both out-of-distribution data and diseases, while outperforming supervised baselines using just zero-shot transfer and linear probes by up to 20%. Our code is available at https://github.com/StanfordMIMI/siaug.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "self-supervised learning", "siamese networks"]}
{"id": "vangansbekeDiscoveringObjectMasks2022", "title": "Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation", "abstract": "The task of unsupervised semantic segmentation aims to cluster pixels into semantically meaningful groups. Specifically, pixels assigned to the same cluster should share high-level semantic properties like their object or part category. This paper presents MaskDistill: a novel framework for unsupervised semantic segmentation based on three key ideas. First, we advocate a data-driven strategy to generate object masks that serve as a pixel grouping prior for semantic segmentation. This approach omits handcrafted priors, which are often designed for specific scene compositions and limit the applicability of competing frameworks. Second, MaskDistill clusters the object masks to obtain pseudo-ground-truth for training an initial object segmentation model. Third, we leverage this model to filter out low-quality object masks. This strategy mitigates the noise in our pixel grouping prior and results in a clean collection of masks which we use to train a final segmentation model. By combining these components, we can considerably outperform previous works for unsupervised semantic segmentation on PASCAL (+11% mIoU) and COCO (+4% mask AP50). Interestingly, as opposed to existing approaches, our framework does not latch onto low-level image cues and is not limited to object-centric datasets. The code and models will be made available.", "venue": "arXiv", "keywords": []}
{"id": "vankadariWhenSunGoes", "title": "When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation", "abstract": "Self-supervised deep learning methods for joint depth and ego-motion estimation can yield accurate trajectories without needing ground-truth training data. However, as they typically use photometric losses, their performance can degrade significantly when the assumptions these losses make (e.g. temporal illumination consistency, a static scene, and the absence of noise and occlusions) are violated. This limits their use for e.g. nighttime sequences, which tend to contain many point light sources (including on dynamic objects) and low signal-to-noise ratio (SNR) in darker image regions. In this paper, we show how to use a combination of three techniques to allow the existing photometric losses to work for both day and nighttime images. First, we introduce a per-pixel neural intensity transformation to compensate for the light changes that occur between successive frames. Second, we predict a per-pixel residual flow map that we use to correct the reprojection correspondences induced by the estimated ego-motion and depth from the networks. And third, we denoise the training images to improve the robustness and accuracy of our approach. These changes allow us to train a single model for both day and nighttime images without needing separate encoders or extra feature networks like existing methods. We perform extensive experiments and ablation studies on the challenging Oxford RobotCar dataset to demonstrate the efficacy of our approach for both day and nighttime sequences.", "venue": "", "keywords": ["depth estimation"]}
{"id": "vayerControllingWassersteinDistances", "title": "Controlling Wasserstein Distances by Kernel Norms with Application to Compressive Statistical Learning", "abstract": "Comparing probability distributions is at the crux of many machine learning algorithms. Maximum Mean Discrepancies (MMD) and Wasserstein distances are two classes of distances between probability distributions that have attracted abundant attention in past years. This paper establishes some conditions under which the Wasserstein distance can be controlled by MMD norms. Our work is motivated by the compressive statistical learning (CSL) theory, a general framework for resource-efficient large scale learning in which the training data is summarized in a single vector (called sketch) that captures the information relevant to the considered learning task. Inspired by existing results in CSL, we introduce the Ho Lower Restricted Isometric Property and show that this property comes with interesting guarantees for compressive statistical learning. Based on the relations between the MMD and the Wasserstein distances, we provide guarantees for compressive statistical learning by introducing and studying the concept of Wasserstein regularity of the learning task, that is when some task-specific metric between probability distributions can be bounded by a Wasserstein distance.", "venue": "", "keywords": ["generalization certification", "highly-analytical", "kernel methods"]}
{"id": "verbraekenSurveyDistributedMachine2021", "title": "A Survey on Distributed Machine Learning", "abstract": "The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.", "venue": "ACM Computing Surveys", "keywords": []}
{"id": "verineExpressivityBiLipschitzNormalizing2023", "title": "On the Expressivity of Bi-Lipschitz Normalizing Flows", "abstract": "An invertible function is bi-Lipschitz if both the function and its inverse have bounded Lipschitz constants. Most state-of-the-art Normalizing Flows are bi-Lipschitz by design or by training to limit numerical errors (among other things). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing Flows and identify several target distributions that are difficult to approximate using such models. Then, we characterize the expressivity of bi-Lipschitz Normalizing Flows by giving several lower bounds on the Total Variation distance between these particularly unfavorable distributions and their best possible approximation. Finally, we show how to use the bounds to adjust the training parameters, and discuss potential remedies.", "venue": "Proceedings of The 14th Asian Conference on Machine Learning", "keywords": ["lipschitz-constraints"]}
{"id": "vermaSARCSoftActor2023", "title": "SARC: Soft Actor Retrospective Critic", "abstract": "The two-time scale nature of SAC, which is an actor-critic algorithm, is characterised by the fact that the critic estimate has not converged for the actor at any given time, but since the critic learns faster than the actor, it ensures eventual consistency between the two. Various strategies have been introduced in literature to learn better gradient estimates to help achieve better convergence. Since gradient estimates depend upon the critic, we posit that improving the critic can provide a better gradient estimate for the actor at each time. Utilizing this, we propose Soft Actor Retrospective Critic (SARC), where we augment the SAC critic loss with another loss term - retrospective loss - leading to faster critic convergence and consequently, better policy gradient estimates for the actor. An existing implementation of SAC can be easily adapted to SARC with minimal modifications. Through extensive experimentation and analysis, we show that SARC provides consistent improvement over SAC on benchmark environments. We plan to open-source the code and all experiment data at: https://github.com/sukritiverma1996/SARC.", "venue": "arXiv", "keywords": ["actor-critic", "reinforcement learning"]}
{"id": "vialardElementaryIntroductionEntropic", "title": "An Elementary Introduction to Entropic Regularization and Proximal Methods for Numerical Optimal Transport", "abstract": "These notes contains the material that I presented to the CEA-EDF-INRIA summer school about numerical optimal transport. These notes are, on purpose, written at an elementary level, with almost no prerequisite knowledge and the writing style is relatively informal. All the methods presented hereafter rely on convex optimization, so we start with a fairly basic introduction to convex analysis and optimization. Then, we present the entropic regularization of the Kantorovich formulation and present the now well known Sinkhorn algorithm, whose convergence is proven in continuous setting with a simple proof. We prove the linear convergence rate of this algorithm with respect to the Hilbert metric. The second numerical method we present use the dynamical formulation of optimal transport proposed by Benamou and Brenier which is solvable via non-smooth convex optimization methods. We end this short course with an overview of other dynamical formulations of optimal transport like problems.", "venue": "", "keywords": ["entropic optimization", "optimal transport"]}
{"id": "viallardLeveragingPACBayesTheory2024", "title": "Leveraging PAC-Bayes Theory and Gibbs Distributions for Generalization Bounds with Complexity Measures", "abstract": "In statistical learning theory, a generalization bound usually involves a complexity measure imposed by the considered theoretical framework. This limits the scope of such bounds, as other forms of capacity measures or regularizations are used in algorithms. In this paper, we leverage the framework of disintegrated PAC-Bayes bounds to derive a general generalization bound instantiable with arbitrary complexity measures. One trick to prove such a result involves considering a commonly used family of distributions: the Gibbs distributions. Our bound stands in probability jointly over the hypothesis and the learning sample, which allows the complexity to be adapted to the generalization gap as it can be customized to fit both the hypothesis class and the task.", "venue": "arXiv", "keywords": ["domain generalization", "generalization certification", "measure theory", "pac learning"]}
{"id": "virmauxLipschitzRegularityDeep2018", "title": "Lipschitz Regularity of Deep Neural Networks: Analysis and Efficient Estimation", "abstract": "Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications. In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures. First, we show that, even for two layer neural networks, the exact computation of this quantity is NP-hard and state-of-art methods may significantly overestimate it. Then, we both extend and improve previous estimation methods by providing AutoLip, the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function. We provide a power method algorithm working with automatic differentiation, allowing efficient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named SeqLip that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on SeqLip in order to tackle very large networks. Our experiments show that SeqLip can significantly improve on the existing upper bounds. Finally, we provide an implementation of AutoLip in the PyTorch environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations. These results also hint at the difficulty to estimate the Lipschitz constant of deep networks.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["lipschitz-constraints"]}
{"id": "volpiGeneralizingUnseenDomains2018", "title": "Generalizing to Unseen Domains via Adversarial Data Augmentation", "abstract": "We are concerned with learning models that generalize well to different \\ domains. We consider a worst-case formulation over data distributions that are near the source domain in the feature space. Only using training data from a single source distribution, we propose an iterative procedure that augments the dataset with examples from a fictitious target domain that is \"hard\" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses, we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g., ridge or lasso). On digit recognition and semantic segmentation tasks, our method learns models improve performance across a range of a priori unknown target domains.", "venue": "arXiv", "keywords": ["adversarial learning", "curriculum learning", "domain generalization", "gans", "generative augmentation"]}
{"id": "vonkugelgenSelfSupervisedLearningData2022", "title": "Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style", "abstract": "Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.", "venue": "arXiv", "keywords": ["self-supervised learning", "style transfer"]}
{"id": "vsOnlineDomainAdaptive2023", "title": "Towards Online Domain Adaptive Object Detection", "abstract": "", "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision", "keywords": ["contrastive learning", "distribution estimation", "domain adaptation", "online da", "transformers"]}
{"id": "vuADVENTAdversarialEntropy2019", "title": "ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation", "abstract": "Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) an entropy loss and (ii) an adversarial loss respectively. We demonstrate state-of-theart performance in semantic segmentation on two challenging ``synthetic-2-real'' set-ups1 and show that the approach can also be used for detection.", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["domain adaptation", "entropic optimization", "gans", "semantic segmentation", "sim-to-real"]}
{"id": "waldCalibrationOutofDomainGeneralization2021", "title": "On Calibration and Out-of-Domain Generalization", "abstract": "Out-of-domain (OOD) generalization is a significant challenge for machine learning models. Many techniques have been proposed to overcome this challenge, often focused on learning models with certain invariance properties. In this work, we draw a link between OOD performance and model calibration, arguing that calibration across multiple domains can be viewed as a special case of an invariant representation leading to better OOD generalization. Specifically, we show that under certain conditions, models which achieve -domain calibration\\ are provably free of spurious correlations. This leads us to propose multi-domain calibration as a measurable and trainable surrogate for the OOD performance of a classifier. We therefore introduce methods that are easy to apply and allow practitioners to improve multi-domain calibration by training or modifying an existing model, leading to better performance on unseen domains. Using four datasets from the recently proposed WILDS OOD benchmark, as well as the Colored MNIST, we demonstrate that training or tuning models so they are calibrated across multiple domains leads to significantly improved performance on unseen test domains. We believe this intriguing connection between calibration and OOD generalization is promising from both a practical and theoretical point of view.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["domain generalization", "generalization certification", "generalization quantification", "loss functions"]}
{"id": "waltonIsomorphismNormalizingFlows", "title": "Isomorphism, Normalizing Flows, and Density Estimation:", "abstract": "Normalizing Flows are a powerful type of generative model that transforms an intractable distribution of data into a more desirable one through the use of bijective functions. Their concept is simple to understand and create powerful models that allow one to work with much simpler distributions than that of the underlying data. In essence, our motivation in studying Normalizing Flows is to preserve relationships between data. Normalizing Flows have a wide variety of uses and applications, providing important statistical information about data as well as enabling more interpretable control over latent structures. They can be used for mathematical applications, such as: variational inference, density estimation, anomoly detection, manifold analysis; as well as more application focused works, such as: pose estimation, speach generation, image generation, and more. An important aspect of Normalizing Flows is that they preserve the latent structure of the data that they are trained upon, which makes gives them to the power to perform the aforementioned tasks. In this review we provide an introduction to Normalizing Flows, clarifying how they differ from other popular generative models, provide an updated overview of the current literature, discuss their applications, as well as the future of these models and how they can play a critical role in AI research. We also aim to clarify the distinction between different generative models in a more clear and precise way than many other works. We aim to make this work a sufficient introduction to Normalizing Flows and be self-contained with little to no background required.", "venue": "", "keywords": ["highly-analytical", "lipschitz-constraints", "normalizing flows"]}
{"id": "waltScikitimageImageProcessing2014", "title": "Scikit-Image: Image Processing in Python", "abstract": "scikit-image is an image processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal \"Modified BSD\" open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-image library, and we showcase several real-world image processing applications that use scikit-image.", "venue": "PeerJ", "keywords": []}
{"id": "wangAdaptingImagebasedRL2024", "title": "Adapting Image-based RL Policies via Predicted Rewards", "abstract": "Image-based reinforcement learning (RL) faces significant challenges in generalization when the visual environment undergoes substantial changes between training and deployment. Under such circumstances, learned policies may not perform well leading to degraded results. Previous approaches to this problem have largely focused on broadening the training observation distribution, employing techniques like data augmentation and domain randomization. However, given the sequential nature of the RL decision-making problem, it is often the case that residual errors are propagated by the learned policy model and accumulate throughout the trajectory, resulting in highly degraded performance. In this paper, we leverage the observation that predicted rewards under domain shift, even though imperfect, can still be a useful signal to guide fine-tuning. We exploit this property to fine-tune a policy using reward prediction in the target domain. We have found that, even under significant domain shift, the predicted reward can still provide meaningful signal and fine-tuning substantially improves the original policy. Our approach, termed Predicted Reward Fine-tuning (PRFT), improves performance across diverse tasks in both simulated benchmarks and real-world experiments. More information is available at project web page: https://sites.google.com/view/prft.", "venue": "arXiv", "keywords": ["reinforcement learning"]}
{"id": "wangAdaptivelySharingMultilevels2022", "title": "Adaptively Sharing Multi-Levels of Distributed Representations in Multi-Task Learning", "abstract": "In multi-task learning, the performance is often sensitive to the relationships between tasks. Thus it is important to study how to exploit the complex relationships across different tasks. One line of research captures the complex task relationships, by increasing the model capacity and thus requiring a large training dataset. However in many real-world applications, the amount of labeled data is limited. In this paper, we propose a light weight and specially designed architecture, which aims to model task relationships for small or middle-sized datasets. The proposed framework learns a task-specific ensemble of sub-networks in different depths, and is able to adapt the model architecture for the given data. The task-specific ensemble parameters are learned simultaneously with the weights of the network by optimizing a single loss function defined with respect to the end task. The hierarchical model structure is able to share both general and specific distributed representations to capture the inherent relationships between tasks. We validate our approach on various types of tasks, including synthetic task, article recommendation task and vision task. The results demonstrate the advantages of our model over several competitive baselines especially when the tasks are less-related.", "venue": "Information Sciences", "keywords": ["adaptive learning", "multi-task learning"]}
{"id": "wangAugMaxAdversarialComposition2022", "title": "AugMax: Adversarial Composition of Random Augmentations for Robust Training", "abstract": "Data augmentation is a simple yet effective way to improve the robustness of deep neural networks (DNNs). Diversity and hardness are two complementary dimensions of data augmentation to achieve robustness. For example, AugMix explores random compositions of a diverse set of augmentations to enhance broader coverage, while adversarial training generates adversarially hard samples to spot the weakness. Motivated by this, we propose a data augmentation framework, termed AugMax, to unify the two aspects of diversity and hardness. AugMax first randomly samples multiple augmentation operators and then learns an adversarial mixture of the selected operators. Being a stronger form of data augmentation, AugMax leads to a significantly augmented input distribution which makes model training more challenging. To solve this problem, we further design a disentangled normalization module, termed DuBIN (Dual-Batch-and-Instance Normalization), that disentangles the instance-wise feature heterogeneity arising from AugMax. Experiments show that AugMax-DuBIN leads to significantly improved out-of-distribution robustness, outperforming prior arts by 3.03%, 3.49%, 1.82% and 0.71% on CIFAR10-C, CIFAR100-C, Tiny ImageNet-C and ImageNet-C. Codes and pretrained models are available: https://github.com/VITA-Group/AugMax.", "venue": "arXiv", "keywords": ["adversarial learning", "auto-augmentation policies", "domain randomization", "gans", "generative augmentation", "instance normalization"]}
{"id": "wangBalancingLogitVariation2023", "title": "Balancing Logit Variation for Long-Tailed Semantic Segmentation", "abstract": "", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "keywords": ["distribution estimation", "domain generalization", "feature engineering"]}
{"id": "wangClassesMatterFinegrained2020", "title": "Classes Matter: A Fine-grained Adversarial Approach to Cross-domain Semantic Segmentation", "abstract": "Despite great progress in supervised semantic segmentation,a large performance drop is usually observed when deploying the model in the wild. Domain adaptation methods tackle the issue by aligning the source domain and the target domain. However, most existing methods attempt to perform the alignment from a holistic view, ignoring the underlying class-level data structure in the target domain. To fully exploit the supervision in the source domain, we propose a fine-grained adversarial learning strategy for class-level feature alignment while preserving the internal structure of semantics across domains. We adopt a fine-grained domain discriminator that not only plays as a domain distinguisher, but also differentiates domains at class level. The traditional binary domain labels are also generalized to domain encodings as the supervision signal to guide the fine-grained feature alignment. An analysis with Class Center Distance (CCD) validates that our fine-grained adversarial strategy achieves better class-level alignment compared to other state-of-the-art methods. Our method is easy to implement and its effectiveness is evaluated on three classical domain adaptation tasks, i.e., GTA5 to Cityscapes, SYNTHIA to Cityscapes and Cityscapes to Cross-City. Large performance gains show that our method outperforms other global feature alignment based and class-wise alignment based counterparts. The code is publicly available at https://github.com/JDAI-CV/FADA.", "venue": "arXiv.org", "keywords": ["adversarial learning", "class balancing", "domain projection", "gans", "semantic segmentation"]}
{"id": "wangCollaborativeDistillationUltraResolution2020", "title": "Collaborative Distillation for Ultra-Resolution Universal Style Transfer", "abstract": "Universal style transfer methods typically leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its application is heavily constrained by the large model size to handle ultra-resolution images given limited memory. In this work, we present a new knowledge distillation method (named Collaborative Distillation) for encoder-decoder based neural style transfer to reduce the convolutional filters. The main idea is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for style transfer models. Moreover, to overcome the feature size mismatch when applying collaborative distillation, a linear embedding loss is introduced to drive the student network to learn a linear embedding of the teacher's features. Extensive experiments show the effectiveness of our method when applied to different universal style transfer approaches (WCT and AdaIN), even if the model size is reduced by 15.5 times. Especially, on WCT with the compressed models, we achieve ultra-resolution (over 40 megapixels) universal style transfer on a 12GB GPU for the first time. Further experiments on optimization-based stylization scheme show the generality of our algorithm on different stylization paradigms. Our code and trained models are available at https://github.com/mingsun-tse/collaborative-distillation.", "venue": "arXiv", "keywords": ["collaborative learning", "knowledge distillation", "style transfer", "super-resolution"]}
{"id": "wangComprehensiveSurveyLoss2022", "title": "A Comprehensive Survey of Loss Functions in Machine Learning", "abstract": "As one of the important research topics in machine learning, loss function plays an important role in the construction of machine learning algorithms and the improvement of their performance, which has been concerned and explored by many researchers. But it still has a big gap to summarize, analyze and compare the classical loss functions. Therefore, this paper summarizes and analyzes 31 classical loss functions in machine learning. Specifically, we describe the loss functions from the aspects of traditional machine learning and deep learning respectively. The former is divided into classification problem, regression problem and unsupervised learning according to the task type. The latter is subdivided according to the application scenario, and here we mainly select object detection and face recognition to introduces their loss functions. In each task or application, in addition to analyzing each loss function from formula, meaning, image and algorithm, the loss functions under the same task or application are also summarized and compared to deepen the understanding and provide help for the selection and improvement of loss function.", "venue": "Annals of Data Science", "keywords": ["loss functions", "surveys"]}
{"id": "wangDAASDifferentiableArchitecture2022", "title": "DAAS: Differentiable Architecture and Augmentation Policy Search", "abstract": "Neural architecture search (NAS) has been an active direction of automatic machine learning (Auto-ML), aiming to explore efficient network structures. The searched architecture is evaluated by training on datasets with fixed data augmentation policies. However, recent works on auto-augmentation show that the suited augmentation policies can vary over different structures. Therefore, this work considers the possible coupling between neural architectures and data augmentation and proposes an effective algorithm jointly searching for them. Specifically, 1) for the NAS task, we adopt a single-path based differentiable method with Gumbel-softmax reparameterization strategy due to its memory efficiency; 2) for the auto-augmentation task, we introduce a novel search method based on policy gradient algorithm, which can significantly reduce the computation complexity. Our approach achieves 97.91% accuracy on CIFAR-10 and 76.6% Top-1 accuracy on ImageNet dataset, showing the outstanding performance of our search algorithm.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "neural architecture search"]}
{"id": "wangDeepLearningMethod2023", "title": "A Deep Learning Method for Optimizing Semantic Segmentation Accuracy of Remote Sensing Images Based on Improved UNet", "abstract": "Abstract Semantic segmentation of remote sensing imagery (RSI) is critical in many domains due to the diverse landscapes and different sizes of geo-objects that RSI contains, making semantic segmentation challenging. In this paper, a convolutional network, named Adaptive Feature Fusion UNet (AFF-UNet), is proposed to optimize the semantic segmentation performance. The model has three key aspects: (1) dense skip connections architecture and an adaptive feature fusion module that adaptively weighs different levels of feature maps to achieve adaptive feature fusion, (2) a channel attention convolution block that obtains the relationship between different channels using a tailored configuration, and (3) a spatial attention module that obtains the relationship between different positions. AFF-UNet was evaluated on two public RSI datasets and was quantitatively and qualitatively compared with other models. Results from the Potsdam dataset showed that the proposed model achieved an increase of 1.09% over DeepLabv3\\,+\\,in terms of the average F1 score and a 0.99% improvement in overall accuracy. The visual qualitative results also demonstrated a reduction in confusion of object classes, better performance in segmenting different sizes of object classes, and better object integrity. Therefore, the proposed AFF-UNet model optimizes the accuracy of RSI semantic segmentation.", "venue": "Scientific Reports", "keywords": ["semantic segmentation"]}
{"id": "wangDeepReinforcementLearning2023", "title": "Deep Reinforcement Learning Enables Adaptive-Image Augmentation for Automated Optical Inspection of Plant Rust", "abstract": "p This study proposes an adaptive image augmentation scheme using deep reinforcement learning (DRL) to improve the performance of a deep learning-based automated optical inspection system. The study addresses the challenge of inconsistency in the performance of single image augmentation methods. It introduces a DRL algorithm, DQN, to select the most suitable augmentation method for each image. The proposed approach extracts geometric and pixel indicators to form states, and uses DeepLab-v3+ model to verify the augmented images and generate rewards. Image augmentation methods are treated as actions, and the DQN algorithm selects the best methods based on the images and segmentation model. The study demonstrates that the proposed framework outperforms any single image augmentation method and achieves better segmentation performance than other semantic segmentation models. The framework has practical implications for developing more accurate and robust automated optical inspection systems, critical for ensuring product quality in various industries. Future research can explore the generalizability and scalability of the proposed framework to other domains and applications. The code for this application is uploaded at ext-link ext-link-type=\"uri\" xlink:href=\"https://github.com/lynnkobe/Adaptive-Image-Augmentation.git\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" https://github.com/lynnkobe/Adaptive-Image-Augmentation.git /ext-link . /p", "venue": "Frontiers in Plant Science", "keywords": []}
{"id": "wangDIBRSynthesizedImageQuality2021", "title": "DIBR-Synthesized Image Quality Assessment With Texture and Depth Information", "abstract": "p Accurately predicting the quality of depth-image-based-rendering (DIBR) synthesized images is of great significance in promoting DIBR techniques. Recently, many DIBR-synthesized image quality assessment (IQA) algorithms have been proposed to quantify the distortion that existed in texture images. However, these methods ignore the damage of DIBR algorithms on the depth structure of DIBR-synthesized images and thus fail to accurately evaluate the visual quality of DIBR-synthesized images. To this end, this paper presents a DIBR-synthesized image quality assessment metric with Texture and Depth Information, dubbed as TDI. TDI predicts the quality of DIBR-synthesized images by jointly measuring the synthesized image's colorfulness, texture structure, and depth structure. The design principle of our TDI includes two points: (1) DIBR technologies bring color deviation to DIBR-synthesized images, and so measuring colorfulness can effectively predict the quality of DIBR-synthesized images. (2) In the hole-filling process, DIBR technologies introduce the local geometric distortion, which destroys the texture structure of DIBR-synthesized images and affects the relationship between the foreground and background of DIBR-synthesized images. Thus, we can accurately evaluate DIBR-synthesized image quality through a joint representation of texture and depth structures. Experiments show that our TDI outperforms the competing state-of-the-art algorithms in predicting the visual quality of DIBR-synthesized images. /p", "venue": "Frontiers in Neuroscience", "keywords": ["depth estimation", "image quality assessment", "texture transfer"]}
{"id": "wangDistributedMultiTaskLearning2016", "title": "Distributed Multi-Task Learning with Shared Representation", "abstract": "We study the problem of distributed multi-task learning with shared representation, where each machine aims to learn a separate, but related, task in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix has low rank. We consider a setting where each task is handled by a different machine, with samples for the task available locally on the machine, and study communication-efficient methods for exploiting the shared structure.", "venue": "arXiv", "keywords": ["multi-task learning"]}
{"id": "wangDistributedMultiTaskLearning2016a", "title": "Distributed Multi-Task Learning", "abstract": "We consider the problem of distributed multi-task learning, where each machine learns a separate, but related, task. Specifically, each machine learns a linear predictor in high-dimensional space, where all tasks share the same small support. We present a communication-efficient estimator based on the debiased lasso and show that it is comparable with the optimal centralized method.", "venue": "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics", "keywords": ["multi-task learning"]}
{"id": "wangDistributedStochasticMultiTask2018", "title": "Distributed Stochastic Multi-Task Learning with Graph Regularization", "abstract": "We propose methods for distributed graph-based multi-task learning that are based on weighted averaging of messages from other machines. Uniform averaging or diminishing stepsize in these methods would yield consensus (single task) learning. We show how simply skewing the averaging weights or controlling the stepsize allows learning different, but related, tasks on the different machines.", "venue": "arXiv", "keywords": ["multi-task learning"]}
{"id": "wangDiverseImageInpainting2022", "title": "Diverse Image Inpainting with Normalizing Flow", "abstract": "", "venue": "Computer Vision -- ECCV 2022", "keywords": ["image inpainting", "normalizing flows"]}
{"id": "wangEfficientLoFTRSemiDense2024", "title": "Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed", "abstract": "We present a novel method for efficiently producing semi-dense matches across images. Previous detector-free matcher LoFTR has shown remarkable matching capability in handling large-viewpoint change and texture-poor scenarios but suffers from low efficiency. We revisit its design choices and derive multiple improvements for both efficiency and accuracy. One key observation is that performing the transformer over the entire feature map is redundant due to shared local information, therefore we propose an aggregated attention mechanism with adaptive token selection for efficiency. Furthermore, we find spatial variance exists in LoFTR's fine correlation module, which is adverse to matching accuracy. A novel two-stage correlation layer is proposed to achieve accurate subpixel correspondences for accuracy improvement. Our efficiency optimized model is \\ faster than LoFTR which can even surpass state-of-the-art efficient sparse matching pipeline SuperPoint + LightGlue. Moreover, extensive experiments show that our method can achieve higher accuracy compared with competitive semi-dense matchers, with considerable efficiency benefits. This opens up exciting prospects for large-scale or latency-sensitive applications such as image retrieval and 3D reconstruction. Project page: https://zju3dv.github.io/efficientloftr.", "venue": "arXiv", "keywords": ["feature pyramids", "transformers"]}
{"id": "wangEfficientTrainGeneralizedCurriculum2024", "title": "EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training", "abstract": "The superior performance of modern visual backbones usually comes with a costly training procedure. We contribute to this issue by generalizing the idea of curriculum learning beyond its original formulation, i.e., training models using easier-to-harder data. Specifically, we reformulate the training curriculum as a soft-selection function, which uncovers progressively more difficult patterns within each example during training, instead of performing easier-to-harder sample selection. Our work is inspired by an intriguing observation on the learning dynamics of visual backbones: during the earlier stages of training, the model predominantly learns to recognize some 'easier-to-learn' discriminative patterns in the data. These patterns, when observed through frequency and spatial domains, incorporate lower-frequency components, and the natural image contents without distortion or data augmentation. Motivated by these findings, we propose a curriculum where the model always leverages all the training data at every learning stage, yet the exposure to the 'easier-to-learn' patterns of each example is initiated first, with harder patterns gradually introduced as training progresses. To implement this idea in a computationally efficient way, we introduce a cropping operation in the Fourier spectrum of the inputs, enabling the model to learn from only the lower-frequency components. Then we show that exposing the contents of natural images can be readily achieved by modulating the intensity of data augmentation. Finally, we integrate these aspects and design curriculum schedules with tailored search algorithms. The resulting method, EfficientTrain++, is simple, general, yet surprisingly effective. It reduces the training time of a wide variety of popular models by 1.5-3.0x on ImageNet-1K/22K without sacrificing accuracy. It also demonstrates efficacy in self-supervised learning (e.g., MAE).", "venue": "arXiv", "keywords": []}
{"id": "wangExploringUncertaintyBasedSelfPrompt2024", "title": "Exploring Uncertainty-Based Self-Prompt for Test-Time Adaptation Semantic Segmentation in Remote Sensing Images", "abstract": "Test-time adaptation (TTA) has been proven to effectively improve the adaptability of deep learning semantic segmentation models facing continuous changeable scenes. However, most of the existing TTA algorithms lack an explicit exploration of domain gaps, especially those based on visual domain prompts. To address these issues, this paper proposes a self-prompt strategy based on uncertainty, guiding the model to continuously focus on regions with high uncertainty (i.e., regions with a larger domain gap). Specifically, we still use the Mean-Teacher architecture with the predicted entropy from the teacher network serving as the input to the prompt module. The prompt module processes uncertain maps and guides the student network to focus on regions with higher entropy, enabling continuous adaptation to new scenes. This is a self-prompting strategy that requires no prior knowledge and is tested on widely used benchmarks. In terms of the average performance, our method outperformed the baseline algorithm in TTA and continual TTA settings of Cityscapes-to-ACDC by 3.3% and 3.9%, respectively. Our method also outperformed the baseline algorithm by 4.1% and 3.1% on the more difficult Cityscapes-to-(Foggy and Rainy) Cityscapes setting, which also surpasses six other current TTA methods.", "venue": "Remote Sensing", "keywords": ["continual da", "domain adaptation", "entropic optimization", "knowledge distillation", "test-time da"]}
{"id": "wangGeneralizingUnseenDomains2022", "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization", "abstract": "Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.", "venue": "arXiv", "keywords": ["surveys"]}
{"id": "wangHighFrequencyComponent2020", "title": "High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks", "abstract": "We investigate the relationship between the frequency spectrum of image data and the generalization behavior of convolutional neural networks (CNN). We first notice CNN's ability in capturing the high-frequency components of images. These high-frequency components are almost imperceptible to a human. Thus the observation leads to multiple hypotheses that are related to the generalization behaviors of CNN, including a potential explanation for adversarial examples, a discussion of CNN's trade-off between robustness and accuracy, and some evidence in understanding training heuristics.", "venue": "arXiv", "keywords": ["cnns", "domain generalization", "robustness analysis", "spectral methods"]}
{"id": "wangKnowledgeGuidedDisambiguation2017", "title": "Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs", "abstract": "Convolutional Neural Networks (CNNs) have made remarkable progress on scene recognition, partially due to these recent large-scale scene datasets, such as the Places and Places2. Scene categories are often defined by multi-level information, including local objects, global layout, and background environment, thus leading to large intra-class variations. In addition, with the increasing number of scene categories, label ambiguity has become another crucial issue in large-scale classification. This paper focuses on large-scale scene recognition and makes two major contributions to tackle these issues. First, we propose a multi-resolution CNN architecture that captures visual content and structure at multiple levels. The multi-resolution CNNs are composed of coarse resolution CNNs and fine resolution CNNs, which are complementary to each other. Second, we design two knowledge guided disambiguation techniques to deal with the problem of label ambiguity. (i) We exploit the knowledge from the confusion matrix computed on validation data to merge ambiguous classes into a super category. (ii) We utilize the knowledge of extra networks to produce a soft label for each image. Then the super categories or soft labels are employed to guide CNN training on the Places2. We conduct extensive experiments on three large-scale image datasets (ImageNet, Places, and Places2), demonstrating the effectiveness of our approach. Furthermore, our method takes part in two major scene recognition challenges, and achieves the second place at the Places2 challenge in ILSVRC 2015, and the first place at the LSUN challenge in CVPR 2016. Finally, we directly test the learned representations on other scene benchmarks, and obtain the new state-of-the-art results on the MIT Indoor67 (86.7 %) and SUN397 (72.0 %). We release the code and models at ://github.com/wanglimin/MRCNN-Scene-Recognition\\.", "venue": "IEEE Transactions on Image Processing", "keywords": []}
{"id": "wangLearningRobustGlobal2019", "title": "Learning Robust Global Representations by Penalizing Local Predictive Power", "abstract": "Despite their renowned in-domain predictive power, convolutional neural networks are known to rely more on high-frequency patterns that humans deem superficial than on low-frequency patterns that agree better with intuitions about what constitutes category membership. This paper proposes a method for training robust convolutional networks by penalizing the predictive power of the local representations learned by earlier layers. Intuitively, our networks are forced to discard predictive signals such as color and texture that can be gleaned from local receptive fields and to rely instead on the global structures of the image. Across a battery of synthetic and benchmark domain adaptation tasks, our method confers improved generalization out of the domain. Additionally, to evaluate cross-domain transfer, we introduce ImageNet-Sketch, a new dataset consisting of sketch-like images that matches the ImageNet classification validation set in scale and dimension.", "venue": "Advances in Neural Information Processing Systems", "keywords": []}
{"id": "wangMatchFormerInterleavingAttention2022", "title": "MatchFormer: Interleaving Attention in Transformers for Feature Matching", "abstract": "", "venue": "Proceedings of the Asian Conference on Computer Vision", "keywords": ["feature fusion", "transformers"]}
{"id": "wangMicroASTSuperFastUltraResolution2022", "title": "MicroAST: Towards Super-Fast Ultra-Resolution Arbitrary Style Transfer", "abstract": "Arbitrary style transfer (AST) transfers arbitrary artistic styles onto content images. Despite the recent rapid progress, existing AST methods are either incapable or too slow to run at ultra-resolutions (e.g., 4K) with limited resources, which heavily hinders their further applications. In this paper, we tackle this dilemma by learning a straightforward and lightweight model, dubbed MicroAST. The key insight is to completely abandon the use of cumbersome pre-trained Deep Convolutional Neural Networks (e.g., VGG) at inference. Instead, we design two micro encoders (content and style encoders) and one micro decoder for style transfer. The content encoder aims at extracting the main structure of the content image. The style encoder, coupled with a modulator, encodes the style image into learnable dual-modulation signals that modulate both intermediate features and convolutional filters of the decoder, thus injecting more sophisticated and flexible style signals to guide the stylizations. In addition, to boost the ability of the style encoder to extract more distinct and representative style signals, we also introduce a new style signal contrastive loss in our model. Compared to the state of the art, our MicroAST not only produces visually superior results but also is 5-73 times smaller and 6-18 times faster, for the first time enabling super-fast (about 0.5 seconds) AST at 4K ultra-resolutions. Code is available at https://github.com/EndyWon/MicroAST.", "venue": "arXiv", "keywords": ["style transfer", "super-resolution"]}
{"id": "wangMixtureofExpertsLearnerSingle2023", "title": "Mixture-of-Experts Learner for Single Long-Tailed Domain Generalization", "abstract": "Domain generalization (DG) refers to the task of training a model on multiple source domains and test it on a different target domain with different distribution. In this paper, we address a more challenging and realistic scenario known as Single Long-Tailed Domain Generalization, where only one source domain is available and the minority class in this domain has an abundance of instances in other domains. To tackle this task, we propose a novel approach called Mixture-of-Experts Learner for Single Long-Tailed Domain Generalization (MoEL), which comprises two key strategies. The first strategy is a simple yet effective data augmentation technique that leverages saliency maps to identify important regions on the original images and preserves these regions during augmentation. The second strategy is a new skill-diverse expert learning approach that trains multiple experts from a single long-tailed source domain and leverages mutual learning to aggregate their learned knowledge for the unknown target domain. We evaluate our method on various benchmark datasets, including Digits-DG, CIFAR-10-C, PACS, and DomainNet, and demonstrate its superior performance compared to previous single domain generalization methods. Additionally, the ablation study is also conducted to illustrate the inner workings of our approach.", "venue": "Proceedings of the 31st ACM International Conference on Multimedia", "keywords": ["domain generalization", "mixture-of-experts", "mutual information measures"]}
{"id": "wangMultimodalTransferHierarchical2017", "title": "Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer", "abstract": "", "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "keywords": []}
{"id": "wangOcclusionAwareUnsupervised2018", "title": "Occlusion Aware Unsupervised Learning of Optical Flow", "abstract": "It has been recently shown that a convolutional neural network can learn optical flow estimation with unsupervised learning. However, the performance of the unsupervised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsupervised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Especially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning.", "venue": "arXiv", "keywords": ["optical flow", "temporal consistency", "unsupervised learning"]}
{"id": "wangProvableDomainGeneralization2022", "title": "Provable Domain Generalization via Invariant-Feature Subspace Recovery", "abstract": "Domain generalization asks for models trained over a set of training environments to perform well in unseen test environments. Recently, a series of algorithms such as Invariant Risk Minimization (IRM) has been proposed for domain generalization. However, Rosenfeld et al. (2021) shows that in a simple linear data model, even if non-convexity issues are ignored, IRM and its extensions cannot generalize to unseen environments with less than ds+1ds+1d_s+1 training environments, where dsdsd_s is the dimension of the spurious-feature subspace. In this paper, we propose to achieve domain generalization with Invariant-feature Subspace Recovery (ISR). Our first algorithm, ISR-Mean, can identify the subspace spanned by invariant features from the first-order moments of the class-conditional distributions, and achieve provable domain generalization with ds+1ds+1d_s+1 training environments under the data model of Rosenfeld et al. (2021). Our second algorithm, ISR-Cov, further reduces the required number of training environments to O(1)O(1)O(1) using the information of second-order moments. Notably, unlike IRM, our algorithms bypass non-convexity issues and enjoy global convergence guarantees. Empirically, our ISRs can obtain superior performance compared with IRM on synthetic benchmarks. In addition, on three real-world image and text datasets, we show that both ISRs can be used as simple yet effective post-processing methods to improve the worst-case accuracy of (pre-)trained models against spurious correlations and group shifts.", "venue": "Proceedings of the 39th International Conference on Machine Learning", "keywords": ["domain generalization", "feature engineering", "risk optimization"]}
{"id": "wangPyramidVisionTransformer2021", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions", "abstract": "Although using convolutional neural networks (CNNs) as backbones achieves great successes in computer vision, this work investigates a simple backbone network useful for many dense prediction tasks without convolutions. Unlike the recently-proposed Transformer model (e.g., ViT) that is specially designed for image classification, we propose Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to prior arts. (1) Different from ViT that typically has low-resolution outputs and high computational and memory cost, PVT can be not only trained on dense partitions of the image to achieve high output resolution, which is important for dense predictions but also using a progressive shrinking pyramid to reduce computations of large feature maps. (2) PVT inherits the advantages from both CNN and Transformer, making it a unified backbone in various vision tasks without convolutions by simply replacing CNN backbones. (3) We validate PVT by conducting extensive experiments, showing that it boosts the performance of many downstream tasks, e.g., object detection, semantic, and instance segmentation. For example, with a comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO dataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. We hope PVT could serve as an alternative and useful backbone for pixel-level predictions and facilitate future researches. Code is available at https://github.com/whai362/PVT.", "venue": "arXiv", "keywords": ["feature pyramids", "transformers"]}
{"id": "wangRealESRGANTrainingRealWorld2021", "title": "Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data", "abstract": "Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.", "venue": "arXiv", "keywords": []}
{"id": "wangRealESRGANTrainingRealWorld2021a", "title": "Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data", "abstract": "Though many attempts have been made in blind superresolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex realworld degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.", "venue": "arXiv", "keywords": []}
{"id": "wangRealESRGANTrainingRealWorld2021b", "title": "Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data", "abstract": "Though many attempts have been made in blind superresolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex realworld degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.", "venue": "arXiv", "keywords": []}
{"id": "wangReSmoothDetectingUtilizing2022", "title": "ReSmooth: Detecting and Utilizing OOD Samples When Training with Data Augmentation", "abstract": "Data augmentation (DA) is a widely used technique for enhancing the training of deep neural networks. Recent DA techniques which achieve state-of-the-art performance always meet the need for diversity in augmented training samples. However, an augmentation strategy that has a high diversity usually introduces out-of-distribution (OOD) augmented samples and these samples consequently impair the performance. To alleviate this issue, we propose ReSmooth, a framework that firstly detects OOD samples in augmented samples and then leverages them. To be specific, we first use a Gaussian mixture model to fit the loss distribution of both the original and augmented samples and accordingly split these samples into in-distribution (ID) samples and OOD samples. Then we start a new training where ID and OOD samples are incorporated with different smooth labels. By treating ID samples and OOD samples unequally, we can make better use of the diverse augmented data. Further, we incorporate our ReSmooth framework with negative data augmentation strategies. By properly handling their intentionally created OOD samples, the classification performance of negative data augmentations is largely ameliorated. Experiments on several classification benchmarks show that ReSmooth can be easily extended to existing augmentation strategies (such as RandAugment, rotate, and jigsaw) and improve on them. Our code is available at https://github.com/Chenyang4/ReSmooth.", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "keywords": ["domain adaptation"]}
{"id": "wangRethinkingImprovingRobustness2021", "title": "Rethinking and Improving the Robustness of Image Style Transfer", "abstract": "Extensive research in neural style transfer methods has shown that the correlation between features extracted by a pre-trained VGG network has a remarkable ability to capture the visual style of an image. Surprisingly, however, this stylization quality is not robust and often degrades significantly when applied to features from more advanced and lightweight networks, such as those in the ResNet family. By performing extensive experiments with different network architectures, we find that residual connections, which represent the main architectural difference between VGG and ResNet, produce feature maps of small entropy, which are not suitable for style transfer. To improve the robustness of the ResNet architecture, we then propose a simple yet effective solution based on a softmax transformation of the feature activations that enhances their entropy. Experimental results demonstrate that this small magic can greatly improve the quality of stylization results, even for networks with random weights. This suggests that the architecture used for feature extraction is more important than the use of learned weights for the task of style transfer.", "venue": "arXiv", "keywords": ["augmentation stability", "entropic optimization", "style transfer"]}
{"id": "wangSaliencyAwareVideoObject2018", "title": "Saliency-Aware Video Object Segmentation", "abstract": "Video saliency, aiming for estimation of a single dominant object in a sequence, offers strong object-level cues for unsupervised video object segmentation. In this paper, we present a geodesic distance based technique that provides reliable and temporally consistent saliency measurement of superpixels as a prior for pixel-wise labeling. Using undirected intra-frame and inter-frame graphs constructed from spatiotemporal edges or appearance and motion, and a skeleton abstraction step to further enhance saliency estimates, our method formulates the pixel-wise segmentation task as an energy minimization problem on a function that consists of unary terms of global foreground and background models, dynamic location models, and pairwise terms of label smoothness potentials. We perform extensive quantitative and qualitative experiments on benchmark datasets. Our method achieves superior performance in comparison to the current state-of-the-art in terms of accuracy and speed.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "keywords": []}
{"id": "wangSampleEfficientActorCritic2017", "title": "Sample Efficient Actor-Critic with Experience Replay", "abstract": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.", "venue": "arXiv", "keywords": []}
{"id": "wangSearchLostOnline2024", "title": "In Search of Lost Online Test-Time Adaptation: A Survey", "abstract": "This article presents a comprehensive survey of online test-time adaptation (OTTA), focusing on effectively adapting machine learning models to distributionally different target data upon batch arrival. Despite the recent proliferation of OTTA methods, conclusions from previous studies are inconsistent due to ambiguous settings, outdated backbones, and inconsistent hyperparameter tuning, which obscure core challenges and hinder reproducibility. To enhance clarity and enable rigorous comparison, we classify OTTA techniques into three primary categories and benchmark them using a modern backbone, the Vision Transformer. Our benchmarks cover conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C, as well as real-world shifts represented by CIFAR-10.1, OfficeHome, and CIFAR-10-Warehouse. The CIFAR-10-Warehouse dataset includes a variety of variations from different search engines and synthesized data generated through diffusion models. To measure efficiency in online scenarios, we introduce novel evaluation metrics, including GFLOPs, wall clock time, and GPU memory usage, providing a clearer picture of the trade-offs between adaptation accuracy and computational overhead. Our findings diverge from existing literature, revealing that (1) transformers demonstrate heightened resilience to diverse domain shifts, (2) the efficacy of many OTTA methods relies on large batch sizes, and (3) stability in optimization and resistance to perturbations are crucial during adaptation, particularly when the batch size is 1. Based on these insights, we highlight promising directions for future research. Our benchmarking toolkit and source code are available at https://github.com/Jo-wang/OTTA_ViT_survey.", "venue": "International Journal of Computer Vision", "keywords": ["domain adaptation", "online da", "surveys", "test-time da"]}
{"id": "wangSpatialAttentiveSingleImage2019", "title": "Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset", "abstract": "Removing rain streaks from a single image has been drawing considerable attention as rain streaks can severely degrade the image quality and affect the performance of existing outdoor vision tasks. While recent CNN-based derainers have reported promising performances, deraining remains an open problem for two reasons. First, existing synthesized rain datasets have only limited realism, in terms of modeling real rain characteristics such as rain shape, direction and intensity. Second, there are no public benchmarks for quantitative comparisons on real rain images, which makes the current evaluation less objective. The core challenge is that real world rain/clean image pairs cannot be captured at the same time. In this paper, we address the single image rain removal problem in two ways. First, we propose a semi-automatic method that incorporates temporal priors and human supervision to generate a high-quality clean image from each input sequence of real rain images. Using this method, we construct a large-scale dataset of \\ \\ rain/rain-free image pairs that covers a wide range of natural rain scenes. Second, to better cover the stochastic distribution of real rain streaks, we propose a novel SPatial Attentive Network (SPANet) to remove rain streaks in a local-to-global manner. Extensive experiments demonstrate that our network performs favorably against the state-of-the-art deraining methods.", "venue": "arXiv.org", "keywords": ["dataset debut"]}
{"id": "wangTentFullyTesttime2021", "title": "Tent: Fully Test-time Adaptation by Entropy Minimization", "abstract": "A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent1): we optimize the model for confidence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training.", "venue": "arXiv", "keywords": ["domain adaptation", "entropic optimization", "self-supervised learning", "test-time da", "unsupervised da"]}
{"id": "wangTransferLearningMinimizing", "title": "Transfer Learning via Minimizing the Performance Gap Between Domains Supplementary Materials", "abstract": "", "venue": "", "keywords": ["domain generalization", "generalization quantification", "transfer learning"]}
{"id": "wangTransferLearningMinimizing2019", "title": "Transfer Learning via Minimizing the Performance Gap Between Domains", "abstract": "We propose a new principle for transfer learning, based on a straightforward intuition: if two domains are similar to each other, the model trained on one domain should also perform well on the other domain, and vice versa. To formalize this intuition, we define the performance gap as a measure of the discrepancy between the source and target domains. We derive generalization bounds for the instance weighting approach to transfer learning, showing that the performance gap can be viewed as an algorithm-dependent regularizer, which controls the model complexity. Our theoretical analysis provides new insight into transfer learning and motivates a set of general, principled rules for designing new instance weighting schemes for transfer learning. These rules lead to gapBoost, a novel and principled boosting approach for transfer learning. Our experimental evaluation on benchmark data sets shows that gapBoost significantly outperforms previous boosting-based transfer learning algorithms.", "venue": "Advances in Neural Information Processing Systems", "keywords": ["domain generalization", "generalization quantification", "transfer learning"]}
{"id": "wangUnifiedJointMaximum2021", "title": "A Unified Joint Maximum Mean Discrepancy for Domain Adaptation", "abstract": "Domain adaptation has received a lot of attention in recent years, and many algorithms have been proposed with impressive progress. However, it is still not fully explored concerning the joint probability distribution (P(X, Y)) distance for this problem, since its empirical estimation derived from the maximum mean discrepancy (joint maximum mean discrepancy, JMMD) will involve complex tensor-product operator that is hard to manipulate. To solve this issue, this paper theoretically derives a unified form of JMMD that is easy to optimize, and proves that the marginal, class conditional and weighted class conditional probability distribution distances are our special cases with different label kernels, among which the weighted class conditional one not only can realize feature alignment across domains in the category level, but also deal with imbalance dataset using the class prior probabilities. From the revealed unified JMMD, we illustrate that JMMD degrades the feature-label dependence (discriminability) that benefits to classification, and it is sensitive to the label distribution shift when the label kernel is the weighted class conditional one. Therefore, we leverage Hilbert Schmidt independence criterion and propose a novel MMD matrix to promote the dependence, and devise a novel label kernel that is robust to label distribution shift. Finally, we conduct extensive experiments on several cross-domain datasets to demonstrate the validity and effectiveness of the revealed theoretical results.", "venue": "arXiv", "keywords": []}
{"id": "wangUnsupervisedDomainAdaptation2023", "title": "Unsupervised Domain Adaptation for Semantic Segmentation via Cross-Region Alignment", "abstract": "Semantic segmentation requires a lot of training data, which necessitates costly annotation. There have been many studies on unsupervised domain adaptation (UDA) from one domain to another, e.g., from computer graphics to real images. However, there is still a gap in accuracy between UDA and supervised training on native domain data. It is arguably attributable to the class-level misalignment between the source and target domain data. To cope with this, we propose a method that applies adversarial training to align two feature distributions in the target domain. It uses a self-training framework to split the image into two regions (i.e., trusted and untrusted), which form two distributions to align in the feature space. We term this approach cross-region adaptation (CRA) to distinguish it from the previous methods of aligning different domain distributions, which we call cross-domain adaptation (CDA). CRA can be applied after any CDA method. Experimental results show that this always improves the accuracy of the combined CDA method.", "venue": "Computer Vision and Image Understanding", "keywords": ["domain adaptation", "semantic segmentation", "unsupervised da"]}
{"id": "wangWhatMakesGood2023", "title": "What Makes a \"Good\" Data Augmentation in Knowledge Distillation -- A Statistical Perspective", "abstract": "Knowledge distillation (KD) is a general neural network training approach that uses a teacher model to guide the student model. Existing works mainly study KD from the network output side (e.g., trying to design a better KD loss function), while few have attempted to understand it from the input side. Especially, its interplay with data augmentation (DA) has not been well understood. In this paper, we ask: Why do some DA schemes (e.g., CutMix) inherently perform much better than others in KD? What makes a \"good\" DA in KD? Our investigation from a statistical perspective suggests that a good DA scheme should reduce the covariance of the teacher-student cross-entropy. A practical metric, the stddev of teacher's mean probability (T. stddev), is further presented and well justified empirically. Besides the theoretical understanding, we also introduce a new entropy-based data-mixing DA scheme, CutMixPick, to further enhance CutMix. Extensive empirical studies support our claims and demonstrate how we can harvest considerable performance gains simply by using a better DA scheme in knowledge distillation.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "knowledge distillation"]}
{"id": "wardImprovingExplorationSoftActorCritic2019", "title": "Improving Exploration in Soft-Actor-Critic with Normalizing Flows Policies", "abstract": "Deep Reinforcement Learning (DRL) algorithms for continuous action spaces are known to be brittle toward hyperparameters as well as sample inefficient. Soft Actor Critic (SAC) proposes an off-policy deep actor critic algorithm within the maximum entropy RL framework which offers greater stability and empirical gains. The choice of policy distribution, a factored Gaussian, is motivated by its easy re-parametrization rather than its modeling power. We introduce Normalizing Flow policies within the SAC framework that learn more expressive classes of policies than simple factored Gaussians. We show empirically on continuous grid world tasks that our approach increases stability and is better suited to difficult exploration in sparse reward settings.", "venue": "arXiv", "keywords": ["actor-critic", "normalizing flows", "reinforcement learning"]}
{"id": "wassermanTopologicalDataAnalysis2018", "title": "Topological Data Analysis", "abstract": "Topological data analysis (TDA) can broadly be described as a collection of data analysis methods that find structure in data. These methods include clustering, manifold estimation, nonlinear dimension reduction, mode estimation, ridge estimation and persistent homology. This paper reviews some of these methods.", "venue": "Annual Review of Statistics and Its Application", "keywords": []}
{"id": "weberCertifyingOutofDomainGeneralization2022", "title": "Certifying Out-of-Domain Generalization for Blackbox Functions", "abstract": "Certifying the robustness of model performance under bounded data distribution drifts has recently attracted intensive interest under the umbrella of distributional robustness. However, existing techniques either make strong assumptions on the model class and loss functions that can be certified, such as smoothness expressed via Lipschitz continuity of gradients, or require to solve complex optimization problems. As a result, the wider application of these techniques is currently limited by its scalability and flexibility -- these techniques often do not scale to large-scale datasets with modern deep neural networks or cannot handle loss functions which may be non-smooth such as the 0-1 loss. In this paper, we focus on the problem of certifying distributional robustness for blackbox models and bounded loss functions, and propose a novel certification framework based on the Hellinger distance. Our certification technique scales to ImageNet-scale datasets, complex models, and a diverse set of loss functions. We then focus on one specific application enabled by such scalability and flexibility, i.e., certifying out-of-domain generalization for large neural networks and loss functions such as accuracy and AUC. We experimentally validate our certification method on a number of datasets, ranging from ImageNet, where we provide the first non-vacuous certified out-of-domain generalization, to smaller classification tasks where we are able to compare with the state-of-the-art and show that our method performs considerably better.", "venue": "International Conference on Machine Learning", "keywords": ["domain generalization", "generalization certification", "generalization quantification", "lipschitz-constraints", "uncertainty quantification"]}
{"id": "weiAcademicGPTEmpoweringAcademic2023", "title": "AcademicGPT: Empowering Academic Research", "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across various natural language processing tasks. Yet, many of these advanced LLMs are tailored for broad, general-purpose applications. In this technical report, we introduce AcademicGPT, designed specifically to empower academic research. AcademicGPT is a continual training model derived from LLaMA2-70B. Our training corpus mainly consists of academic papers, thesis, content from some academic domain, high-quality Chinese data and others. While it may not be extensive in data scale, AcademicGPT marks our initial venture into a domain-specific GPT tailored for research area. We evaluate AcademicGPT on several established public benchmarks such as MMLU and CEval, as well as on some specialized academic benchmarks like PubMedQA, SCIEval, and our newly-created ComputerScienceQA, to demonstrate its ability from general knowledge ability, to Chinese ability, and to academic ability. Building upon AcademicGPT's foundation model, we also developed several applications catered to the academic area, including General Academic Question Answering, AI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract Generation.", "venue": "arXiv", "keywords": []}
{"id": "weiDatadependentSampleComplexity2020", "title": "Data-Dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation", "abstract": "Existing Rademacher complexity bounds for neural networks rely only on norm control of the weight matrices and depend exponentially on depth via a product of the matrix norms. Lower bounds show that this exponential dependence on depth is unavoidable when no additional properties of the training data are considered. We suspect that this conundrum comes from the fact that these bounds depend on the training data only through the margin. In practice, many data-dependent techniques such as Batchnorm improve the generalization performance. For feedforward neural nets as well as RNNs, we obtain tighter Rademacher complexity bounds by considering additional data-dependent properties of the network: the norms of the hidden layers of the network, and the norms of the Jacobians of each layer with respect to all previous layers. Our bounds scale polynomially in depth when these empirical quantities are small, as is usually the case in practice. To obtain these bounds, we develop general tools for augmenting a sequence of functions to make their composition Lipschitz and then covering the augmented functions. Inspired by our theory, we directly regularize the network's Jacobians during training and empirically demonstrate that this improves test performance.", "venue": "arXiv", "keywords": ["generalization quantification", "highly-analytical", "lipschitz-constraints", "loss functions"]}
{"id": "weiFeatureCorrectiveTransfer2024", "title": "Feature Corrective Transfer Learning: End-to-End Solutions to Object Detection in Non-Ideal Visual Conditions", "abstract": "A significant challenge in the field of object detection lies in the system's performance under non-ideal imaging conditions, such as rain, fog, low illumination, or raw Bayer images that lack ISP processing. Our study introduces \"Feature Corrective Transfer Learning\", a novel approach that leverages transfer learning and a bespoke loss function to facilitate the end-to-end detection of objects in these challenging scenarios without the need to convert non-ideal images into their RGB counterparts. In our methodology, we initially train a comprehensive model on a pristine RGB image dataset. Subsequently, non-ideal images are processed by comparing their feature maps against those from the initial ideal RGB model. This comparison employs the Extended Area Novel Structural Discrepancy Loss (EANSDL), a novel loss function designed to quantify similarities and integrate them into the detection loss. This approach refines the model's ability to perform object detection across varying conditions through direct feature map correction, encapsulating the essence of Feature Corrective Transfer Learning. Experimental validation on variants of the KITTI dataset demonstrates a significant improvement in mean Average Precision (mAP), resulting in a 3.8-8.1% relative enhancement in detection under non-ideal conditions compared to the baseline model, and a less marginal performance difference within 1.3% of the mAP@[0.5:0.95] achieved under ideal conditions by the standard Faster RCNN algorithm.", "venue": "arXiv", "keywords": ["domain adaptation", "feature engineering", "loss functions", "transfer learning"]}
{"id": "weiWindmapleAwesomeAutoML2024", "title": "Windmaple/Awesome-AutoML", "abstract": "Curating a list of AutoML-related research, tools, projects and other resources", "venue": "", "keywords": []}
{"id": "wenCAPVSTNetContentAffinity2023", "title": "CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer", "abstract": "Content affinity loss including feature and pixel affinity is a main problem which leads to artifacts in photorealistic and video style transfer. This paper proposes a new framework named CAP-VSTNet, which consists of a new reversible residual network and an unbiased linear transform module, for versatile style transfer. This reversible residual network can not only preserve content affinity but not introduce redundant information as traditional reversible networks, and hence facilitate better stylization. Empowered by Matting Laplacian training loss which can address the pixel affinity loss problem led by the linear transform, the proposed framework is applicable and effective on versatile style transfer. Extensive experiments show that CAP-VSTNet can produce better qualitative and quantitative results in comparison with the state-of-the-art methods.", "venue": "arXiv", "keywords": ["affinity modeling", "critical citation", "style transfer"]}
{"id": "wenDoesResistanceStyletransfer2024", "title": "Does Resistance to Style-Transfer Equal Global Shape Bias? Measuring Network Sensitivity to Global Shape Configuration", "abstract": "Deep learning models are known to exhibit a strong texture bias, while human tends to rely heavily on global shape structure for object recognition. The current benchmark for evaluating a model's global shape bias is a set of style-transferred images with the assumption that resistance to the attack of style transfer is related to the development of global structure sensitivity in the model. In this work, we show that networks trained with style-transfer images indeed learn to ignore style, but its shape bias arises primarily from local detail. We provide a Structure Testbench (DiST)\\ as a direct measurement of global structure sensitivity. Our test includes 2400 original images from ImageNet-1K, each of which is accompanied by two images with the global shapes of the original image disrupted while preserving its texture via the texture synthesis program. We found that \\\\(1) models that performed well on the previous cue-conflict dataset do not fare well in the proposed DiST; (2) the supervised trained Vision Transformer (ViT) lose its global spatial information from positional embedding, leading to no significant advantages over Convolutional Neural Networks (CNNs) on DiST. While self-supervised learning methods, especially mask autoencoder significantly improves the global structure sensitivity of ViT. (3) Improving the global structure sensitivity is orthogonal to resistance to style-transfer, indicating that the relationship between global shape structure and local texture detail is not an either/or relationship. Training with DiST images and style-transferred images are complementary, and can be combined to train network together to enhance the global shape sensitivity and robustness of local features.\\ Our code will be hosted in github: https://github.com/leelabcnbc/DiST", "venue": "arXiv", "keywords": ["bias sources", "promising", "style transfer"]}
{"id": "wengFlowbasedDeepGenerative2018", "title": "Flow-Based Deep Generative Models", "abstract": "So far, I've written about two types of generative models, GAN and VAE. Neither of them explicitly learns the probability density function of real data, \\ (where \\ ) --- because it is really hard! Taking the generative model with latent variables as an example, \\ can hardly be calculated as it is intractable to go through all possible values of the latent code \\ .", "venue": "", "keywords": []}
{"id": "wenzelAssayingOutOfDistributionGeneralization2022", "title": "Assaying Out-Of-Distribution Generalization in Transfer Learning", "abstract": "", "venue": "Advances in Neural Information Processing Systems", "keywords": ["critical citation", "generalization quantification", "highly-analytical", "promising", "transfer learning"]}
{"id": "WhatAlgorithmDoes", "title": "What Algorithm Does Google (YouTube) Use to Compress a Video? How Are More than a Billion Videos Retrieved in a Second?", "abstract": "Answer (1 of 3): YouTube uses H.264 or MPEG-4 Part 10, a block-oriented motion-compensation-based video compression standard which is one of the most commonly used formats in the world for the recording, compression, and distribution of video content. This format provides good video quality at su...", "venue": "Quora", "keywords": []}
{"id": "wignessRUGDDatasetAutonomous2019", "title": "A RUGD Dataset for Autonomous Navigation and Visual Perception in Unstructured Outdoor Environments", "abstract": "Research in autonomous driving has benefited from a number of visual datasets collected from mobile platforms, leading to improved visual perception, greater scene understanding, and ultimately higher intelligence. However, this set of existing data collectively represents only highly structured, urban environments. Operation in unstructured environments, e.g., humanitarian assistance and disaster relief or off-road navigation, bears little resemblance to these existing data. To address this gap, we introduce the Robot Unstructured Ground Driving (RUGD) dataset with video sequences captured from a small, unmanned mobile robot traversing in unstructured environments. Most notably, this data differs from existing autonomous driving benchmark data in that it contains significantly more terrain types, irregular class boundaries, minimal structured markings, and presents challenging visual properties often experienced in off road navigation, e.g., blurred frames. Over 7, 000 frames of pixel-wise annotation are included with this dataset, and we perform an initial benchmark using state-of-the-art semantic segmentation architectures to demonstrate the unique challenges this data introduces as it relates to navigation tasks.", "venue": "2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "keywords": ["dataset debut"]}
{"id": "williamsNeuralSplinesFitting2021", "title": "Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks", "abstract": "We present Neural Splines, a technique for 3D surface reconstruction that is based on random feature kernels arising from infinitely-wide shallow ReLU networks. Our method achieves state-of-the-art results, outperforming recent neural network-based techniques and widely used Poisson Surface Reconstruction (which, as we demonstrate, can also be viewed as a type of kernel method). Because our approach is based on a simple kernel formulation, it is easy to analyze and can be accelerated by general techniques designed for kernel-based learning. We provide explicit analytical expressions for our kernel and argue that our formulation can be seen as a generalization of cubic spline interpolation to higher dimensions. In particular, the RKHS norm associated with Neural Splines biases toward smooth interpolants.", "venue": "arXiv", "keywords": ["3d scenes", "neural splines"]}
{"id": "wuCCPLContrastiveCoherence2022", "title": "CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer", "abstract": "In this paper, we aim to devise a universally versatile style transfer method capable of performing artistic, photo-realistic, and video style transfer jointly, without seeing videos during training. Previous single-frame methods assume a strong constraint on the whole image to maintain temporal consistency, which could be violated in many cases. Instead, we make a mild and reasonable assumption that global inconsistency is dominated by local inconsistencies and devise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local patches. CCPL can preserve the coherence of the content source during style transfer without degrading stylization. Moreover, it owns a neighbor-regulating mechanism, resulting in a vast reduction of local distortions and considerable visual quality improvement. Aside from its superior performance on versatile style transfer, it can be easily extended to other tasks, such as image-to-image translation. Besides, to better fuse content and style features, we propose Simple Covariance Transformation (SCT) to effectively align second-order statistics of the content feature with the style feature. Experiments demonstrate the effectiveness of the resulting model for versatile style transfer, when armed with CCPL.", "venue": "arXiv", "keywords": ["affinity modeling", "loss functions", "style transfer"]}
{"id": "wuEFANetExchangeableFeature2019", "title": "EFANet: Exchangeable Feature Alignment Network for Arbitrary Style Transfer", "abstract": "Style transfer has been an important topic both in computer vision and graphics. Since the seminal work of Gatys et al. first demonstrates the power of stylization through optimization in the deep feature space, quite a few approaches have achieved real-time arbitrary style transfer with straightforward statistic matching techniques. In this work, our key observation is that only considering features in the input style image for the global deep feature statistic matching or local patch swap may not always ensure a satisfactory style transfer; see e.g., Figure 1. Instead, we propose a novel transfer framework, EFANet, that aims to jointly analyze and better align exchangeable features extracted from content and style image pair. In this way, the style features from the style image seek for the best compatibility with the content information in the content image, leading to more structured stylization results. In addition, a new whitening loss is developed for purifying the computed content features and better fusion with styles in feature space. Qualitative and quantitative experiments demonstrate the advantages of our approach.", "venue": "arXiv", "keywords": ["feature-level augmentation", "style transfer"]}
{"id": "wuEquivalenceJuleszGibbs1999", "title": "Equivalence of Julesz and Gibbs Texture Ensembles", "abstract": "Research on texture has been pursued along two different lines. The first line of research, pioneered by Julesz (1962), seeks the essential ingredients in terms of features and statistics in human texture perception. This leads us to a mathematical definition of texture as a Julesz ensemble. A Julesz ensemble is the maximum set of images that share the same value of some basic feature statistics as the image lattice /spl Lambda//spl rarr/Z/sup 2/, or equivalently it is a uniform distribution on this set. The second line of research studies statistical models, in particular, Markov random field (MRF) and FRAME models (Zhu et al., 1997), to characterize texture patterns locally. In this article, we bridge the two lines by the fundamental principle of equivalence of ensembles in statistical mechanics (Gibbs, 1902). We prove that 1) the conditional probability of an arbitrary image patch given its environment, under the Julesz ensemble or the uniform model, is inevitably a FRAME (MRF) model, and 2) the limit of the FRAME (MRF) model, which we called the Gibbs ensemble, is equivalent to a Julesz ensemble as /spl Lambda//spl rarr/Z/sup 2/. Thus the advantages of the two methodologies can be fully utilized.", "venue": "Proceedings of the Seventh IEEE International Conference on Computer Vision", "keywords": ["texture transfer"]}
{"id": "wuGeneralizationEffectsLinear2023", "title": "On the Generalization Effects of Linear Transformations in Data Augmentation", "abstract": "Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations that preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations that mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST. Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how uncertain the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms random sampling methods by 1.24% on CIFAR-100 using Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA Adversarial AutoAugment on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.", "venue": "arXiv", "keywords": ["augmentation stability", "surveys"]}
{"id": "wuGeneralizationEffectsLinear2023a", "title": "On the Generalization Effects of Linear Transformations in Data Augmentation", "abstract": "Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations that preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations that mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST. Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how uncertain the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms random sampling methods by 1.24% on CIFAR-100 using Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA Adversarial AutoAugment on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "domain generalization"]}
{"id": "wuImprovingSemanticSegmentation2019", "title": "Improving Semantic Segmentation via Dilated Affinity", "abstract": "Introducing explicit constraints on the structural predictions has been an effective way to improve the performance of semantic segmentation models. Existing methods are mainly based on insufficient hand-crafted rules that only partially capture the image structure, and some methods can also suffer from the efficiency issue. As a result, most of the state-of-the-art fully convolutional networks did not adopt these techniques. In this work, we propose a simple, fast yet effective method that exploits structural information through direct supervision with minor additional expense. To be specific, our method explicitly requires the network to predict semantic segmentation as well as dilated affinity, which is a sparse version of pair-wise pixel affinity. The capability of telling the relationships between pixels are directly built into the model and enhance the quality of segmentation in two stages. 1) Joint training with dilated affinity can provide robust feature representations and thus lead to finer segmentation results. 2) The extra output of affinity information can be further utilized to refine the original segmentation with a fast propagation process. Consistent improvements are observed on various benchmark datasets when applying our framework to the existing state-of-the-art model. Codes will be released soon.", "venue": "arXiv", "keywords": ["affinity modeling", "semantic segmentation"]}
{"id": "wuMultiMetricLatentFactor", "title": "A Multi-Metric Latent Factor Model for Analyzing High-Dimensional and Sparse Data", "abstract": "High-dimensional and sparse (HiDS) matrices are omnipresent in a variety of big data-related applications. Latent factor analysis (LFA) is a typical representation learning method that extracts useful yet latent knowledge from HiDS matrices via low-rank approximation. Current LFA-based models mainly focus on a single-metric representation, where the representation strategy designed for the approximation Loss function, is fixed and exclusive. However, real-world HiDS matrices are commonly heterogeneous and inclusive and have diverse underlying patterns, such that a single-metric representation is most likely to yield inferior performance. Motivated by this, we in this paper propose a multi-metric latent factor (MMLF) model. Its main idea is twofold: 1) two vector spaces and three Lp-norms are simultaneously employed to develop six variants of LFA model, each of which resides in a unique metric representation space, and 2) all the variants are ensembled with a tailored, self-adaptive weighting strategy. As such, our proposed MMLF enjoys the merits originated from a set of disparate metric spaces all at once, achieving the comprehensive and unbiased representation of HiDS matrices. Theoretical study guarantees that MMLF attains a performance gain. Extensive experiments on eight real-world HiDS datasets, spanning a wide range of industrial and science domains, verify that our MMLF significantly outperforms ten state-of-the-art, shallow and deep counterparts.", "venue": "arXiv", "keywords": ["feature engineering", "manifold learning", "representation learning"]}
{"id": "wuMultiMetricLatentFactor2022", "title": "A Multi-Metric Latent Factor Model for Analyzing High-Dimensional and Sparse Data", "abstract": "High-dimensional and sparse (HiDS) matrices are omnipresent in a variety of big data-related applications. Latent factor analysis (LFA) is a typical representation learning method that extracts useful yet latent knowledge from HiDS matrices via low-rank approximation. Current LFA-based models mainly focus on a single-metric representation, where the representation strategy designed for the approximation Loss function, is fixed and exclusive. However, real-world HiDS matrices are commonly heterogeneous and inclusive and have diverse underlying patterns, such that a single-metric representation is most likely to yield inferior performance. Motivated by this, we in this paper propose a multi-metric latent factor (MMLF) model. Its main idea is two-fold: 1) two vector spaces and three Lp-norms are simultaneously employed to develop six variants of LFA model, each of which resides in a unique metric representation space, and 2) all the variants are ensembled with a tailored, self-adaptive weighting strategy. As such, our proposed MMLF enjoys the merits originated from a set of disparate metric spaces all at once, achieving the comprehensive and unbiased representation of HiDS matrices. Theoretical study guarantees that MMLF attains a performance gain. Extensive experiments on eight real-world HiDS datasets, spanning a wide range of industrial and science domains, verify that our MMLF significantly outperforms ten state-of-the-art, shallow and deep counterparts.", "venue": "arXiv", "keywords": []}
{"id": "wuOnlineAdaptiveFault2024", "title": "Online Adaptive Fault Diagnosis With Test-Time Domain Adaptation", "abstract": "Cross-domain bearing fault diagnosis algorithms have garnered considerable attention in recent years due to their robust ability to address domain bias. However, prevailing methods often grapple with two key challenges: the absence of privacy preservation (necessitating access to source domain data) and the inability to facilitate real-time predictions (requiring iterative training on complete target domain data). In response to these issues, this article introduces an algorithm designed to adapt a pretrained model to the target domain in an online fashion. Notably, data augmentation is employed for pretraining the source domain model, enhancing the generalization capabilities. Subsequently, self-supervised learning is integrated through weight average updating. Furthermore, a memory bank-based approach is introduced to augment the compactness of features within the same class. Evaluation on several public datasets demonstrates that our model not only effectively enhances the diagnostic accuracy of the source model, but also achieves state-of-the-art results compared to other test-time adaptation methods.", "venue": "IEEE Transactions on Industrial Informatics", "keywords": ["anomaly detection", "domain adaptation", "knowledge distillation", "online da", "self-training", "test-time da"]}
{"id": "wuPeerCollaborativeLearning2021", "title": "Peer Collaborative Learning for Online Knowledge Distillation", "abstract": "Traditional knowledge distillation uses a two-stage training strategy to transfer knowledge from a high-capacity teacher model to a compact student model, which relies heavily on the pre-trained teacher. Recent online knowledge distillation alleviates this limitation by collaborative learning, mutual learning and online ensembling, following a one-stage end-to-end training fashion. However, collaborative learning and mutual learning fail to construct an online high-capacity teacher, whilst online ensembling ignores the collaboration among branches and its logit summation impedes the further optimisation of the ensemble teacher. In this work, we propose a novel Peer Collaborative Learning method for online knowledge distillation, which integrates online ensembling and network collaboration into a unified framework. Specifically, given a target network, we construct a multi-branch network for training, in which each branch is called a peer. We perform random augmentation multiple times on the inputs to peers and assemble feature representations outputted from peers with an additional classifier as the peer ensemble teacher. This helps to transfer knowledge from a highcapacity teacher to peers, and in turn further optimises the ensemble teacher. Meanwhile, we employ the temporal mean model of each peer as the peer mean teacher to collaboratively transfer knowledge among peers, which helps each peer to learn richer knowledge and facilitates to optimise a more stable model with better generalisation. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet show that the proposed method significantly improves the generalisation of various backbone networks and outperforms the state-of-theart methods.", "venue": "arXiv", "keywords": ["collaborative learning", "knowledge distillation", "online da"]}
{"id": "wuSiamDoGeDomainGeneralizable2022", "title": "SiamDoGe: Domain Generalizable Semantic Segmentation Using Siamese Network", "abstract": "Deep learning-based approaches usually suffer from performance drop on out-of-distribution samples, therefore domain generalization is often introduced to improve the robustness of deep models. Domain randomization (DR) is a common strategy to improve the generalization capability of semantic segmentation networks, however, existing DR-based algorithms require collecting auxiliary domain images to stylize the training samples. In this paper, we propose a novel domain generalizable semantic segmentation method, ``SiamDoGe'', which builds upon a DR approach without using auxiliary domains and employs a Siamese architecture to learn domain-agnostic features from the training dataset. Particularly, the proposed method takes two augmented versions of each training sample as input and produces the corresponding predictions in parallel. Throughout this process, the features from each branch are randomized by those from the other to enhance the feature diversity of training samples. Then the predictions produced from the two branches are enforced to be consistent conditioned on feature sensitivity. Extensive experiment results demonstrate the proposed method exhibits better generalization ability than existing state-of-the-arts across various unseen target domains.", "venue": "Computer Vision -- ECCV 2022", "keywords": ["domain generalization", "domain randomization", "promising", "self-training", "semantic segmentation", "siamese networks"]}
{"id": "wuStabilityBasedGeneralizationAnalysis2020", "title": "Stability-Based Generalization Analysis of Distributed Learning Algorithms for Big Data", "abstract": "As one of the efficient approaches to deal with big data, divide-and-conquer distributed algorithms, such as the distributed kernel regression, bootstrap, structured perception training algorithms, and so on, are proposed and broadly used in learning systems. Some learning theories have been built to analyze the feasibility, approximation, and convergence bounds of these distributed learning algorithms. However, less work has been studied on the stability of these distributed learning algorithms. In this paper, we discuss the generalization bounds of distributed learning algorithms from the view of algorithmic stability. First, we introduce a definition of uniform distributed stability for distributed algorithms and study the distributed algorithms' generalization risk bounds. Then, we analyze the stability properties and generalization risk bounds of a kind of regularization-based distributed algorithms. Two generalization distributed risks obtained show that the generalization distributed risk bounds for the difference between their generalization distributed and empirical distributed/leave-one-computer-out risks are closely related to the size of samples inline-formula tex-math notation=\"LaTeX\" /tex-math /inline-formula and the amount of working computers inline-formula tex-math notation=\"LaTeX\" /tex-math /inline-formula as inline-formula tex-math notation=\"LaTeX\" /tex-math /inline-formula . Furthermore, the results in this paper indicate that, for a good generalization regularized distributed kernel algorithm, the regularization parameter inline-formula tex-math notation=\"LaTeX\" /tex-math /inline-formula should be adjusted with the change of the term inline-formula tex-math notation=\"LaTeX\" /tex-math /inline-formula . These theoretic discoveries provide the useful guidance when deploying the distributed algorithms on practical big data platforms. We explore our theoretic analyses through two simulation experiments. Finally, we discuss some problems about the sufficient amount of working computers, nonequivalence, and generalization for distributed learning. We show that the rules for the computation on one single computer may not always hold for distributed learning.", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "keywords": []}
{"id": "wuSuperresolutionDomainAdaptation2022", "title": "Super-Resolution Domain Adaptation Networks for Semantic Segmentation via Pixel and Output Level Aligning", "abstract": "Recently, unsupervised domain adaptation (UDA) has attracted increasing attention to address the domain shift problem in the semantic segmentation task. Although previous UDA methods have achieved promising performance, they still suffer from the distribution gaps between source and target domains, especially the resolution discrepancy in the remote sensing images. To address this problem, this study designs a novel end-to-end semantic segmentation network, namely, Super-Resolution Domain Adaptation Network (SRDA-Net). SRDA-Net can simultaneously achieve the super-resolution task and the domain adaptation task, thus satisfying the requirement of semantic segmentation for remote sensing images, which usually involve various resolution images. The proposed SRDA-Net includes three parts: a super-resolution and segmentation (SRS) model, which focuses on recovering high-resolution image and predicting segmentation map, a pixel-level domain classifier (PDC) for determining which domain the pixel belongs to, and an output-space domain classifier (ODC) for distinguishing which domain the pixel contribution is from. By jointly optimizing SRS with two classifiers, the proposed method can not only eliminate the resolution difference between source and target domains but also improve the performance of the semantic segmentation task. Experimental results on two remote sensing datasets with different resolutions demonstrate that SRDA-Net performs favorably against some state-of-the-art methods in terms of accuracy and visual quality. Code and models are available at https://github.com/tangzhenjie/SRDA-Net .", "venue": "Frontiers in Earth Science", "keywords": ["adversarial learning", "domain adaptation", "semantic segmentation"]}
{"id": "wuSupplementalMaterialSiamDoGe", "title": "Supplemental Material for ``SiamDoGe: Domain Generalizable Semantic Segmentation Using Siamese Network''", "abstract": "", "venue": "", "keywords": []}
{"id": "xiaCMDACrossModalityDomain2023", "title": "CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation", "abstract": "Most nighttime semantic segmentation studies are based on domain adaptation approaches and image input. However, limited by the low dynamic range of conventional cameras, images fail to capture structural details and boundary information in low-light conditions. Event cameras, as a new form of vision sensors, are complementary to conventional cameras with their high dynamic range. To this end, we propose a novel unsupervised Cross-Modality Domain Adaptation (CMDA) framework to leverage multi-modality (Images and Events) information for nighttime semantic segmentation, with only labels on daytime images. In CMDA, we design the Image Motion-Extractor to extract motion information and the Image Content-Extractor to extract content information from images, in order to bridge the gap between different modalities (Images Events) and domains (Day Night). Besides, we introduce the first image-event nighttime semantic segmentation dataset. Extensive experiments on both the public image dataset and the proposed imageevent dataset demonstrate the effectiveness of our proposed approach. We open-source our code, models, and dataset at https://github.com/XiaRho/CMDA.", "venue": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)", "keywords": ["domain adaptation", "segformer", "semantic segmentation"]}
{"id": "xiaJointBilateralLearning2020", "title": "Joint Bilateral Learning for Real-time Universal Photorealistic Style Transfer", "abstract": "Photorealistic style transfer is the task of transferring the artistic style of an image onto a content target, producing a result that is plausibly taken with a camera. Recent approaches, based on deep neural networks, produce impressive results but are either too slow to run at practical resolutions, or still contain objectionable artifacts. We propose a new end-to-end model for photorealistic style transfer that is both fast and inherently generates photorealistic results. The core of our approach is a feed-forward neural network that learns local edge-aware affine transforms that automatically obey the photorealism constraint. When trained on a diverse set of images and a variety of styles, our model can robustly apply style transfer to an arbitrary pair of input images. Compared to the state of the art, our method produces visually superior results and is three orders of magnitude faster, enabling real-time performance at 4K on a mobile phone. We validate our method with ablation and user studies.", "venue": "arXiv", "keywords": ["bidirectional learning", "style transfer"]}
{"id": "xiaoModelAdaptationTest2024", "title": "Beyond Model Adaptation at Test Time: A Survey", "abstract": "Machine learning algorithms have achieved remarkable success across various disciplines, use cases and applications, under the prevailing assumption that training and test samples are drawn from the same distribution. Consequently, these algorithms struggle and become brittle even when samples in the test distribution start to deviate from the ones observed during training. Domain adaptation and domain generalization have been studied extensively as approaches to address distribution shifts across test and train domains, but each has its limitations. Test-time adaptation, a recently emerging learning paradigm, combines the benefits of domain adaptation and domain generalization by training models only on source data and adapting them to target data during test-time inference. In this survey, we provide a comprehensive and systematic review on test-time adaptation, covering more than 400 recent papers. We structure our review by categorizing existing methods into five distinct categories based on what component of the method is adjusted for test-time adaptation: the model, the inference, the normalization, the sample, or the prompt, providing detailed analysis of each. We further discuss the various preparation and adaptation settings for methods within these categories, offering deeper insights into the effective deployment for the evaluation of distribution shifts and their real-world application in understanding images, video and 3D, as well as modalities beyond vision. We close the survey with an outlook on emerging research opportunities for test-time adaptation.", "venue": "arXiv", "keywords": ["domain adaptation", "surveys", "test-time da"]}
{"id": "xiaoSampleawareRandAugment2023", "title": "Sample-Aware RandAugment", "abstract": "Automatic data augmentation (AutoDA) improves the generalization of neural networks by filling in the missing data in the target distribution. However, mainstream AutoDA methods suffer from either a time-consuming search process that sets barriers for a wide range of applications, or limited performance due to a lack of dynamic adjustments to policy during training. We propose an asymmetric search-free augmentation strategy Sample-aware RandAugment (SRA) that dynamically adjusts the augmentation policy while maintaining a simple implementation. SRA introduces a heuristic score-based module to dynamically evaluate the difficulty of the original training data, which guides the appropriate augmentation independently for each sample. SRA consists of three steps: 1) distribution exploration, 2) sample perception, and 3) distribution refinement. In a variety of settings, SRA significantly shrinks the gap between search-based and search-free AutoDA methods. The proposed method achieves 78.31% ResNet-50 Top-1 accuracy on ImageNet, which is the state-of-the-art among search-free methods. SRA can lead to simpler, more effective, and more practical AutoDA designs for diverse applications in the future.", "venue": "", "keywords": []}
{"id": "xiaoWhatShouldNot2021", "title": "What Should Not Be Contrastive in Contrastive Learning", "abstract": "Recent self-supervised contrastive methods have been able to produce impressive transferable visual representations by learning to be invariant to different data augmentations. However, these methods implicitly assume a particular set of representational invariances (e.g., invariance to color), and can perform poorly when a downstream task violates this assumption (e.g., distinguishing red vs. yellow cars). We introduce a contrastive learning framework which does not require prior knowledge of specific, task-dependent invariances. Our model learns to capture varying and invariant factors for visual representations by constructing separate embedding spaces, each of which is invariant to all but one augmentation. We use a multi-head network with a shared backbone which captures information across each augmentation and alone outperforms all baselines on downstream tasks. We further find that the concatenation of the invariant and varying spaces performs best across all tasks we investigate, including coarse-grained, fine-grained, and few-shot downstream classification tasks, and various data corruptions.", "venue": "arXiv", "keywords": ["contrastive learning", "representation learning", "self-supervised learning"]}
{"id": "xiaRealtimeLocalizedPhotorealistic2020", "title": "Real-Time Localized Photorealistic Video Style Transfer", "abstract": "We present a novel algorithm for transferring artistic styles of semantically meaningful local regions of an image onto local regions of a target video while preserving its photorealism. Local regions may be selected either fully automatically from an image, through using video segmentation algorithms, or from casual user guidance such as scribbles. Our method, based on a deep neural network architecture inspired by recent work in photorealistic style transfer, is real-time and works on arbitrary inputs without runtime optimization once trained on a diverse dataset of artistic styles. By augmenting our video dataset with noisy semantic labels and jointly optimizing over style, content, mask, and temporal losses, our method can cope with a variety of imperfections in the input and produce temporally coherent videos without visual artifacts. We demonstrate our method on a variety of style images and target videos, including the ability to transfer different styles onto multiple objects simultaneously, and smoothly transition between styles in time.", "venue": "arXiv", "keywords": ["style transfer"]}
{"id": "xieSegFormerSimpleEfficient2021", "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers", "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.", "venue": "arXiv", "keywords": ["ablation candidates", "feature pyramids", "segformer", "semantic segmentation", "transformers"]}
{"id": "xieSePiCoSemanticGuidedPixel2022", "title": "SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic Segmentation", "abstract": "Domain adaptive semantic segmentation attempts to make satisfactory dense predictions on an unlabeled target domain by utilizing the supervised model trained on a labeled source domain. In this work, we propose Semantic-Guided Pixel Contrast (SePiCo), a novel one-stage adaptation framework that highlights the semantic concepts of individual pixels to promote learning of class-discriminative and class-balanced pixel representations across domains, eventually boosting the performance of self-training methods. Specifically, to explore proper semantic concepts, we first investigate a centroid-aware pixel contrast that employs the category centroids of the entire source domain or a single source image to guide the learning of discriminative features. Considering the possible lack of category diversity in semantic concepts, we then blaze a trail of distributional perspective to involve a sufficient quantity of instances, namely distribution-aware pixel contrast, in which we approximate the true distribution of each semantic category from the statistics of labeled source data. Moreover, such an optimization objective can derive a closed-form upper bound by implicitly involving an infinite number of (dis)similar pairs, making it computationally efficient. Extensive experiments show that SePiCo not only helps stabilize training but also yields discriminative representations, making significant progress on both synthetic-to-real and daytime-to-nighttime adaptation scenarios.", "venue": "arXiv.org", "keywords": ["contrastive learning", "domain adaptation", "highly-analytical", "loss functions", "segformer", "self-training", "semantic segmentation"]}
{"id": "xingAssocABSiASjB", "title": "Z Assoc(A,B) = i A j B Wij", "abstract": "", "venue": "", "keywords": ["affinity modeling", "deep clustering"]}
{"id": "xiOnlineUnsupervisedVideo2024", "title": "Online Unsupervised Video Object Segmentation via Contrastive Motion Clustering", "abstract": "Online unsupervised video object segmentation (UVOS) uses the previous frames as its input to automatically separate the primary object(s) from a streaming video without using any further manual annotation. A major challenge is that the model has no access to the future and must rely solely on the history, i.e., the segmentation mask is predicted from the current frame as soon as it is captured. In this work, a novel contrastive motion clustering algorithm with an optical flow as its input is proposed for the online UVOS by exploiting the common fate principle that visual elements tend to be perceived as a group if they possess the same motion pattern. We build a simple and effective auto-encoder to iteratively summarize non-learnable prototypical bases for the motion pattern, while the bases in turn help learn the representation of the embedding network. Further, a contrastive learning strategy based on a boundary prior is developed to improve foreground and background feature discrimination in the representation learning stage. The proposed algorithm can be optimized on arbitrarily-scale data (i.e., frame, clip, dataset) and performed in an online fashion. Experiments on DAVIS16, FBMS, and SegTrackV2 datasets show that the accuracy of our method surpasses the previous state-of-the-art (SoTA) online UVOS method by a margin of 0.8%, 2.9%, and 1.1%, respectively. Furthermore, by using an online deep subspace clustering to tackle the motion grouping, our method is able to achieve higher accuracy at 3 faster inference time compared to SoTA online UVOS method, and making a good trade-off between effectiveness and efficiency.", "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "keywords": ["clustering", "contrastive learning", "online da", "semantic segmentation", "temporal consistency", "unsupervised learning"]}
{"id": "XMOL", "title": "X-MOL", "abstract": "", "venue": "x-mol.net", "keywords": []}
{"id": "xuAuxiliaryTasksEnhanced2024", "title": "Auxiliary Tasks Enhanced Dual-affinity Learning for Weakly Supervised Semantic Segmentation", "abstract": "Most existing weakly supervised semantic segmentation (WSSS) methods rely on Class Activation Mapping (CAM) to extract coarse class-specific localization maps using image-level labels. Prior works have commonly used an off-line heuristic thresholding process that combines the CAM maps with off-the-shelf saliency maps produced by a general pre-trained saliency model to produce more accurate pseudo-segmentation labels. We propose AuxSegNet+, a weakly supervised auxiliary learning framework to explore the rich information from these saliency maps and the significant inter-task correlation between saliency detection and semantic segmentation. In the proposed AuxSegNet+, saliency detection and multi-label image classification are used as auxiliary tasks to improve the primary task of semantic segmentation with only image-level ground-truth labels. We also propose a cross-task affinity learning mechanism to learn pixel-level affinities from the saliency and segmentation feature maps. In particular, we propose a cross-task dual-affinity learning module to learn both pairwise and unary affinities, which are used to enhance the task-specific features and predictions by aggregating both query-dependent and query-independent global context for both saliency detection and semantic segmentation. The learned cross-task pairwise affinity can also be used to refine and propagate CAM maps to provide better pseudo labels for both tasks. Iterative improvement of segmentation performance is enabled by cross-task affinity learning and pseudo-label updating. Extensive experiments demonstrate the effectiveness of the proposed approach with new state-of-the-art WSSS results on the challenging PASCAL VOC and MS COCO benchmarks.", "venue": "arXiv", "keywords": ["affinity modeling", "promising", "semantic segmentation", "semi-supervised learning", "siamese networks", "visualizations"]}
{"id": "xuComprehensiveOverviewFishEye2023", "title": "A Comprehensive Overview of Fish-Eye Camera Distortion Correction Methods", "abstract": "The fisheye camera, with its unique wide field of view and other characteristics, has found extensive applications in various fields. However, the fisheye camera suffers from significant distortion compared to pinhole cameras, resulting in distorted images of captured objects. Fish-eye camera distortion is a common issue in digital image processing, requiring effective correction techniques to enhance image quality. This review provides a comprehensive overview of various methods used for fish-eye camera distortion correction. The article explores the polynomial distortion model, which utilizes polynomial functions to model and correct radial distortions. Additionally, alternative approaches such as panorama mapping, grid mapping, direct methods, and deep learning-based methods are discussed. The review highlights the advantages, limitations, and recent advancements of each method, enabling readers to make informed decisions based on their specific needs.", "venue": "arXiv", "keywords": ["fisheye distortion", "homography estimation", "surveys"]}
{"id": "xuComprehensiveSurveyImage2023", "title": "A Comprehensive Survey of Image Augmentation Techniques for Deep Learning", "abstract": "", "venue": "Pattern Recognition", "keywords": ["surveys"]}
{"id": "xuCrossDomainImageClassification2019", "title": "Cross-Domain Image Classification through Neural-Style Transfer Data Augmentation", "abstract": "In particular, the lack of sufficient amounts of domain-specific data can reduce the accuracy of a classifier. In this paper, we explore the effects of style transfer-based data transformation on the accuracy of a convolutional neural network classifiers in the context of automobile detection under adverse winter weather conditions. The detection of automobiles under highly adverse weather conditions is a difficult task as such conditions present large amounts of noise in each image. The InceptionV2 architecture is trained on a composite dataset, consisting of either normal car image dataset , a mixture of normal and style transferred car images, or a mixture of normal car images and those taken at blizzard conditions, at a ratio of 80:20. All three classifiers are then tested on a dataset of car images taken at blizzard conditions and on vehicle-free snow landscape images. We evaluate and contrast the effectiveness of each classifier upon each dataset, and discuss the strengths and weaknesses of style-transfer based approaches to data augmentation.", "venue": "arXiv", "keywords": ["style transfer", "synthetic weather"]}
{"id": "xuDIRLDomainInvariantRepresentation2022", "title": "DIRL: Domain-Invariant Representation Learning for Generalizable Semantic Segmentation", "abstract": "Model generalization to the unseen scenes is crucial to realworld applications, such as autonomous driving, which requires robust vision systems. To enhance the model generalization, domain generalization through learning the domaininvariant representation has been widely studied. However, most existing works learn the shared feature space within multi-source domains but ignore the characteristic of the feature itself (e.g., the feature sensitivity to the domain-specific style). Therefore, we propose the Domain-invariant Representation Learning (DIRL) for domain generalization which utilizes the feature sensitivity as the feature prior to guide the enhancement of the model generalization capability. The guidance reflects in two folds: 1) Feature re-calibration that introduces the Prior Guided Attention Module (PGAM) to emphasize the insensitive features and suppress the sensitive features. 2): Feature whiting that proposes the Guided Feature Whiting (GFW) to remove the feature correlations which are sensitive to the domain-specific style. We construct the domain-invariant representation which suppresses the effect of the domain-specific style on the quality and correlation of the features. As a result, our method is simple yet effective, and can enhance the robustness of various backbone networks with little computational cost. Extensive experiments over multiple domains generalizable segmentation tasks show the superiority of our approach to other methods.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "keywords": ["domain generalization", "representation learning", "semantic segmentation"]}
{"id": "xueImageStitchingAugmentation2024", "title": "Image Stitching via Augmentation Selection and Comprehensive Optimization of Homography Models", "abstract": "The naturalness of prominent objects is important information in image stitching research, which directly affects the satisfaction of electronic consumers when acquiring large scenes. Most current image stitching algorithms use the best homography model for registration features, which usually cannot guarantee the integrity of static and dynamic objects. Aiming to alleviate the problem, we propose an image stitching method in this paper. Specifically, it consists of two parts, augmentation selection and comprehensive optimization of homography models. We first obtain multiple homography models and further refine the model results by cluster analysis to form a reasonable homography candidate set, next the most reasonable homography model and best stitching result can be obtained with the maximal evaluation value. Then, mesh optimization and multiple matching are performed on each model to eliminate artifacts, while optimal seams are optimized to reduce distortion. Extensive experimental results show that our method improves the eval scores by 10.15%, and reaches superior performance over integrity, primitiveness, and uniqueness on both static and dynamic markers.", "venue": "IEEE Transactions on Consumer Electronics", "keywords": ["homography estimation"]}
{"id": "xuHowNeuralNetworks2020", "title": "How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks", "abstract": "We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) -- structured networks with MLP modules -- have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently \"diverse\". Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings.", "venue": "arXiv.org", "keywords": ["critical citation", "distribution estimation", "feature engineering"]}
{"id": "xuLearningDynamicStyle2023", "title": "Learning Dynamic Style Kernels for Artistic Style Transfer", "abstract": "Arbitrary style transfer has been demonstrated to be efficient in artistic image generation. Previous methods either globally modulate the content feature ignoring local details, or overly focus on the local structure details leading to style leakage. In contrast to the literature, we propose a new scheme \\``style kernel\"\\ that learns \\ spatially adaptive kernels\\ for per-pixel stylization, where the convolutional kernels are dynamically generated from the global style-content aligned feature and then the learned kernels are applied to modulate the content feature at each spatial position. This new scheme allows flexible both global and local interactions between the content and style features such that the wanted styles can be easily transferred to the content image while at the same time the content structure can be easily preserved. To further enhance the flexibility of our style transfer method, we propose a Style Alignment Encoding (SAE) module complemented with a Content-based Gating Modulation (CGM) module for learning the dynamic style kernels in focusing regions. Extensive experiments strongly demonstrate that our proposed method outperforms state-of-the-art methods and exhibits superior performance in terms of visual quality and efficiency.", "venue": "arXiv", "keywords": ["kernel methods", "style transfer"]}
{"id": "xuMaskPlusImprovingMask2020", "title": "MaskPlus: Improving Mask Generation for Instance Segmentation", "abstract": "Instance segmentation is a promising yet challenging topic in computer vision. Recent approaches such as Mask R-CNN typically divide this problem into two parts -- a detection component and a mask generation branch, and mostly focus on the improvement of the detection part. In this paper, we present an approach that extends Mask RCNN with five novel techniques for improving the mask generation branch and reducing the conflicts between the mask branch and the detection component in training. These five techniques are independent to each other and can be flexibly utilized in building various instance segmentation architectures for increasing the overall accuracy. We demonstrate the effectiveness of our approach with tests on the COCO dataset.", "venue": "2020 IEEE Winter Conference on Applications of Computer Vision (WACV)", "keywords": []}
{"id": "xuSignalProcessingImplicit2022", "title": "Signal Processing for Implicit Neural Representations", "abstract": "", "venue": "Advances in Neural Information Processing Systems", "keywords": []}
{"id": "xuUniversalAdaptiveData2023", "title": "Universal Adaptive Data Augmentation", "abstract": "Existing automatic data augmentation (DA) methods either ignore updating DA's parameters according to the target model's state during training or adopt update strategies that are not effective enough. In this work, we design a novel data augmentation strategy called ``Universal Adaptive Data Augmentation'' (UADA). Different from existing methods, UADA would adaptively update DA's parameters according to the target model's gradient information during training: given a pre-defined set of DA operations, we randomly decide types and magnitudes of DA operations for every data batch during training, and adaptively update DA's parameters along the gradient direction of the loss concerning DA's parameters. In this way, UADA can increase the training loss of the target networks, and the target networks would learn features from harder samples to improve the generalization. Moreover, UADA is very general and can be utilized in numerous tasks, e.g., image classification, semantic segmentation and object detection. Extensive experiments with various models are conducted on CIFAR-10, CIFAR100, ImageNet, tiny-ImageNet, Cityscapes, and VOC07+12 to prove the significant performance improvements brought by our proposed adaptive augmentation.", "venue": "arXiv", "keywords": []}
{"id": "xuUniversalAdaptiveData2023a", "title": "Universal Adaptive Data Augmentation", "abstract": "Existing automatic data augmentation (DA) methods either ignore updating DA's parameters according to the target model's state during training or adopt update strategies that are not effective enough. In this work, we design a novel data augmentation strategy called ``Universal Adaptive Data Augmentation'' (UADA). Different from existing methods, UADA would adaptively update DA's parameters according to the target model's gradient information during training: given a pre-defined set of DA operations, we randomly decide types and magnitudes of DA operations for every data batch during training, and adaptively update DA's parameters along the gradient direction of the loss concerning DA's parameters. In this way, UADA can increase the training loss of the target networks, and the target networks would learn features from harder samples to improve the generalization. Moreover, UADA is very general and can be utilized in numerous tasks, e.g., image classification, semantic segmentation and object detection. Extensive experiments with various models are conducted on CIFAR-10, CIFAR100, ImageNet, tiny-ImageNet, Cityscapes, and VOC07+12 to prove the significant performance improvements brought by our proposed adaptive augmentation.", "venue": "arXiv", "keywords": []}
{"id": "xuUniversalAdaptiveData2023b", "title": "Universal Adaptive Data Augmentation", "abstract": "Existing automatic data augmentation (DA) methods either ignore updating DA's parameters according to the target model's state during training or adopt update strategies that are not effective enough. In this work, we design a novel data augmentation strategy called \"Universal Adaptive Data Augmentation\" (UADA). Different from existing methods, UADA would adaptively update DA's parameters according to the target model's gradient information during training: given a pre-defined set of DA operations, we randomly decide types and magnitudes of DA operations for every data batch during training, and adaptively update DA's parameters along the gradient direction of the loss concerning DA's parameters. In this way, UADA can increase the training loss of the target networks, and the target networks would learn features from harder samples to improve the generalization. Moreover, UADA is very general and can be utilized in numerous tasks, e.g., image classification, semantic segmentation and object detection. Extensive experiments with various models are conducted on CIFAR-10, CIFAR-100, ImageNet, tiny-ImageNet, Cityscapes, and VOC07+12 to prove the significant performance improvements brought by UADA.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "yamashitaLearningDomainagnosticVisual2021", "title": "Learning Domain-Agnostic Visual Representation for Computational Pathology Using Medically-Irrelevant Style Transfer Augmentation", "abstract": "Suboptimal generalization of machine learning models on unseen data is a key challenge which hampers the clinical applicability of such models to medical imaging. Although various methods such as domain adaptation and domain generalization have evolved to combat this challenge, learning robust and generalizable representations is core to medical image understanding, and continues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation for histoPathology), a form of data augmentation based on random style transfer from non-medical style source such as artistic paintings, for learning domain-agnostic visual representations in computational pathology. Style transfer replaces the low-level texture content of an image with the uninformative style of randomly selected style source image, while preserving the original high-level semantic content. This improves robustness to domain shift and can be used as a simple yet powerful tool for learning domain-agnostic representations. We demonstrate that STRAP leads to state-of-the-art performance, particularly in the presence of domain shifts, on two particular classification tasks in computational pathology.", "venue": "IEEE Transactions on Medical Imaging", "keywords": ["representation learning", "style transfer"]}
{"id": "yangAdaAugmentTuningFreeAdaptive2024", "title": "AdaAugment: A Tuning-Free and Adaptive Approach to Enhance Data Augmentation", "abstract": "Data augmentation (DA) is widely employed to improve the generalization performance of deep models. However, most existing DA methods use augmentation operations with random magnitudes throughout training. While this fosters diversity, it can also inevitably introduce uncontrolled variability in augmented data, which may cause misalignment with the evolving training status of the target models. Both theoretical and empirical findings suggest that this misalignment increases the risks of underfitting and overfitting. To address these limitations, we propose AdaAugment, an innovative and tuning-free Adaptive Augmentation method that utilizes reinforcement learning to dynamically adjust augmentation magnitudes for individual training samples based on real-time feedback from the target network. Specifically, AdaAugment features a dual-model architecture consisting of a policy network and a target network, which are jointly optimized to effectively adapt augmentation magnitudes. The policy network optimizes the variability within the augmented data, while the target network utilizes the adaptively augmented samples for training. Extensive experiments across benchmark datasets and deep architectures demonstrate that AdaAugment consistently outperforms other state-of-the-art DA methods in effectiveness while maintaining remarkable efficiency.", "venue": "arXiv", "keywords": []}
{"id": "yangChangeHardCloser2023", "title": "Change Is Hard: A Closer Look at Subpopulation Shift", "abstract": "Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly effective even without any group information. Finally, unlike existing works that solely aim to improve worst-group accuracy (WGA), we demonstrate the fundamental tradeoff between WGA and other important metrics, highlighting the need to carefully choose testing metrics. Code and data are available at: https://github.com/YyzHarry/SubpopBench.", "venue": "arXiv", "keywords": ["bias sources", "generalization quantification", "surveys"]}
{"id": "yangComputationOffloadingMultiAccess2020", "title": "Computation Offloading in Multi-Access Edge Computing Networks: A Multi-Task Learning Approach", "abstract": "Multi-access edge computing (MEC) has already shown the potential in enabling mobile devices to bear the computation-intensive applications by offloading some tasks to a nearby access point (AP) integrated with a MEC server (MES). However, due to the varying network conditions and limited computation resources of the MES, the offloading decisions taken by a mobile device and the computational resources allocated by the MES may not be efficiently achieved with the lowest cost. In this paper, we propose a dynamic offloading framework for the MEC network, in which the uplink non-orthogonal multiple access (NOMA) is used to enable multiple devices to upload their tasks via the same frequency band. We formulate the offloading decision problem as a multiclass classification problem and formulate the MES computational resource allocation problem as a regression problem. Then a multi-task learning based feedforward neural network (MTFNN) model is designed to jointly optimize the offloading decision and computational resource allocation. Numerical results illustrate that the proposed MTFNN outperforms the conventional optimization method in terms of inference accuracy and computation complexity.", "venue": "arXiv", "keywords": ["multi-task learning"]}
{"id": "yangComputationOffloadingMultiAccess2021", "title": "Computation Offloading in Multi-Access Edge Computing: A Multi-Task Learning Approach", "abstract": "Multi-access edge computing (MEC) has already shown great potential in enabling mobile devices to bear the computation-intensive applications by offloading some computing jobs to a nearby access point (AP) integrated with a MEC server (MES). However, due to the varying network conditions and limited computational resources of the MES, the offloading decisions taken by a mobile device and the computational resources allocated by the MES can be formulated as a mixed-integer nonlinear programming (MINLP) problem, which may not be optimized with the lowest cost. In this paper, we propose a novel offloading framework for the multi-server MEC network where each AP is equipped with an MES assisting mobile users (MUs) in executing computation-intensive jobs via offloading. Specifically, we formulate the offloading decision problem as a multiclass classification problem and formulate the MES computational resource allocation problem as a regression problem. Then a multi-task learning based feedforward neural network (MTFNN) model is designed and trained to jointly optimize the offloading decision and computational resource allocation. Numerical results show that the proposed MTFNN outperforms the conventional optimization method in terms of inference accuracy and computational complexity.", "venue": "IEEE Transactions on Mobile Computing", "keywords": []}
{"id": "yangFDAFourierDomain2020", "title": "FDA: Fourier Domain Adaptation for Semantic Segmentation", "abstract": "We describe a simple method for unsupervised domain adaptation, whereby the discrepancy between the source and target distributions is reduced by swapping the low-frequency spectrum of one with the other. We illustrate the method in semantic segmentation, where densely annotated images are aplenty in one domain (synthetic data), but difficult to obtain in another (real images). Current state-of-the-art methods are complex, some requiring adversarial optimization to render the backbone of a neural network invariant to the discrete domain selection variable. Our method does not require any training to perform the domain alignment, just a simple Fourier Transform and its inverse. Despite its simplicity, it achieves state-of-the-art performance in the current benchmarks, when integrated into a relatively standard semantic segmentation model. Our results indicate that even simple procedures can discount nuisance variability in the data that more sophisticated methods struggle to learn away.", "venue": "arXiv", "keywords": ["domain adaptation", "promising", "semantic segmentation", "spectral methods"]}
{"id": "yangFedSiamDADualaggregatedFederated2022", "title": "FedSiam-DA: Dual-aggregated Federated Learning via Siamese Network under Non-IID Data", "abstract": "Federated learning is a distributed learning that allows each client to keep the original data locally and only upload the parameters of the local model to the server. Despite federated learning can address data island, it remains challenging to train with data heterogeneous in a real application. In this paper, we propose FedSiam-DA, a novel dual-aggregated contrastive federated learning approach, to personalize both local and global models, under various settings of data heterogeneity. Firstly, based on the idea of contrastive learning in the siamese network, FedSiam-DA regards the local and global model as different branches of the siamese network during the local training and controls the update direction of the model by constantly changing model similarity to personalize the local model. Secondly, FedSiam-DA introduces dynamic weights based on model similarity for each local model and exercises the dual-aggregated mechanism to further improve the generalization of the global model. Moreover, we provide extensive experiments on benchmark datasets, the results demonstrate that FedSiam-DA achieves outperforming several previous FL approaches on heterogeneous datasets.", "venue": "arXiv", "keywords": ["federated learning", "siamese networks"]}
{"id": "yangFishFormerAnnulusSlicingbased2022", "title": "FishFormer: Annulus Slicing-based Transformer for Fisheye Rectification with Efficacy Domain Exploration", "abstract": "Numerous significant progress on fisheye image rectification has been achieved through CNN. Nevertheless, constrained by a fixed receptive field, the global distribution and the local symmetry of the distortion have not been fully exploited. To leverage these two characteristics, we introduced Fishformer that processes the fisheye image as a sequence to enhance global and local perception. We tuned the Transformer according to the structural properties of fisheye images. First, the uneven distortion distribution in patches generated by the existing square slicing method confuses the network, resulting in difficult training. Therefore, we propose an annulus slicing method to maintain the consistency of the distortion in each patch, thus perceiving the distortion distribution well. Second, we analyze that different distortion parameters have their own efficacy domains. Hence, the perception of the local area is as important as the global, but Transformer has a weakness for local texture perception. Therefore, we propose a novel layer attention mechanism to enhance the local perception and texture transfer. Our network simultaneously implements global perception and focused local perception decided by the different parameters. Extensive experiments demonstrate that our method provides superior performance compared with state-of-the-art methods.", "venue": "arXiv", "keywords": ["critical citation", "fisheye distortion", "promising", "transformers"]}
{"id": "yangFocalAttentionLongRange", "title": "Focal Attention for Long-Range Interactions in Vision Transformers", "abstract": "Recently, Vision Transformer and its variants have shown great promise on various computer vision tasks. The ability of capturing local and global visual dependencies through self-attention is the key to its success. However, this also brings challenges due to quadratic computational overhead, especially for the high-resolution vision tasks (e.g., object detection). Many recent works have attempted to reduce the cost and improve model performance by applying either coarse-grained global attention or fine-grained local attention. However, both approaches cripple the modeling power of the original self-attention mechanism of multi-layer Transformers, leading to sub-optimal solutions. In this paper, we present focal attention, a new attention mechanism that incorporates both fine-grained local and coarse-grained global interactions. In this new mechanism, each token attends its closest surrounding tokens at fine granularity and the tokens far away at coarse granularity, and thus can capture both short- and long-range visual dependencies efficiently and effectively. With focal attention, we build a new variant of Vision Transformer models, called Focal Transformers, which achieve superior performance over the state-of-theart (SoTA) Vision Transformers on a range of public image classification and object detection benchmarks. In particular, our Focal Transformer models with a moderate size of 51.1M and a large size of 89.8M achieve 83.6% and 84.0% Top-1 accuracy, respectively, on ImageNet classification at 224 224. When employed as the backbones, Focal Transformers achieve consistent and substantial improvements over the current SoTA Swin Transformers across 6 different object detection methods. Our largest Focal Transformer yields 58.7/59.0 box mAPs and 50.9/51.3 mask mAPs on COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation, creating new SoTA on three of the most challenging computer vision tasks. Our code is available at: https://github. com/microsoft/Focal-Transformer.", "venue": "", "keywords": ["transformers"]}
{"id": "yangGeneralizedSemanticSegmentation2023", "title": "Generalized Semantic Segmentation by Self-Supervised Source Domain Projection and Multi-Level Contrastive Learning", "abstract": "Deep networks trained on the source domain show degraded performance when tested on unseen target domain data. To enhance the model's generalization ability, most existing domain generalization methods learn domain invariant features by suppressing domain sensitive features. Different from them, we propose a Domain Projection and Contrastive Learning (DPCL) approach for generalized semantic segmentation, which includes two modules: Self-supervised Source Domain Projection (SSDP) and Multi-level Contrastive Learning (MLCL). SSDP aims to reduce domain gap by projecting data to the source domain, while MLCL is a learning scheme to learn discriminative and generalizable features on the projected data. During test time, we first project the target data by SSDP to mitigate domain shift, then generate the segmentation results by the learned segmentation network based on MLCL. At test time, we can update the projected data by minimizing our proposed pixel-to-pixel contrastive loss to obtain better results. Extensive experiments for semantic segmentation demonstrate the favorable generalization capability of our method on benchmark datasets.", "venue": "arXiv", "keywords": ["contrastive learning", "domain projection", "self-supervised learning", "semantic segmentation"]}
{"id": "yangImageDataAugmentation2023", "title": "Image Data Augmentation for Deep Learning: A Survey", "abstract": "Deep learning has achieved remarkable results in many computer vision tasks. Deep neural networks typically rely on large amounts of training data to avoid overfitting. However, labeled data for real-world applications may be limited. By improving the quantity and diversity of training data, data augmentation has become an inevitable part of deep learning model training with image data. As an effective way to improve the sufficiency and diversity of training data, data augmentation has become a necessary part of successful application of deep learning models on image data. In this paper, we systematically review different image data augmentation methods. We propose a taxonomy of reviewed methods and present the strengths and limitations of these methods. We also conduct extensive experiments with various data augmentation methods on three typical computer vision tasks, including semantic segmentation, image classification and object detection. Finally, we discuss current challenges faced by data augmentation and future research directions to put forward some useful research guidance.", "venue": "arXiv", "keywords": ["feature-level augmentation", "gans", "mixture augmentations", "surveys", "traditional augmentation"]}
{"id": "yangLabelDrivenReconstructionDomain2020", "title": "Label-Driven Reconstruction for Domain Adaptation in Semantic Segmentation", "abstract": "Unsupervised domain adaptation enables to alleviate the need for pixel-wise annotation in the semantic segmentation. One of the most common strategies is to translate images from the source domain to the target domain and then align their marginal distributions in the feature space using adversarial learning. However, source-to-target translation enlarges the bias in translated images and introduces extra computations, owing to the dominant data size of the source domain. Furthermore, consistency of the joint distribution in source and target domains cannot be guaranteed through global feature alignment. Here, we present an innovative framework, designed to mitigate the image translation bias and align cross-domain features with the same category. This is achieved by 1) performing the target-to-source translation and 2) reconstructing both source and target images from their predicted labels. Extensive experiments on adapting from synthetic to real urban scene understanding demonstrate that our framework competes favorably against existing state-of-the-art methods.", "venue": "Computer Vision -- ECCV 2020", "keywords": ["domain adaptation", "feature-level augmentation", "gans", "image-to-image", "semantic segmentation", "sim-to-real", "unsupervised da"]}
{"id": "yangSemanticFiltering2016", "title": "Semantic Filtering", "abstract": "", "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "keywords": ["filter augmentations"]}
{"id": "yangSurveyAutomatedData2023", "title": "A Survey of Automated Data Augmentation Algorithms for Deep Learning-Based Image Classification Tasks", "abstract": "In recent years, one of the most popular techniques in the computer vision community has been the deep learning technique. As a data-driven technique, deep model requires enormous amounts of accurately labelled training data, which is often inaccessible in many real-world applications. A data-space solution is Data Augmentation (DA), that can artificially generate new images out of original samples. Image augmentation strategies can vary by dataset, as different data types might require different augmentations to facilitate model training. However, the design of DA policies has been largely decided by the human experts with domain knowledge, which is considered to be highly subjective and error-prone. To mitigate such problem, a novel direction is to automatically learn the image augmentation policies from the given dataset using Automated Data Augmentation (AutoDA) techniques. The goal of AutoDA models is to find the optimal DA policies that can maximize the model performance gains. This survey discusses the underlying reasons of the emergence of AutoDA technology from the perspective of image classification. We identify three key components of a standard AutoDA model: a search space, a search algorithm and an evaluation function. Based on their architecture, we provide a systematic taxonomy of existing image AutoDA approaches. This paper presents the major works in AutoDA field, discussing their pros and cons, and proposing several potential directions for future improvements.", "venue": "Knowledge and Information Systems", "keywords": ["auto-augmentation policies", "semantic segmentation", "surveys"]}
{"id": "yangTestTimeAdaptation2024", "title": "Towards Test Time Adaptation via Calibrated Entropy Minimization", "abstract": "Robust models must demonstrate strong generalizability, even amid environmental changes. However, the complex variability and noise in real-world data often lead to a pronounced performance gap between the training and testing phases. Researchers have recently introduced test-time-domain adaptation (TTA) to address this challenge. TTA methods primarily adapt source-pretrained models to a target domain using only unlabeled test data. This study found that existing TTA methods consider only the largest logit as a pseudo-label and aim to minimize the entropy of test time predictions. This maximizes the predictive confidence of the model. However, this corresponds to the model being overconfident in the local test scenarios. In response, we introduce a novel confidence-calibration loss function called Calibrated Entropy Test-Time Adaptation (CETA), which considers the model's largest logit and the next-highest-ranked one, aiming to strike a balance between overconfidence and underconfidence. This was achieved by incorporating a sample-wise regularization term. We also provide a theoretical foundation for the proposed loss function. Experimentally, our method outperformed existing strategies on benchmark corruption datasets across multiple models, underscoring the efficacy of our approach.", "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "keywords": ["domain adaptation", "entropic optimization", "loss functions", "test-time da"]}
{"id": "yanVisualizingInvisibleOccluded2019", "title": "Visualizing the Invisible: Occluded Vehicle Segmentation and Recovery", "abstract": "In this paper, we propose a novel iterative multi-task framework to complete the segmentation mask of an occluded vehicle and recover the appearance of its invisible parts. In particular, to improve the quality of the segmentation completion, we present two coupled discriminators and introduce an auxiliary 3D model pool for sampling authentic silhouettes as adversarial samples. In addition, we propose a two-path structure with a shared network to enhance the appearance recovery capability. By iteratively performing the segmentation completion and the appearance recovery, the results will be progressively refined. To evaluate our method, we present a dataset, the Occluded Vehicle dataset, containing synthetic and real-world occluded vehicle images. We conduct comparison experiments on this dataset and demonstrate that our model outperforms the state-of-theart in tasks of recovering segmentation mask and appearance for occluded vehicles. Moreover, we also demonstrate that our appearance recovery approach can benefit the occluded vehicle tracking in real-world videos.", "venue": "arXiv", "keywords": ["image inpainting", "image-to-image"]}
{"id": "yeDecentralizedMultiTaskLearning2019", "title": "Decentralized Multi-Task Learning Based on Extreme Learning Machines", "abstract": "In multi-task learning (MTL), related tasks learn jointly to improve generalization performance. To exploit the high learning speed of extreme learning machines (ELMs), we apply the ELM framework to the MTL problem, where the output weights of ELMs for all the tasks are learned collaboratively. We first present the ELM based MTL problem in the centralized setting, which is solved by the proposed MTL-ELM algorithm. Due to the fact that many data sets of different tasks are geo-distributed, decentralized machine learning is studied. We formulate the decentralized MTL problem based on ELM as majorized multi-block optimization with coupled bi-convex objective functions. To solve the problem, we propose the DMTL-ELM algorithm, which is a hybrid Jacobian and Gauss-Seidel Proximal multi-block alternating direction method of multipliers (ADMM). Further, to reduce the computation load of DMTL-ELM, DMTL-ELM with first-order approximation (FO-DMTL-ELM) is presented. Theoretical analysis shows that the convergence to the stationary point of DMTL-ELM and FO-DMTL-ELM can be guaranteed conditionally. Through simulations, we demonstrate the convergence of proposed MTL-ELM, DMTL-ELM, and FO-DMTL-ELM algorithms, and also show that they can outperform existing MTL methods. Moreover, by adjusting the dimension of hidden feature space, there exists a trade-off between communication load and learning accuracy for DMTL-ELM.", "venue": "arXiv", "keywords": ["multi-task learning"]}
{"id": "yeOutofDistributionGeneralizationAnalysis2021", "title": "Out-of-Distribution Generalization Analysis via Influence Function", "abstract": "The mismatch between training and target data is one major challenge for current machine learning systems. When training data is collected from multiple domains and the target domains include all training domains and other new domains, we are facing an Out-of-Distribution (OOD) generalization problem that aims to find a model with the best OOD accuracy. One of the definitions of OOD accuracy is worst-domain accuracy. In general, the set of target domains is unknown, and the worst over target domains may be unseen when the number of observed domains is limited. In this paper, we show that the worst accuracy over the observed domains may dramatically fail to identify the OOD accuracy. To this end, we introduce Influence Function, a classical tool from robust statistics, into the OOD generalization problem and suggest the variance of influence function to monitor the stability of a model on training domains. We show that the accuracy on test domains and the proposed index together can help us discern whether OOD algorithms are needed and whether a model achieves good OOD generalization.", "venue": "arXiv", "keywords": ["domain generalization", "generalization quantification", "mutual information measures", "promising", "risk optimization"]}
{"id": "yeRandomizedNeuralNetworks2021", "title": "Randomized Neural Networks Based Decentralized Multi-Task Learning via Hybrid Multi-Block ADMM", "abstract": "In multi-task learning (MTL), related tasks learn jointly to improve generalization performance. To exploit the high learning speed of feed-forward neural networks (FNN), we apply the randomized single-hidden layer FNN (RSF) to the MTL problem, where the output weights of RSFs for all the tasks are learned collaboratively. We first present the RSF based MTL problem in the centralized setting, which is solved by the proposed MTL-RSF algorithm. Due to the fact that many data sets of different tasks are geo-distributed, decentralized machine learning is studied. We formulate the decentralized MTL problem based on RSF as majorized multi-block optimization with coupled bi-convex objective functions. To solve the problem, we propose the DMTL-RSF algorithm, which is a hybrid Jacobian and Gauss-Seidel Proximal multi-block alternating direction method of multipliers (ADMM). Further, to reduce the computation load of DMTL-RSF, DMTL-RSF with first-order approximation (FO-DMTL-RSF) is presented. Theoretical analysis shows that the convergence to the stationary point of proposed decentralized algorithms can be guaranteed conditionally. Through simulations, we demonstrate the convergence of presented algorithms, and also show that they can outperform existing MTL methods. Moreover, by adjusting the dimension of hidden feature space, there exists a trade-off between communication load and learning accuracy for DMTL-RSF.", "venue": "IEEE Transactions on Signal Processing", "keywords": ["multi-task learning"]}
{"id": "yeTheoreticalFrameworkOutofDistribution2021", "title": "Towards a Theoretical Framework of Out-of-Distribution Generalization", "abstract": "Generalization to out-of-distribution (OOD) data is one of the central problems in modern machine learning. Recently, there is a surge of attempts to propose algorithms that mainly build upon the idea of extracting invariant features. Although intuitively reasonable, theoretical understanding of what kind of invariance can guarantee OOD generalization is still limited, and generalization to arbitrary out-of-distribution is clearly impossible. In this work, we take the first step towards rigorous and quantitative definitions of 1) what is OOD; and 2) what does it mean by saying an OOD problem is learnable. We also introduce a new concept of expansion function, which characterizes to what extent the variance is amplified in the test domains over the training domains, and therefore give a quantitative meaning of invariant features. Based on these, we prove OOD generalization error bounds. It turns out that OOD generalization largely depends on the expansion function. As recently pointed out by Gulrajani and Lopez-Paz (2020), any OOD learning algorithm without a model selection module is incomplete. Our theory naturally induces a model selection criterion. Extensive experiments on benchmark OOD datasets demonstrate that our model selection criterion has a significant advantage over baselines.", "venue": "arXiv.org", "keywords": ["generalization certification", "generalization quantification", "highly-analytical", "promising", "surveys"]}
{"id": "yeUniversalSemanticSegmentation2020", "title": "Universal Semantic Segmentation for Fisheye Urban Driving Images", "abstract": "Semantic segmentation is a critical method in the field of autonomous driving. When performing semantic image segmentation, a wider field of view (FoV) helps to obtain more information about the surrounding environment, making automatic driving safer and more reliable, which could be offered by fisheye cameras. However, large public fisheye datasets are not available, and the fisheye images captured by the fisheye camera with large FoV comes with large distortion, so commonly-used semantic segmentation model cannot be directly utilized. In this paper, a seven degrees of freedom (DoF) augmentation method is proposed to transform rectilinear image to fisheye image in a more comprehensive way. In the training process, rectilinear images are transformed into fisheye images in seven DoF, which simulates the fisheye images taken by cameras of different positions, orientations and focal lengths. The result shows that training with the seven-DoF augmentation can improve the model's accuracy and robustness against different distorted fisheye data. This seven-DoF augmentation provides a universal semantic segmentation solution for fisheye cameras in different autonomous driving applications. Also, we provide specific parameter settings of the augmentation for autonomous driving. At last, we tested our universal semantic segmentation model on real fisheye images and obtained satisfactory results. The code and configurations are released at https://github.com/Yaozhuwa/FisheyeSeg.", "venue": "arXiv", "keywords": ["fisheye distortion", "semantic segmentation"]}
{"id": "yogamaniWoodScapeMultiTaskMultiCamera2019", "title": "WoodScape: A Multi-Task, Multi-Camera Fisheye Dataset for Autonomous Driving", "abstract": "Lidar 3D View Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of their prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images. We release the first extensive fisheye automotive dataset, WoodScape, named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images. With WoodScape, we would like to encourage the community to adapt computer vision models for fisheye camera instead of using na rectification.", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "keywords": ["dataset debut", "fisheye distortion"]}
{"id": "yooHierarchicalSpatiotemporalTransformers2023", "title": "Hierarchical Spatiotemporal Transformers for Video Object Segmentation", "abstract": "This paper presents a novel framework called HST for semi-supervised video object segmentation (VOS). HST extracts image and video features using the latest Swin Transformer and Video Swin Transformer to inherit their inductive bias for the spatiotemporal locality, which is essential for temporally coherent VOS. To take full advantage of the image and video features, HST casts image and video features as a query and memory, respectively. By applying efficient memory read operations at multiple scales, HST produces hierarchical features for the precise reconstruction of object masks. HST shows effectiveness and robustness in handling challenging scenarios with occluded and fast-moving objects under cluttered backgrounds. In particular, HST-B outperforms the state-of-the-art competitors on multiple popular benchmarks, i.e., YouTube-VOS (85.0%), DAVIS 2017 (85.9%), and DAVIS 2016 (94.0%).", "venue": "arXiv", "keywords": ["temporal consistency", "transformers"]}
{"id": "yooPhotorealisticStyleTransfer", "title": "Photorealistic Style Transfer via Wavelet Transforms", "abstract": "", "venue": "", "keywords": ["style transfer"]}
{"id": "yooPhotorealisticStyleTransfer2019", "title": "Photorealistic Style Transfer via Wavelet Transforms", "abstract": "Recent style transfer models have provided promising artistic results. However, given a photograph as a reference style, existing methods are limited by spatial distortions or unrealistic artifacts, which should not happen in real photographs. We introduce a theoretically sound correction to the network architecture that remarkably enhances photorealism and faithfully transfers the style. The key ingredient of our method is wavelet transforms that naturally fits in deep networks. We propose a wavelet corrected transfer based on whitening and coloring transforms (WCT\\ ) that allows features to preserve their structural information and statistical properties of VGG feature space during stylization. This is the first and the only end-to-end model that can stylize a \\ resolution image in 4.7 seconds, giving a pleasing and photorealistic quality without any post-processing. Last but not least, our model provides a stable video stylization without temporal constraints. Our code, generated images, and pre-trained models are all available at https://github.com/ClovaAI/WCT2.", "venue": "arXiv", "keywords": ["domain projection", "feature whitening", "feature-level augmentation", "style ind learning", "style transfer"]}
{"id": "YouTubeNowUsing", "title": "YouTube Is Now Using A.I. on Videos That Had Previously Mastered Games Including Chess and Go", "abstract": "This is the first commercial use of MuZero, which previously beat humans at chess, Go and Atari games.", "venue": "Fortune", "keywords": []}
{"id": "yuanUsingDataAugmentation2020", "title": "Using Data Augmentation Based Reinforcement Learning for Daily Stock Trading", "abstract": "In algorithmic trading, adequate training data set is key to making profits. However, stock trading data in units of a day can not meet the great demand for reinforcement learning. To address this problem, we proposed a framework named data augmentation based reinforcement learning (DARL) which uses minute-candle data (open, high, low, close) to train the agent. The agent is then used to guide daily stock trading. In this way, we can increase the instances of data available for training in hundreds of folds, which can substantially improve the reinforcement learning effect. But not all stocks are suitable for this kind of trading. Therefore, we propose an access mechanism based on skewness and kurtosis to select stocks that can be traded properly using this algorithm. In our experiment, we find proximal policy optimization (PPO) is the most stable algorithm to achieve high risk-adjusted returns. Deep Q-learning (DQN) and soft actor critic (SAC) can beat the market in Sharp Ratio.", "venue": "Electronics", "keywords": ["reinforcement learning"]}
{"id": "yuBoostingMappingFunctionality2019", "title": "Boosting Mapping Functionality of Neural Networks via Latent Feature Generation Based on Reversible Learning", "abstract": "This paper addresses a boosting method for mapping functionality of neural networks in visual recognition such as image classification and face recognition. We present reversible learning for generating and learning latent features using the network itself. By generating latent features corresponding to hard samples and applying the generated features in a training stage, reversible learning can improve a mapping functionality without additional data augmentation or handling the bias of dataset. We demonstrate an efficiency of the proposed method on the MNIST,Cifar-10/100, and Extremely Biased and poorly categorized dataset (EBPC dataset). The experimental results show that the proposed method can outperform existing state-of-the-art methods in visual recognition. Extensive analysis shows that our method can efficiently improve the mapping capability of a network.", "venue": "arXiv", "keywords": ["feature engineering"]}
{"id": "yuCMTDeepLabClusteringMask2022", "title": "CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation", "abstract": "We propose Clustering Mask Transformer (CMTDeepLab), a transformer-based framework for panoptic segmentation designed around clustering. It rethinks the existing transformer architectures used in segmentation and detection; CMT-DeepLab considers the object queries as cluster centers, which fill the role of grouping the pixels when applied to segmentation. The clustering is computed with an alternating procedure, by first assigning pixels to the clusters by their feature affinity, and then updating the cluster centers and pixel features. Together, these operations comprise the Clustering Mask Transformer (CMT) layer, which produces cross-attention that is denser and more consistent with the final segmentation task. CMTDeepLab improves the performance over prior art significantly by 4.4% PQ, achieving a new state-of-the-art of 55.7% PQ on the COCO test-dev set.", "venue": "arXiv", "keywords": []}
{"id": "yuContextPriorScene2020", "title": "Context Prior for Scene Segmentation", "abstract": "Recent works have widely explored the contextual dependencies to achieve more accurate segmentation results. However, most approaches rarely distinguish different types of contextual dependencies, which may pollute the scene understanding. In this work, we directly supervise the feature aggregation to distinguish the intra-class and inter-class context clearly. Specifically, we develop a Context Prior with the supervision of the Affinity Loss. Given an input image and corresponding ground truth, Affinity Loss constructs an ideal affinity map to supervise the learning of Context Prior. The learned Context Prior extracts the pixels belonging to the same category, while the reversed prior focuses on the pixels of different classes. Embedded into a conventional deep CNN, the proposed Context Prior Layer can selectively capture the intra-class and inter-class contextual dependencies, leading to robust feature representation. To validate the effectiveness, we design an effective Context Prior Network (CPNet). Extensive quantitative and qualitative evaluations demonstrate that the proposed model performs favorably against state-of-the-art semantic segmentation approaches. More specifically, our algorithm achieves 46.3% mIoU on ADE20K, 53.9% mIoU on PASCAL-Context, and 81.3% mIoU on Cityscapes. Code is available at https://git.io/ContextPrior.", "venue": "arXiv", "keywords": ["affinity modeling", "loss functions", "semantic segmentation", "visualizations"]}
{"id": "yuDistributionShiftInversion2023", "title": "Distribution Shift Inversion for Out-of-Distribution Prediction", "abstract": "Machine learning society has witnessed the emergence of a myriad of Out-of-Distribution (OoD) algorithms, which address the distribution shift between the training and the testing distribution by searching for a unified predictor or invariant feature representation. However, the task of directly mitigating the distribution shift in the unseen testing set is rarely investigated, due to the unavailability of the testing distribution during the training phase and thus the impossibility of training a distribution translator mapping between the training and testing distribution. In this paper, we explore how to bypass the requirement of testing distribution for distribution translator training and make the distribution translation useful for OoD prediction. We propose a portable Distribution Shift Inversion (DSI) algorithm, in which, before being fed into the prediction model, the OoD testing samples are first linearly combined with additional Gaussian noise and then transferred back towards the training distribution using a diffusion model trained only on the source distribution. Theoretical analysis reveals the feasibility of our method. Experimental results, on both multiple-domain generalization datasets and single-domain generalization datasets, show that our method provides a general performance gain when plugged into a wide range of commonly used OoD algorithms. Our code is available at https://github.com/yu-rp/Distribution-Shift-Iverson.", "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["distribution estimation", "domain generalization", "generative augmentation", "generative inversion"]}
{"id": "yueDomainRandomizationPyramid2022", "title": "Domain Randomization and Pyramid Consistency: Simulation-to-Real Generalization without Accessing Target Domain Data", "abstract": "We propose to harness the potential of simulation for the semantic segmentation of real-world self-driving scenes in a domain generalization fashion. The segmentation network is trained without any data of target domains and tested on the unseen target domains. To this end, we propose a new approach of domain randomization and pyramid consistency to learn a model with high generalizability. First, we propose to randomize the synthetic images with the styles of real images in terms of visual appearances using auxiliary datasets, in order to effectively learn domain-invariant representations. Second, we further enforce pyramid consistency across different \"stylized\" images and within an image, in order to learn domain-invariant and scale-invariant features, respectively. Extensive experiments are conducted on the generalization from GTA and SYNTHIA to Cityscapes, BDDS and Mapillary; and our method achieves superior results over the state-of-the-art techniques. Remarkably, our generalization results are on par with or even better than those obtained by state-of-the-art simulation-to-real domain adaptation methods, which access the target domain data at training time.", "venue": "arXiv", "keywords": ["consistency training", "domain randomization", "feature pyramids", "promising", "representation learning", "sim-to-real", "style transfer"]}
{"id": "yueLinearOptimizationWasserstein2022", "title": "On Linear Optimization over Wasserstein Balls", "abstract": "Wasserstein balls, which contain all probability measures within a pre-specified Wasserstein distance to a reference measure, have recently enjoyed wide popularity in the distributionally robust optimization and machine learning communities to formulate and solve data-driven optimization problems with rigorous statistical guarantees. In this technical note we prove that the Wasserstein ball is weakly compact under mild conditions, and we offer necessary and sufficient conditions for the existence of optimal solutions. We also characterize the sparsity of solutions if the Wasserstein ball is centred at a discrete reference measure. In comparison with the existing literature, which has proved similar results under different conditions, our proofs are self-contained and shorter, yet mathematically rigorous, and our necessary and sufficient conditions for the existence of optimal solutions are easily verifiable in practice.", "venue": "Mathematical Programming", "keywords": ["highly-analytical", "manifold learning"]}
{"id": "yukselSemanticPerturbationsNormalizing2021", "title": "Semantic Perturbations With Normalizing Flows for Improved Generalization", "abstract": "", "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "keywords": ["domain generalization", "feature-level augmentation", "normalizing flows"]}
{"id": "yuMeasuringDependenceMatrixbased2021", "title": "Measuring Dependence with Matrix-based Entropy Functional", "abstract": "Measuring the dependence of data plays a central role in statistics and machine learning. In this work, we summarize and generalize the main idea of existing information-theoretic dependence measures into a higher-level perspective by the Shearer's inequality. Based on our generalization, we then propose two measures, namely the matrix-based normalized total correlation and the matrix-based normalized dual total correlation, to quantify the dependence of multiple variables in arbitrary dimensional space, without explicit estimation of the underlying data distributions. We show that our measures are differentiable and statistically more powerful than prevalent ones. We also show the impact of our measures in four different machine learning problems, namely the gene regulatory network inference, the robust machine learning under covariate shift and non-Gaussian noises, the subspace outlier detection, and the understanding of the learning dynamics of convolutional neural networks, to demonstrate their utilities, advantages, as well as implications to those problems.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "keywords": ["entropic optimization", "highly-analytical", "kernel methods", "measure theory"]}
{"id": "yuMultiScaleContextAggregation2016", "title": "Multi-Scale Context Aggregation by Dilated Convolutions", "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.", "venue": "arXiv", "keywords": ["bias sources", "cnns", "semantic segmentation"]}
{"id": "yunCutMixRegularizationStrategy2019", "title": "CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features", "abstract": "Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch .", "venue": "arXiv", "keywords": ["mixture augmentations"]}
{"id": "zellingerCentralMomentDiscrepancy2019", "title": "Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning", "abstract": "The learning of domain-invariant representations in the context of domain adaptation with neural networks is considered. We propose a new regularization method that minimizes the discrepancy between domain-specific latent feature representations directly in the hidden activation space. Although some standard distribution matching approaches exist that can be interpreted as the matching of weighted sums of moments, e.g. Maximum Mean Discrepancy (MMD), an explicit order-wise matching of higher order moments has not been considered before. We propose to match the higher order central moments of probability distributions by means of order-wise moment differences. Our model does not require computationally expensive distance and kernel matrix computations. We utilize the equivalent representation of probability distributions by moment sequences to define a new distance function, called Central Moment Discrepancy (CMD). We prove that CMD is a metric on the set of probability distributions on a compact interval. We further prove that convergence of probability distributions on compact intervals w.r.t. the new metric implies convergence in distribution of the respective random variables. We test our approach on two different benchmark data sets for object recognition (Office) and sentiment analysis of product reviews (Amazon reviews). CMD achieves a new state-of-the-art performance on most domain adaptation tasks of Office and outperforms networks trained with MMD, Variational Fair Autoencoders and Domain Adversarial Neural Networks on Amazon reviews. In addition, a post-hoc parameter sensitivity analysis shows that the new approach is stable w.r.t. parameter changes in a certain interval. The source code of the experiments is publicly available.", "venue": "arXiv", "keywords": ["domain generalization", "representation learning"]}
{"id": "zengVideoTransUNetTemporallyBlended2022", "title": "Video-TransUNet: Temporally Blended Vision Transformer for CT VFSS Instance Segmentation", "abstract": "We propose Video-TransUNet, a deep architecture for instance segmentation in medical CT videos constructed by integrating temporal feature blending into the TransUNet deep learning framework. In particular, our approach amalgamates strong frame representation via a ResNet CNN backbone, multi-frame feature blending via a Temporal Context Module (TCM), non-local attention via a Vision Transformer, and reconstructive capabilities for multiple targets via a UNet-based convolutional-deconvolutional architecture with multiple heads. We show that this new network design can significantly outperform other state-of-the-art systems when tested on the segmentation of bolus and pharynx/larynx in Videofluoroscopic Swallowing Study (VFSS) CT sequences. On our VFSS2022 dataset it achieves a dice coefficient of 0.8796 and an average surface distance of 1.0379 pixels. Note that tracking the pharyngeal bolus accurately is a particularly important application in clinical practice since it constitutes the primary method for diagnostics of swallowing impairment. Our findings suggest that the proposed model can indeed enhance the TransUNet architecture via exploiting temporal information and improving segmentation performance by a significant margin. We publish key source code, network weights, and ground truth annotations for simplified performance reproduction.", "venue": "arXiv", "keywords": ["temporal consistency", "transformers"]}
{"id": "zhangAdversarialAutoAugment2019", "title": "Adversarial AutoAugment", "abstract": "Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12x reduction in computing cost and 11x shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data.", "venue": "arXiv", "keywords": ["adversarial learning", "auto-augmentation policies"]}
{"id": "zhangAnomalyAwareSemanticSegmentation2023", "title": "Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation", "abstract": "Within the context of autonomous driving, encountering unknown objects becomes inevitable during deployment in the open world. Therefore, it is crucial to equip standard semantic segmentation models with anomaly awareness. Many previous approaches have utilized synthetic out-ofdistribution (OoD) data augmentation to tackle this problem. In this work, we advance the OoD synthesis process by reducing the domain gap between the OoD data and driving scenes, effectively mitigating the style difference that might otherwise act as an obvious shortcut during training. Additionally, we propose a simple fine-tuning loss that effectively induces a pre-trained semantic segmentation model to generate a ``none of the given classes'' prediction, leveraging per-pixel OoD scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline enables the use of pre-trained models for anomaly segmentation while maintaining the performance on the original task.", "venue": "2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "keywords": ["semantic segmentation", "style transfer"]}
{"id": "zhangArtBankArtisticStyle2023", "title": "ArtBank: Artistic Style Transfer with Pre-trained Diffusion Model and Implicit Style Prompt Bank", "abstract": "Artistic style transfer aims to repaint the content image with the learned artistic style. Existing artistic style transfer methods can be divided into two categories: small model-based approaches and pre-trained large-scale model-based approaches. Small model-based approaches can preserve the content strucuture, but fail to produce highly realistic stylized images and introduce artifacts and disharmonious patterns; Pre-trained large-scale model-based approaches can generate highly realistic stylized images but struggle with preserving the content structure. To address the above issues, we propose ArtBank, a novel artistic style transfer framework, to generate highly realistic stylized images while preserving the content structure of the content images. Specifically, to sufficiently dig out the knowledge embedded in pre-trained large-scale models, an Implicit Style Prompt Bank (ISPB), a set of trainable parameter matrices, is designed to learn and store knowledge from the collection of artworks and behave as a visual prompt to guide pre-trained large-scale models to generate highly realistic stylized images while preserving content structure. Besides, to accelerate training the above ISPB, we propose a novel Spatial-Statistical-based self-Attention Module (SSAM). The qualitative and quantitative experiments demonstrate the superiority of our proposed method over state-of-the-art artistic style transfer methods.", "venue": "arXiv", "keywords": ["diffusion models", "image synthesis", "prompt-based", "style transfer"]}
{"id": "zhangAttacksWhichNot2020", "title": "Attacks Which Do Not Kill Training Make Adversarial Learning Stronger", "abstract": "Adversarial training based on the minimax formulation is necessary for obtaining adversarial robustness of trained models. However, it is conservative or even pessimistic so that it sometimes hurts the natural generalization. In this paper, we raise a fundamental question---do we have to trade off natural generalization for adversarial robustness? We argue that adversarial training is to employ confident adversarial data for updating the current model. We propose a novel approach of friendly adversarial training (FAT): rather than employing most adversarial data maximizing the loss, we search for least adversarial (i.e., friendly adversarial) data minimizing the loss, among the adversarial data that are confidently misclassified. Our novel formulation is easy to implement by just stopping the most adversarial data searching algorithms such as PGD (projected gradient descent) early, which we call early-stopped PGD. Theoretically, FAT is justified by an upper bound of the adversarial risk. Empirically, early-stopped PGD allows us to answer the earlier question negatively---adversarial robustness can indeed be achieved without compromising the natural generalization.", "venue": "arXiv", "keywords": ["adversarial learning", "adversarial robustness", "robustness analysis"]}
{"id": "zhangAuxAdaptStableEfficient2022", "title": "AuxAdapt: Stable and Efficient Test-Time Adaptation for Temporally Consistent Video Semantic Segmentation", "abstract": "In video segmentation, generating temporally consistent results across frames is as important as achieving frame-wise accuracy. This paper presents an efficient, intuitive, and unsupervised online adaptation method, AuxAdapt, for improving the temporal consistency of most neural network models. It does not require optical flow and only takes one pass of the video. Since inconsistency mainly arises from the model's uncertainty in its output, we propose an adaptation scheme where the model learns from its own segmentation decisions as it streams a video, which allows producing more confident and temporally consistent labeling for similarly-looking pixels across frames. For stability and efficiency, we leverage a small auxiliary segmentation network (AuxNet) to assist with this adaptation. More specifically, AuxNet readjusts the decision of the original segmentation network (Main-Net) by adding its own estimations to that of MainNet. At every frame, only AuxNet is updated via back-propagation while keeping MainNet fixed. We extensively evaluate our test-time adaptation approach on standard video benchmarks, including Cityscapes, CamVid, and KITTI. The results demonstrate that our approach provides label-wise accurate, temporally consistent, and computationally efficient adaptation.", "venue": "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)", "keywords": ["auxillary networks", "promising", "semantic segmentation", "temporal consistency", "test-time da", "unsupervised da"]}
{"id": "zhangCaptureControlContent2023", "title": "Capture and Control Content Discrepancies via Normalised Flow Transfer", "abstract": "Unsupervised Style Transfer (UST) has recently been a hot topic in Computer Vision, and this type of work has been exemplified by CycleGAN. Although the existing UST methods have proven to be useful, in many circumstances, we want to be able to manage not just the transformation of a single piece of instance, but also the morphological features of the two data sets' underlying distributions. To this end, we propose a novel framework called Normalised Flow Transfer (NFT), where a reversible probability transform using the normalised flow method is developed to transfer the data in the first domain to the second, so as to exhibit their probabilities under both domains in the mapping process. A particularly interesting application under this framework is that when the data sets in two domains contain numerous clusters based on their finite class labels, we can control the distribution pattern across the two domains and apply any constraints on the underlying distributions of the same class. The experimental results show that not only can we devise many new complex style transfer functions, but also our framework has better image generation capabilities in terms of evaluation metrics, including mean square error, inception score and Frechet inception distance.", "venue": "Pattern Recognition Letters", "keywords": ["distribution estimation", "normalizing flows", "style transfer"]}
{"id": "zhangCategoryAnchorGuidedUnsupervised", "title": "Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation", "abstract": "Unsupervised domain adaptation (UDA) aims to enhance the generalization capability of a certain model from a source domain to a target domain. UDA is of particular significance since no extra effort is devoted to annotating target domain samples. However, the different data distributions in the two domains, or domain shift/discrepancy, inevitably compromise the UDA performance. Although there has been a progress in matching the marginal distributions between two domains, the classifier favors the source domain features and makes incorrect predictions on the target domain due to category-agnostic feature alignment. In this paper, we propose a novel category anchor-guided (CAG) UDA model for semantic segmentation, which explicitly enforces category-aware feature alignment to learn shared discriminative features and classifiers simultaneously. First, the category-wise centroids of the source domain features are used as guided anchors to identify the active features in the target domain and also assign them pseudo-labels. Then, we leverage an anchor-based pixel-level distance loss and a discriminative loss to drive the intra-category features closer and the inter-category features further apart, respectively. Finally, we devise a stagewise training mechanism to reduce the error accumulation and adapt the proposed model progressively. Experiments on both the GTA5 Cityscapes and SYNTHIA Cityscapes scenarios demonstrate the superiority of our CAG-UDA model over the state-of-the-art methods. The code is available at https://github.com/RogerZhangzz/CAG_UDA.", "venue": "", "keywords": ["class balancing", "feature engineering", "semantic segmentation", "sim-to-real", "unsupervised da"]}
{"id": "zhangContrastiveDeepSupervision2022", "title": "Contrastive Deep Supervision", "abstract": "The success of deep learning is usually accompanied by the growth in neural network depth. However, the traditional training method only supervises the neural network at its last layer and propagates the supervision layer-by-layer, which leads to hardship in optimizing the intermediate layers. Recently, deep supervision has been proposed to add auxiliary classifiers to the intermediate layers of deep neural networks. By optimizing these auxiliary classifiers with the supervised task loss, the supervision can be applied to the shallow layers directly. However, deep supervision conflicts with the well-known observation that the shallow layers learn low-level features instead of task-biased high-level semantic features. To address this issue, this paper proposes a novel training framework named Contrastive Deep Supervision, which supervises the intermediate layers with augmentation-based contrastive learning. Experimental results on nine popular datasets with eleven models demonstrate its effects on general image classification, fine-grained image classification and object detection in supervised learning, semi-supervised learning and knowledge distillation. Codes have been released in Github.", "venue": "arXiv.org", "keywords": ["contrastive learning", "knowledge distillation", "semi-supervised learning"]}
{"id": "zhangDeepImageBlending2019", "title": "Deep Image Blending", "abstract": "Image composition is an important operation to create visual content. Among image composition tasks, image blending aims to seamlessly blend an object from a source image onto a target image with lightly mask adjustment. A popular approach is Poisson image blending, which enforces the gradient domain smoothness in the composite image. However, this approach only considers the boundary pixels of target image, and thus can not adapt to texture of target image. In addition, the colors of the target image often seep through the original source object too much causing a significant loss of content of the source object. We propose a Poisson blending loss that achieves the same purpose of Poisson image blending. In addition, we jointly optimize the proposed Poisson blending loss as well as the style and content loss computed from a deep network, and reconstruct the blending region by iteratively updating the pixels using the L-BFGS solver. In the blending image, we not only smooth out gradient domain of the blending boundary but also add consistent texture into the blending region. User studies show that our method outperforms strong baselines as well as state-of-the-art approaches when placing objects onto both paintings and real-world images.", "venue": "arXiv", "keywords": []}
{"id": "zhangDistributedMultitaskClassification2018", "title": "Distributed Multi-Task Classification: A Decentralized Online Learning Approach", "abstract": "", "venue": "Machine Learning", "keywords": ["multi-task learning"]}
{"id": "zhangDistributedOptimizationDegenerate2021", "title": "Distributed Optimization for Degenerate Loss Functions Arising from Over-Parameterization", "abstract": "We consider distributed optimization with degenerate loss functions, where the optimal sets of local loss functions have a non-empty intersection. This regime often arises in optimizing large-scale multi-agent AI systems (e.g., deep learning systems), where the number of trainable weights far exceeds the number of training samples, leading to highly degenerate loss surfaces. Under appropriate conditions, we prove that distributed gradient descent in this case converges even when communication is arbitrarily less frequent, which is not the case for non-degenerate loss functions. Moreover, we quantitatively analyze the convergence rate, as well as the communication and computation trade-off, providing insights into designing efficient distributed optimization algorithms. Our theoretical findings are confirmed by both distributed convex optimization and deep learning experiments.", "venue": "Artificial Intelligence", "keywords": []}
{"id": "zhangDomainSpecificRiskMinimization2023", "title": "Domain-Specific Risk Minimization for Out-of-Distribution Generalization", "abstract": "Recent domain generalization (DG) approaches typically use the hypothesis learned on source domains for inference on the unseen target domain. However, such a hypothesis can be arbitrarily far from the optimal one for the target domain, induced by a gap termed ``adaptivity gap''. Without exploiting the domain information from the unseen test samples, adaptivity gap estimation and minimization are intractable, which hinders us to robustify a model to any unknown distribution. In this paper, we first establish a generalization bound that explicitly considers the adaptivity gap. Our bound motivates two strategies to reduce the gap: the first one is ensembling multiple classifiers to enrich the hypothesis space, then we propose effective gap estimation methods for guiding the selection of a better hypothesis for the target. The other method is minimizing the gap directly by adapting model parameters using online target samples. We thus propose -specific Risk Minimization (DRM)\\. During training, DRM models the distributions of different source domains separately; for inference, DRM performs online model steering using the source hypothesis for each arriving target sample. Extensive experiments demonstrate the effectiveness of the proposed DRM for domain generalization with the following advantages: 1) it significantly outperforms competitive baselines on different distributional shift settings; 2) it achieves either comparable or superior accuracies on all source domains compared to vanilla empirical risk minimization; 3) it remains simple and efficient during training, and 4) it is complementary to invariant learning approaches.", "venue": "arXiv", "keywords": ["distribution estimation", "domain generalization", "risk optimization"]}
{"id": "zhangEdgeEnhancedImage2023", "title": "Edge Enhanced Image Style Transfer via Transformers", "abstract": "In recent years, arbitrary image style transfer has attracted more and more attention. Given a pair of content and style images, a stylized one is hoped that retains the content from the former while catching style patterns from the latter. However, it is difficult to simultaneously keep well the trade-off between the content details and the style features. To stylize the image with sufficient style patterns, the content details may be damaged and sometimes the objects of images can not be distinguished clearly. For this reason, we present a new transformer-based method named STT for image style transfer and an edge loss which can enhance the content details apparently to avoid generating blurred results for excessive rendering on style features. Qualitative and quantitative experiments demonstrate that STT achieves comparable performance to state-of-the-art image style transfer methods while alleviating the content leak problem.", "venue": "arXiv", "keywords": ["style transfer", "transformers"]}
{"id": "zhangFlatnessAwareMinimizationDomain2023", "title": "Flatness-Aware Minimization for Domain Generalization", "abstract": "Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD's out-ofdistribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets.", "venue": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)", "keywords": []}
{"id": "zhangFlatnessAwareMinimizationDomain2023a", "title": "Flatness-Aware Minimization for Domain Generalization", "abstract": "Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets. Additionally, we confirm that FAD is capable of discovering flatter optima in comparison to other zeroth-order and first-order flatness-aware optimization methods.", "venue": "arXiv", "keywords": ["domain generalization", "loss functions"]}
{"id": "zhangGeneralizingDeepLearning2020", "title": "Generalizing Deep Learning for Medical Image Segmentation to Unseen Domains via Deep Stacked Transformation", "abstract": "ecent advances in deep learning for medical image segmentation demonstrate expert-level accuracy.ecent advances in deep learning for medical image segmentation demonstrate expert-level accuracy.R However, application of these models in clinically realistic environments can result in poor generalization and decreased accuracy, mainly due to the domain shift across different hospitals, scanner vendors, imaging protocols, and patient populations etc. Common transfer learning and domain adaptation techniques are proposed to address this bottleneck. However, these solutions require data (and annotations) from the target domain to retrain the model, and is therefore restrictive in practice for widespread model deployment. Ideally, we wish to have a trained (locked) model that can work uniformly well across unseen domains without further training. In this paper, we propose a deep stacked transformation approach for domain generalization. Specifically, a series of n stacked transformations are applied to each image during network training. The underlying assumption is that the ``expected'' domain shift for a specific medical imaging modality could be simulated by applying extensive data augmentation on a single source domain, and consequently, a deep model trained on the augmented ``big'' data (BigAug) could generalize well on unseen domains. We exploit four surprisingly effective, but previously understudied, image-based characteristics for data augmentation to overcome the domain generalization problem. We train and evaluate the BigAug model (with n = 9 transformations) on three different 3D segmentation tasks (prostate gland, left atrial, left ventricle) covering two medical imaging modalities (MRI and ultrasound) involving eight publicly available challenge datasets. The results show that when training on relatively small dataset (n=10 32 volumes, depending on the size of the available datasets) from a single source domain: (i) BigAug models degrade an average of 11% (Dice score change) from source to unseen domain, substantially better than conventional augmentation (degrading 39%) and CycleGAN-based domain adaptation method (degrading 25%), (ii) BigAug is better than ``shallower'' stacked transforms (i.e. those with fewer transforms) on unseen domains and demonstrates modest improvement to conventional augmentation on the source domain, (iii) after training with BigAug on one source domain, performance on an unseen domain is similar to training a model from scratch on that domain when using the same number of training samples. When training on large datasets (n=465 volumes) with BigAug, (iv) application to unseen domains reaches the performance of state-of-the-art fully supervised models that are trained and tested on their source domains. These findings establish a strong benchmark for the study of domain generalization in medical imaging, and can be generalized to the design of highly robust deep segmentation models for clinical deployment.", "venue": "IEEE transactions on medical imaging", "keywords": ["auto-augmentation policies"]}
{"id": "zhangGoodDataAugmentation2023", "title": "A Good Data Augmentation Policy Is Not All You Need: A Multi-Task Learning Perspective", "abstract": "Data augmentation, which improves the diversity of datasets by applying image transformations, has become one of the most effective techniques in visual representation learning. Usually, the design of augmentation policies faces a diversity-difficulty trade-off. On the one hand, a simple augmentation leads to a low training set diversity, which can not improve model performance significantly. On the other hand, an excessively hard augmentation has an overlarge regularization effect which harms model performance. Recently, automatic augmentation methods have been proposed to address this issue by searching the optimal data augmentation policy from a predefined searching space. However, these methods still suffer from heavy searching overhead or complex optimization objectives. In this paper, instead of searching the optimal augmentation policy, we propose to break the diversity-difficulty trade-off from a multi-task learning perspective. By formulating model learning on the augmented images and the original images as the auxiliary task and the primary task in multi-task learning respectively, the hard augmentation does not directly influence the training of the primary branch and thus its negative influence can be alleviated. Hence, neural networks can learn valuable semantic information even with a totally random augmentation policy. Experimental results on ten datasets for four tasks demonstrate the superiority of our method over the other twelve methods. Codes have been released in https://github.com/ArchipLab-LinfengZhang/data-augmentation-multi-task.", "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "keywords": []}
{"id": "zhangGTAVNightRainPhotometricRealistic2022", "title": "GTAV-NightRain: Photometric Realistic Large-scale Dataset for Night-time Rain Streak Removal", "abstract": "Rain is transparent, which reflects and refracts light in the scene to the camera. In outdoor vision, rain, especially rain streaks degrade visibility and therefore need to be removed. In existing rain streak removal datasets, although density, scale, direction and intensity have been considered, transparency is not fully taken into account. This problem is particularly serious in night scenes, where the appearance of rain largely depends on the interaction with scene illuminations and changes drastically on different positions within the image. This is problematic, because unrealistic dataset causes serious domain bias. In this paper, we propose GTAV-NightRain dataset, which is a large-scale synthetic night-time rain streak removal dataset. Unlike existing datasets, by using 3D computer graphic platform (namely GTA V), we are allowed to infer the three dimensional interaction between rain and illuminations, which insures the photometric realness. Current release of the dataset contains 12,860 HD rainy images and 1,286 corresponding HD ground truth images in diversified night scenes. A systematic benchmark and analysis are provided along with the dataset to inspire further research.", "venue": "arXiv.org", "keywords": ["dataset debut"]}
{"id": "zhangHierarchicalLifelongLearning2021", "title": "Hierarchical Lifelong Learning by Sharing Representations and Integrating Hypothesis", "abstract": "In lifelong machine learning (LML) systems, consecutive new tasks from changing circumstances are learned and added to the system. However, sufficiently labeled data are indispensable for extracting intertask relationships before transferring knowledge in classical supervised LML systems. Inadequate labels may deteriorate the performance due to the poor initial approximation. In order to extend the typical LML system, we propose a novel hierarchical lifelong learning algorithm (HLLA) consisting of two following layers: 1) the knowledge layer consisted of shared representations and integrated knowledge basis at the bottom and 2) parameterized hypothesis functions with features at the top. Unlabeled data is leveraged in HLLA for pretraining of the shared representations. We also have considered a selective inherited updating method to deal with intertask distribution shifting. Experiments show that our HLLA method outperforms many other recent LML algorithms, especially when dealing with higher dimensional, lower correlation, and fewer labeled data problems.", "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems", "keywords": []}
{"id": "zhangHighlyRealisticArtistic2024", "title": "Towards Highly Realistic Artistic Style Transfer via Stable Diffusion with Step-aware and Layer-aware Prompt", "abstract": "Artistic style transfer aims to transfer the learned artistic style onto an arbitrary content image, generating artistic stylized images. Existing generative adversarial network-based methods fail to generate highly realistic stylized images and always introduce obvious artifacts and disharmonious patterns. Recently, large-scale pre-trained diffusion models opened up a new way for generating highly realistic artistic stylized images. However, diffusion model-based methods generally fail to preserve the content structure of input content images well, introducing some undesired content structure and style patterns. To address the above problems, we propose a novel pre-trained diffusionbased artistic style transfer method, called LSAST, which can generate highly realistic artistic stylized images while preserving the content structure of input content images well, without bringing obvious artifacts and disharmonious style patterns. Specifically, we introduce a Step-aware and Layer-aware Prompt Space, a set of learnable prompts, which can learn the style information from the collection of artworks and dynamically adjusts the input images' content structure and style pattern. To train our prompt space, we propose a novel inversion method, called Step-ware and Layer-aware Prompt Inversion, which allows the prompt space to learn the style information of the artworks collection. In addition, we inject a pre-trained conditional branch of ControlNet into our LSAST, which further improved our framework's ability to maintain content structure. Extensive experiments demonstrate that our proposed method can generate more highly realistic artistic stylized images than the state-of-theart artistic style transfer methods. Code is available at https://github.com/Jamie-Cheung/LSAST.", "venue": "Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence", "keywords": []}
{"id": "zhangImprovedEvaluationMethod2017", "title": "An Improved Evaluation Method for Optical Flow of Endpoint Error", "abstract": "A corrected evaluation benchmark is proposed based on the Middlebury benchmark. The major contributions of the presented benchmark include the following: error metrics of endpoint error are modified, and the corrected metrics can decrease the detrimental influence of human factors and reflect the actual flow evaluation performance. In addition, the metric of normalization endpoint error is projected to reveal the relative error of the flow result endpoint, which can eliminate the influence of small flow errors. For flow error statistics, a mathematical expectation is employed instead of the current mathematical average as the expectation can reflect a more appropriate error distribution for the flow result. In addition, robust statistics and the accuracy measures derived from the Middlebury benchmark are employed to evaluate the robustness of optical flow in the proposed method.", "venue": "Proceedings of the International Conference on Computer Networks and Communication Technology (CNCT 2016)", "keywords": ["optical flow", "temporal consistency"]}
{"id": "zhangInversionBasedStyleTransfer2023", "title": "Inversion-Based Style Transfer with Diffusion Models", "abstract": "The artistic style within a painting is the means of expression, which includes not only the painting material, colors, and brushstrokes, but also the high-level attributes including semantic elements, object shapes, etc. Previous arbitrary example-guided artistic image generation methods often fail to control shape changes or convey elements. The pre-trained text-to-image synthesis diffusion probabilistic models have achieved remarkable quality, but it often requires extensive textual descriptions to accurately portray attributes of a particular painting. We believe that the uniqueness of an artwork lies precisely in the fact that it cannot be adequately explained with normal language. Our key idea is to learn artistic style directly from a single painting and then guide the synthesis without providing complex textual descriptions. Specifically, we assume style as a learnable textual description of a painting. We propose an inversion-based style transfer method (InST), which can efficiently and accurately learn the key information of an image, thus capturing and transferring the artistic style of a painting. We demonstrate the quality and efficiency of our method on numerous paintings of various artists and styles. Code and models are available at https://github.com/zyxElsa/InST.", "venue": "arXiv", "keywords": ["diffusion models", "generative inversion", "style transfer"]}
{"id": "zhangLearningShapeInvariantRepresentation2023", "title": "Learning Shape-Invariant Representation for Generalizable Semantic Segmentation", "abstract": "Semantic segmentation assigns a category for each pixel and has achieved great success in a supervised manner. However, it fails to generalize well in new domains due to the domain gap. Domain adaptation is a popular way to solve this issue, but it needs target data and cannot handle unavailable domains. In domain generalization (DG), the model is trained without the target data and DG aims to generalize well in new unavailable domains. Recent works reveal that shape recognition is beneficial for generalization but still lack exploration in semantic segmentation. Meanwhile, the object shapes also exist a discrepancy in different domains, which is often ignored by the existing works. Thus, we propose a Shape-Invariant Learning (SIL) framework to focus on learning shape-invariant representation for better generalization. Specifically, we first define the structural edge, which considers both the object boundary and the inner structure of the object to provide more discrimination cues. Then, a shape perception learning strategy including a texture feature discrepancy reduction loss and a structural feature discrepancy enlargement loss is proposed to enhance the shape perception ability of the model by embedding the structural edge as a shape prior. Finally, we use shape deformation augmentation to generate samples with the same content and different shapes. Essentially, our SIL framework performs implicit shape distribution alignment at the domain-level to learn shape-invariant representation. Extensive experiments show that our SIL framework achieves state-of-the-art performance.", "venue": "IEEE Transactions on Image Processing", "keywords": []}
{"id": "zhangMixupEmpiricalRisk2018", "title": "Mixup: Beyond Empirical Risk Minimization", "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.", "venue": "arXiv", "keywords": ["foundational", "risk optimization"]}
{"id": "zhangMixupEmpiricalRisk2018a", "title": "Mixup: Beyond Empirical Risk Minimization", "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.", "venue": "arXiv", "keywords": ["foundational", "mixture augmentations", "risk optimization"]}
{"id": "zhangMultimodalStyleTransfer2020", "title": "Multimodal Style Transfer via Graph Cuts", "abstract": "An assumption widely used in recent neural style transfer methods is that image styles can be described by global statics of deep features like Gram or covariance matrices. Alternative approaches have represented styles by decomposing them into local pixel or neural patches. Despite the recent progress, most existing methods treat the semantic patterns of style image uniformly, resulting unpleasing results on complex styles. In this paper, we introduce a more flexible and general universal style transfer technique: multimodal style transfer (MST). MST explicitly considers the matching of semantic patterns in content and style images. Specifically, the style image features are clustered into sub-style components, which are matched with local content features under a graph cut formulation. A reconstruction network is trained to transfer each sub-style and render the final stylized result. We also generalize MST to improve some existing methods. Extensive experiments demonstrate the superior effectiveness, robustness, and flexibility of MST.", "venue": "arXiv", "keywords": ["deep clustering", "style transfer"]}
{"id": "zhangMultipleFusionAdaptation2021", "title": "Multiple Fusion Adaptation: A Strong Framework for Unsupervised Semantic Segmentation Adaptation", "abstract": "This paper challenges the cross-domain semantic segmentation task, aiming to improve the segmentation accuracy on the unlabeled target domain without incurring additional annotation. Using the pseudo-label-based unsupervised domain adaptation (UDA) pipeline, we propose a novel and effective Multiple Fusion Adaptation (MFA) method. MFA basically considers three parallel information fusion strategies, i.e., the cross-model fusion, temporal fusion and a novel online-offline pseudo label fusion. Specifically, the online-offline pseudo label fusion encourages the adaptive training to pay additional attention to difficult regions that are easily ignored by offline pseudo labels, therefore retaining more informative details. While the other two fusion strategies may look standard, MFA pays significant efforts to raise the efficiency and effectiveness for integration, and succeeds in injecting all the three strategies into a unified framework. Experiments on two widely used benchmarks, i.e., GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes, show that our method significantly improves the semantic segmentation adaptation, and sets up new state of the art (58.2% and 62.5% mIoU, respectively). The code will be available at https://github.com/KaiiZhang/MFA.", "venue": "arXiv", "keywords": ["domain adaptation", "feature fusion", "self-supervised learning", "semantic segmentation", "temporal consistency", "unsupervised da"]}
{"id": "zhangOrientationawareSemanticSegmentation2019", "title": "Orientation-Aware Semantic Segmentation on Icosahedron Spheres", "abstract": "We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640 x 1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art.", "venue": "arXiv", "keywords": []}
{"id": "zhangOverviewMultitaskLearning2018", "title": "An Overview of Multi-Task Learning", "abstract": "As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.", "venue": "National Science Review", "keywords": []}
{"id": "zhangPerceptualConsistencyVideo2022", "title": "Perceptual Consistency in Video Segmentation", "abstract": "", "venue": "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)", "keywords": ["semantic segmentation", "temporal consistency"]}
{"id": "zhangPrincipledDisentanglementDomain2022", "title": "Towards Principled Disentanglement for Domain Generalization", "abstract": "A fundamental challenge for machine learning models is generalizing to out-of-distribution (OOD) data, in part due to spurious correlations. To tackle this challenge, we first formalize the OOD generalization problem as constrained optimization, called Disentanglement-constrained Domain Generalization (DDG). We relax this non-trivial constrained optimization problem to a tractable form with finite-dimensional parameterization and empirical approxi-mation. Then a theoretical analysis of the extent to which the above transformations deviates from the original problem is provided. Based on the transformation, we propose a primal-dual algorithm for joint representation disentanglement and domain generalization. In contrast to traditional approaches based on domain adversarial training and domain labels, DDG jointly learns semantic and variation encoders for disentanglement, enabling flexible manipulation and augmentation on training data. DDG aims to learn intrinsic representations of semantic concepts that are invariant to nuisance factors and generalizable across domains. Comprehensive experiments on popular benchmarks show that DDG can achieve competitive OOD performance and uncover interpretable salient structures within data.", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": ["domain generalization", "highly-analytical", "representation learning", "risk optimization"]}
{"id": "zhangPrincipledDisentanglementDomain2022a", "title": "Towards Principled Disentanglement for Domain Generalization", "abstract": "A fundamental challenge for machine learning models is generalizing to out-of-distribution (OOD) data, in part due to spurious correlations. To tackle this challenge, we first formalize the OOD generalization problem as constrained optimization, called Disentanglement-constrained Domain Generalization (DDG). We relax this non-trivial constrained optimization problem to a tractable form with finite-dimensional parameterization and empirical approximation. Then a theoretical analysis of the extent to which the above transformations deviates from the original problem is provided. Based on the transformation, we propose a primaldual algorithm for joint representation disentanglement and domain generalization. In contrast to traditional approaches based on domain adversarial training and domain labels, DDG jointly learns semantic and variation encoders for disentanglement, enabling flexible manipulation and augmentation on training data. DDG aims to learn intrinsic representations of semantic concepts that are invariant to nuisance factors and generalizable across domains. Comprehensive experiments on popular benchmarks show that DDG can achieve competitive OOD performance and uncover interpretable salient structures within data.", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "keywords": []}
{"id": "zhangQuantifyingImprovingTransferability2021", "title": "Quantifying and Improving Transferability in Domain Generalization", "abstract": "Out-of-distribution generalization is one of the key challenges when transferring a model from the lab to the real world. Existing efforts mostly focus on building invariant features among source and target domains. Based on invariant features, a high-performing classifier on source domains could hopefully behave equally well on a target domain. In other words, the invariant features are \\. However, in practice, there are no perfectly transferable features, and some algorithms seem to learn\"more transferable\"features than others. How can we understand and quantify such \\? In this paper, we formally define transferability that one can quantify and compute in domain generalization. We point out the difference and connection with common discrepancy measures between domains, such as total variation and Wasserstein distance. We then prove that our transferability can be estimated with enough samples and give a new upper bound for the target error based on our transferability. Empirically, we evaluate the transferability of the feature embeddings learned by existing algorithms for domain generalization. Surprisingly, we find that many algorithms are not quite learning transferable features, although few could still survive. In light of this, we propose a new algorithm for learning transferable features and test it over various benchmark datasets, including RotatedMNIST, PACS, Office-Home and WILDS-FMoW. Experimental results show that the proposed algorithm achieves consistent improvement over many state-of-the-art algorithms, corroborating our theoretical findings.", "venue": "Neural Information Processing Systems", "keywords": ["feature engineering", "generalization certification", "generalization quantification", "promising", "representation learning"]}
{"id": "zhangSegViTSemanticSegmentation2022", "title": "SegViT: Semantic Segmentation with Plain Vision Transformers", "abstract": "We explore the capability of plain Vision Transformers (ViTs) for semantic segmentation and propose the SegVit. Previous ViT-based segmentation networks usually learn a pixel-level representation from the output of the ViT. Differently, we make use of the fundamental component -- attention mechanism, to generate masks for semantic segmentation. Specifically, we propose the Attention-to-Mask (ATM) module, in which the similarity maps between a set of learnable class tokens and the spatial feature maps are transferred to the segmentation masks. Experiments show that our proposed SegVit using the ATM module outperforms its counterparts using the plain ViT backbone on the ADE20K dataset and achieves new state-of-the-art performance on COCO-Stuff-10K and PASCAL-Context datasets. Furthermore, to reduce the computational cost of the ViT backbone, we propose query-based down-sampling (QD) and query-based up-sampling (QU) to build a Shrunk structure. With the proposed Shrunk structure, the model can save up to \\ computations while maintaining competitive performance.", "venue": "arXiv", "keywords": ["ablation candidates", "semantic segmentation", "transformers"]}
{"id": "zhangSurveyMultiTaskLearning2021", "title": "A Survey on Multi-Task Learning", "abstract": "Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.", "venue": "arXiv", "keywords": []}
{"id": "zhangTheoreticallyPrincipledTradeoff2019", "title": "Theoretically Principled Trade-off between Robustness and Accuracy", "abstract": "We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by 11.41% in terms of mean L_2 perturbation distance.", "venue": "Proceedings of the 36th International Conference on Machine Learning", "keywords": ["generalization quantification", "highly-analytical"]}
{"id": "zhangUnderstandingDeepLearning2017", "title": "Understanding Deep Learning Requires Rethinking Generalization", "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.", "venue": "arXiv", "keywords": ["domain generalization", "foundational", "generalization quantification", "surveys"]}
{"id": "zhangUnderstandingDeepLearning2021", "title": "Understanding Deep Learning (Still) Requires Rethinking Generalization", "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.", "venue": "Communications of the ACM", "keywords": ["domain generalization", "generalization quantification", "surveys"]}
{"id": "zhangUnifiedDomainAdaptive2024", "title": "Unified Domain Adaptive Semantic Segmentation", "abstract": "Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS) aims to transfer the supervision from a labeled source domain to an unlabeled target domain. The majority of existing UDA-SS works typically consider images whilst recent attempts have extended further to tackle videos by modeling the temporal dimension. Although the two lines of research share the major challenges -- overcoming the underlying domain distribution shift, their studies are largely independent, resulting in fragmented insights, a lack of holistic understanding, and missed opportunities for cross-pollination of ideas. This fragmentation prevents the unification of methods, leading to redundant efforts and suboptimal knowledge transfer across image and video domains. Under this observation, we advocate unifying the study of UDA-SS across video and image scenarios, enabling a more comprehensive understanding, synergistic advancements, and efficient knowledge sharing. To that end, we explore the unified UDA-SS from a general data augmentation perspective, serving as a unifying conceptual framework, enabling improved generalization, and potential for cross-pollination of ideas, ultimately contributing to the overall progress and practical impact of this field of research. Specifically, we propose a Quad-directional Mixup (QuadMix) method, characterized by tackling distinct point attributes and feature inconsistencies through four-directional paths for intra- and inter-domain mixing in a feature space. To deal with temporal shifts with videos, we incorporate optical flow-guided feature aggregation across spatial and temporal dimensions for fine-grained domain alignment. Extensive experiments show that our method outperforms the state-of-the-art works by large margins on four challenging UDA-SS benchmarks. Our source code and models will be released at ://github.com/ZHE-SAPI/UDASS\\.", "venue": "arXiv", "keywords": ["domain adaptation", "domain projection", "optical flow", "promising", "semantic segmentation", "temporal consistency", "unsupervised da"]}
{"id": "zhangUnifiedFrameworkGeneralizable2018", "title": "A Unified Framework for Generalizable Style Transfer: Style and Content Separation", "abstract": "Image style transfer has drawn broad attention in recent years. However, most existing methods aim to explicitly model the transformation between different styles, and the learned model is thus not generalizable to new styles. We here propose a unified style transfer framework for both character typeface transfer and neural style transfer tasks leveraging style and content separation. A key merit of such framework is its generalizability to new styles and contents. The overall framework consists of style encoder, content encoder, mixer and decoder. The style encoder and content encoder are used to extract the style and content representations from the corresponding reference images. The mixer integrates the above two representations and feeds it into the decoder to generate images with the target style and content. During training, the encoder networks learn to extract styles and contents from limited size of style/content reference images. This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special `multi-task' learning scenario. The encoders are expected to capture the underlying features for different styles and contents which is generalizable to new styles and contents. Under this framework, we design two individual networks for character typeface transfer and neural style transfer, respectively. For character typeface transfer, to separate the style features and content features, we leverage the conditional dependence of styles and contents given an image. For neural style transfer, we leverage the statistical information of feature maps in certain layers to represent style. Extensive experimental results have demonstrated the effectiveness and robustness of the proposed methods.", "venue": "arXiv", "keywords": ["gans", "multi-task learning", "style transfer"]}
{"id": "zhangWassersteinWassersteinAutoEncoders2019", "title": "Wasserstein-Wasserstein Auto-Encoders", "abstract": "To address the challenges in learning deep generative models (e.g.,the blurriness of variational auto-encoder and the instability of training generative adversarial networks, we propose a novel deep generative model, named Wasserstein-Wasserstein auto-encoders (WWAE). We formulate WWAE as minimization of the penalized optimal transport between the target distribution and the generated distribution. By noticing that both the prior \\ and the aggregated posterior \\ of the latent code Z can be well captured by Gaussians, the proposed WWAE utilizes the closed-form of the squared Wasserstein-2 distance for two Gaussians in the optimization process. As a result, WWAE does not suffer from the sampling burden and it is computationally efficient by leveraging the reparameterization trick. Numerical results evaluated on multiple benchmark datasets including MNIST, fashion- MNIST and CelebA show that WWAE learns better latent structures than VAEs and generates samples of better visual quality and higher FID scores than VAEs and GANs.", "venue": "arXiv", "keywords": []}
{"id": "zhangWhenUnseenDomain2019", "title": "When Unseen Domain Generalization Is Unnecessary? Rethinking Data Augmentation", "abstract": "Recent advances in deep learning for medical image segmentation demonstrate expert-level accuracy. However, in clinically realistic environments, such methods have marginal performance due to differences in image domains, including different imaging protocols, device vendors and patient populations. Here we consider the problem of domain generalization, when a model is trained once, and its performance generalizes to unseen domains. Intuitively, within a specific medical imaging modality the domain differences are smaller relative to natural images domain variability. We rethink data augmentation for medical 3D images and propose a deep stacked transformations (DST) approach for domain generalization. Specifically, a series of n stacked transformations are applied to each image in each mini-batch during network training to account for the contribution of domain-specific shifts in medical images. We comprehensively evaluate our method on three tasks: segmentation of whole prostate from 3D MRI, left atrial from 3D MRI, and left ventricle from 3D ultrasound. We demonstrate that when trained on a small source dataset, (i) on average, DST models on unseen datasets degrade only by 11% (Dice score change), compared to the conventional augmentation (degrading 39%) and CycleGAN-based domain adaptation method (degrading 25%), (ii) when evaluation on the same domain, DST is also better albeit only marginally. (iii) When training on large-sized data, DST on unseen domains reaches performance of state-of-the-art fully supervised models. These findings establish a strong benchmark for the study of domain generalization in medical imaging, and can be generalized to the design of robust deep segmentation models for clinical deployment.", "venue": "arXiv", "keywords": ["auto-augmentation policies", "domain generalization"]}
{"id": "zhaoAutomaticSemanticStyle2020", "title": "Automatic Semantic Style Transfer Using Deep Convolutional Neural Networks and Soft Masks", "abstract": "This paper presents an automatic image synthesis method to transfer the style of an example image to a content image. When standard neural style transfer approaches are used, the textures and colours in different semantic regions of the style image are often applied inappropriately to the content image, ignoring its semantic layout and ruining the transfer result. In order to reduce or avoid such effects, we propose a novel method based on automatically segmenting the objects and extracting their soft semantic masks from the style and content images, in order to preserve the structure of the content image while having the style transferred. Each soft mask of the style image represents a specific part of the style image, corresponding to the soft mask of the content image with the same semantics. Both the soft masks and source images are provided as multichannel input to an augmented deep CNN framework for style transfer which incorporates a generative Markov random field model. The results on various images show that our method outperforms the most recent techniques.", "venue": "The Visual Computer", "keywords": ["cnns", "style transfer"]}
{"id": "zhaoBcsUSTUniversalStyle2023", "title": "BcsUST: Universal Style Transformation Network for Balanced Content Styles", "abstract": "Composition, color, and brushstrokes are the primary factors in evaluating and appreciating artwork and are vital points to be considered and addressed in arbitrary style transfer tasks. However, existing methods do not balance these elements well, so we propose an approach that balances content structure and style patterns for universal style transformation (BcsUST). Specifically, two lightweight encoders based on residual networks pass the extracted style and content features into the multi-domain structure attention module, which applies a self-attention mechanism to align the input features point by point followed by manifold alignment. A secondary alignment strategy is used to integrate global styles harmoniously into the semantic structure of the content, while obtaining content details to keep the structure intact. Then, the texture modulator generates style convolution parameters to dynamically adjust the filter within the micro-decoder feature subspace, injecting complex and flexible style signals for stylization to enrich the colors and form exquisite strokes. In addition, this work proposes a two-stage training strategy that introduces an artistic appreciation loss to further balance content structure and stylistic signals. Numerous qualitative and quantitative studies demonstrate that the BcsUST framework produces images that resemble the artists' paintings more closely.", "venue": "Journal of Electronic Imaging", "keywords": ["style transfer"]}
{"id": "zhaoCloserLookFewshot2023", "title": "A Closer Look at Few-shot Image Generation", "abstract": "Modern GANs excel at generating high quality and diverse images. However, when transferring the pretrained GANs on small target data (e.g., 10-shot), the generator tends to replicate the training samples. Several methods have been proposed to address this few-shot image generation task, but there is a lack of effort to analyze them under a unified framework. As our first contribution, we propose a framework to analyze existing methods during the adaptation. Our analysis discovers that while some methods have disproportionate focus on diversity preserving which impede quality improvement, all methods achieve similar quality after convergence. Therefore, the better methods are those that can slow down diversity degradation. Furthermore, our analysis reveals that there is still plenty of room to further slow down diversity degradation.", "venue": "arXiv", "keywords": ["few-shot learning", "image-to-image"]}
{"id": "zhaoDataAugmentationUsing2019", "title": "Data Augmentation Using Learned Transformations for One-Shot Medical Image Segmentation", "abstract": "Image segmentation is an important task in many medical applications. Methods based on convolutional neural networks attain state-of-the-art accuracy; however, they typically rely on supervised training with large labeled datasets. Labeling medical images requires significant expertise and time, and typical hand-tuned approaches for data augmentation fail to capture the complex variations in such images. We present an automated data augmentation method for synthesizing labeled medical images. We demonstrate our method on the task of segmenting magnetic resonance imaging (MRI) brain scans. Our method requires only a single segmented scan, and leverages other unlabeled scans in a semi-supervised approach. We learn a model of transformations from the images, and use the model along with the labeled example to synthesize additional labeled examples. Each transformation is comprised of a spatial deformation field and an intensity change, enabling the synthesis of complex effects such as variations in anatomy and image acquisition procedures. We show that training a supervised segmenter with these new examples provides significant improvements over state-of-the-art methods for one-shot biomedical image segmentation. Our code is available at https://github.com/xamyzhao/brainstorm.", "venue": "arXiv", "keywords": ["auto-augmentation policies"]}
{"id": "zhaoMaximumEntropyAdversarialData2020", "title": "Maximum-Entropy Adversarial Data Augmentation for Improved Generalization and Robustness", "abstract": "Adversarial data augmentation has shown promise for training robust deep neural networks against unforeseen data shifts or corruptions. However, it is difficult to define heuristics to generate effective fictitious target distributions containing \"hard\" adversarial perturbations that are largely different from the source distribution. In this paper, we propose a novel and effective regularization term for adversarial data augmentation. We theoretically derive it from the information bottleneck principle, which results in a maximum-entropy formulation. Intuitively, this regularization term encourages perturbing the underlying source distribution to enlarge predictive uncertainty of the current model, so that the generated \"hard\" adversarial perturbations can improve the model robustness during training. Experimental results on three standard benchmarks demonstrate that our method consistently outperforms the existing state of the art by a statistically significant margin.", "venue": "arXiv", "keywords": ["adversarial learning", "domain generalization", "entropic optimization", "gans", "generative augmentation"]}
{"id": "zhaoMaximumEntropyAdversarialData2020a", "title": "Maximum-Entropy Adversarial Data Augmentation for Improved Generalization and Robustness", "abstract": "Adversarial data augmentation has shown promise for training robust deep neural networks against unforeseen data shifts or corruptions. However, it is difficult to define heuristics to generate effective fictitious target distributions containing \"hard\" adversarial perturbations that are largely different from the source distribution. In this paper, we propose a novel and effective regularization term for adversarial data augmentation. We theoretically derive it from the information bottleneck principle, which results in a maximum-entropy formulation. Intuitively, this regularization term encourages perturbing the underlying source distribution to enlarge predictive uncertainty of the current model, so that the generated \"hard\" adversarial perturbations can improve the model robustness during training. Experimental results on three standard benchmarks demonstrate that our method consistently outperforms the existing state of the art by a statistically significant margin.", "venue": "arXiv", "keywords": ["adversarial robustness", "domain generalization", "entropic optimization"]}
{"id": "zhaoMultisourceDomainAdaptation", "title": "Multi-Source Domain Adaptation for Semantic Segmentation", "abstract": "Simulation-to-real domain adaptation for semantic segmentation has been actively studied for various applications such as autonomous driving. Existing methods mainly focus on a single-source setting, which cannot easily handle a more practical scenario of multiple sources with different distributions. In this paper, we propose to investigate multi-source domain adaptation for semantic segmentation. Specifically, we design a novel framework, termed Multi-source Adversarial Domain Aggregation Network (MADAN), which can be trained in an end-to-end manner. First, we generate an adapted domain for each source with dynamic semantic consistency while aligning at the pixel-level cycle-consistently towards the target. Second, we propose sub-domain aggregation discriminator and cross-domain cycle discriminator to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and target domain while training the segmentation network. Extensive experiments from synthetic GTA and SYNTHIA to real Cityscapes and BDDS datasets demonstrate that the proposed MADAN model outperforms state-of-the-art approaches. Our source code is released at: https://github.com/Luodian/MADAN.", "venue": "", "keywords": ["adversarial learning", "domain adaptation", "feature engineering", "semantic segmentation", "sim-to-real"]}
{"id": "zheDilatedConvolutionalPixels2021", "title": "Dilated Convolutional Pixels Affinity Network for Weakly Supervised Semantic Segmentation", "abstract": "This paper studies semantic segmentation primarily under image-level weak-supervision. Most state-of-the-art technologies have recently used deep classification networks to create small and sparse discriminatory seed regions of each interest target as pseudo-labels for training segmentation networks, which achieve inferior performance compared with the fully supervised setting. We propose a Dilated convolutional pixels affinity network (DCPAN) to localize and expand the seed regions of objects to bridge this gap. Although introduced dilated convolutional units enable capture of additional location information of objects, it falsely highlighted true negative regions as dilated rate enlarge. To address this problem, we properly integrate dilated convolutional units with different dilated rates and self-attention mechanisms to obtain pixel affinity measure matrix for promoting classification network to generate high-quality object seed regions as pseudo-labels; thus, the performance of the segmentation network is boosted. Furthermore, although our approach seems simple, our method obtains a competitive performance, and experiments show that the performance of DCPAN outperforms other state-of-art approaches in weakly-supervised settings, which only use image-level labels on the Pascal VOC 2012 dataset.", "venue": "Chinese Journal of Electronics", "keywords": ["affinity modeling", "cnns", "semantic segmentation", "semi-supervised learning"]}
{"id": "zhengguoliWeightedGuidedImage2015", "title": "Weighted Guided Image Filtering", "abstract": "It is known that local filtering-based edgepreserving smoothing techniques suffer from halo artifacts. In this paper, a weighted guided image filter (WGIF) is introduced by incorporating an edge-aware weighting into an existing guided image filter (GIF) to address the problem. The WGIF inherits advantages of both global and local smoothing filters in the sense that: 1) the complexity of the WGIF is O(N) for an image with N pixels, which is same as the GIF and 2) the WGIF can avoid halo artifacts like the existing global smoothing filters. The WGIF is applied for single image detail enhancement, single image haze removal, and fusion of differently exposed images. Experimental results show that the resultant algorithms produce images with better visual quality and at the same time halo artifacts can be reduced/avoided from appearing in the final images with negligible increment on running times.", "venue": "IEEE Transactions on Image Processing", "keywords": ["filter augmentations", "guided filters"]}
{"id": "zhengRectifyingPseudoLabel2020", "title": "Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation", "abstract": "This paper focuses on the unsupervised domain adaptation of transferring the knowledge from the source domain to the target domain in the context of semantic segmentation. Existing approaches usually regard the pseudo label as the ground truth to fully exploit the unlabeled target-domain data. Yet the pseudo labels of the target-domain data are usually predicted by the model trained on the source domain. Thus, the generated labels inevitably contain the incorrect prediction due to the discrepancy between the training domain and the test domain, which could be transferred to the final adapted model and largely compromises the training process. To overcome the problem, this paper proposes to explicitly estimate the prediction uncertainty during training to rectify the pseudo label learning for unsupervised semantic segmentation adaptation. Given the input image, the model outputs the semantic segmentation prediction as well as the uncertainty of the prediction. Specifically, we model the uncertainty via the prediction variance and involve the uncertainty into the optimization objective. To verify the effectiveness of the proposed method, we evaluate the proposed method on two prevalent synthetic-to-real semantic segmentation benchmarks, i.e., GTA5 - Cityscapes and SYNTHIA - Cityscapes, as well as one cross-city benchmark, i.e., Cityscapes - Oxford RobotCar. We demonstrate through extensive experiments that the proposed approach (1) dynamically sets different confidence thresholds according to the prediction variance, (2) rectifies the learning from noisy pseudo labels, and (3) achieves significant improvements over the conventional pseudo label learning and yields competitive performance on all three benchmarks.", "venue": "arXiv.org", "keywords": ["domain adaptation", "semantic segmentation", "semi-supervised learning", "uncertainty quantification"]}
{"id": "zhengRethinkingSemanticSegmentation2021", "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers", "abstract": "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.", "venue": "arXiv", "keywords": ["semantic segmentation"]}
{"id": "zhengSTaDAStyleTransfer2019", "title": "STaDA: Style Transfer as Data Augmentation", "abstract": "The success of training deep Convolutional Neural Networks (CNNs) heavily depends on a significant amount of labelled data. Recent research has found that neural style transfer algorithms can apply the artistic style of one image to another image without changing the latter's high-level semantic content, which makes it feasible to employ neural style transfer as a data augmentation method to add more variation to the training dataset. The contribution of this paper is a thorough evaluation of the effectiveness of the neural style transfer as a data augmentation method for image classification tasks. We explore the state-of-the-art neural style transfer algorithms and apply them as a data augmentation method on Caltech 101 and Caltech 256 dataset, where we found around 2% improvement from 83% to 85% of the image classification accuracy with VGG16, compared with traditional data augmentation strategies. We also combine this new method with conventional data augmentation approaches to further improve the performance of image classification. This work shows the potential of neural style transfer in computer vision field, such as helping us to reduce the difficulty of collecting sufficient labelled data and improve the performance of generic image-based deep learning algorithms.", "venue": "Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications", "keywords": ["style transfer", "surveys", "traditional augmentation"]}
{"id": "zhouAffinitySpaceAdaptation2021", "title": "Affinity Space Adaptation for Semantic Segmentation Across Domains", "abstract": "Semantic segmentation with dense pixel-wise annotation has achieved excellent performance thanks to deep learning. However, the generalization of semantic segmentation in the wild remains challenging. In this paper, we address the problem of unsupervised domain adaptation (UDA) in semantic segmentation. Motivated by the fact that source and target domain have invariant semantic structures, we propose to exploit such invariance across domains by leveraging cooccurring patterns between pairwise pixels in the output of structured semantic segmentation. This is different from most existing approaches that attempt to adapt domains based on individual pixel-wise information in image, feature, or output level. Specifically, we perform domain adaptation on the affinity relationship between adjacent pixels termed affinity space of source and target domain. To this end, we develop two affinity space adaptation strategies: affinity space cleaning and adversarial affinity space alignment. Extensive experiments demonstrate that the proposed method achieves superior performance against some state-of-the-art methods on several challenging benchmarks for semantic segmentation across domains. The code is available at https://github.com/idealwei/ASANet.", "venue": "IEEE Transactions on Image Processing", "keywords": ["adversarial learning", "affinity modeling", "self-training", "semantic segmentation", "unsupervised da"]}
{"id": "zhouDomainGeneralizationMixStyle2021", "title": "Domain Generalization with MixStyle", "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs. images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.", "venue": "arXiv", "keywords": ["domain generalization", "instance normalization", "mixture augmentations"]}
{"id": "zhouDomainGeneralizationOptimal2021", "title": "Domain Generalization via Optimal Transport with Metric Similarity Learning", "abstract": "Generalizing knowledge to unseen domains, where data and labels are unavailable, is crucial for machine learning models. We tackle the domain generalization problem to learn from multiple source domains and generalize to a target domain with unknown statistics. The crucial idea is to extract the underlying invariant features across all the domains. Previous domain generalization approaches mainly focused on learning invariant features and stacking the learned features from each source domain to generalize to a new target domain while ignoring the label information, which will lead to indistinguishable features with an ambiguous classification boundary. One possible solution is to constrain the label-similarity when extracting the invariant features and take advantage of the label similarities for class-specific cohesion and separation of features across domains. Therefore we adopt optimal transport with Wasserstein distance, which could constrain the class label similarity, for adversarial training and also further deploy a metric learning objective to leverage the label information for achieving distinguishable classification boundary. Empirical results show that our proposed method could outperform most of the baselines. Furthermore, ablation studies also demonstrate the effectiveness of each component of our method.", "venue": "Neurocomputing", "keywords": ["adversarial learning", "domain generalization", "generalization quantification", "measure theory", "optimal transport"]}
{"id": "zhouDomainGeneralizationSurvey2022", "title": "Domain Generalization: A Survey", "abstract": "Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other relevant fields like domain adaptation and transfer learning. Then, we conduct a thorough review into existing methods and theories. Finally, we conclude this survey with insights and discussions on future research directions.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "keywords": ["domain generalization", "surveys"]}
{"id": "zhouLearningGenerateNovel2021", "title": "Learning to Generate Novel Domains for Domain Generalization", "abstract": "This paper focuses on domain generalization (DG), the task of learning from multiple source domains a model that generalizes well to unseen domains. A main challenge for DG is that the available source domains often exhibit limited diversity, hampering the model's ability to learn to generalize. We therefore employ a data generator to synthesize data from pseudo-novel domains to augment the source domains. This explicitly increases the diversity of available training domains and leads to a more generalizable model. To train the generator, we model the distribution divergence between source and synthesized pseudo-novel domains using optimal transport, and maximize the divergence. To ensure that semantics are preserved in the synthesized data, we further impose cycle-consistency and classification losses on the generator. Our method, L2A-OT (Learning to Augment by Optimal Transport) outperforms current state-of-the-art DG methods on four benchmark datasets.", "venue": "arXiv", "keywords": ["adversarial learning", "domain generalization", "gans", "optimal transport"]}
{"id": "zhouMetaAugmentSampleAwareData2021", "title": "MetaAugment: Sample-Aware Data Augmentation Policy Learning", "abstract": "Automated data augmentation has shown superior performance in image recognition. Existing works search for dataset-level augmentation policies without considering individual sample variations, which are likely to be sub-optimal. On the other hand, learning different policies for different samples naively could greatly increase the computing cost. In this paper, we learn a sample-aware data augmentation policy efficiently by formulating it as a sample reweighting problem. Specifically, an augmentation policy network takes a transformation and the corresponding augmented image as inputs, and outputs a weight to adjust the augmented image loss computed by a task network. At training stage, the task network minimizes the weighted losses of augmented training images, while the policy network minimizes the loss of the task network on a validation set via meta-learning. We theoretically prove the convergence of the training procedure and further derive the exact convergence rate. Superior performance is achieved on widely-used benchmarks including CIFAR-10/100, Omniglot, and ImageNet.", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "keywords": ["auto-augmentation policies", "meta-learning"]}
{"id": "zhouUnsupervisedDomainAdaptive2023", "title": "Unsupervised Domain Adaptive Detection with Network Stability Analysis", "abstract": "Domain adaptive detection aims to improve the generality of a detector, learned from the labeled source domain, on the unlabeled target domain. In this work, drawing inspiration from the concept of stability from the control theory that a robust system requires to remain consistent both externally and internally regardless of disturbances, we propose a novel framework that achieves unsupervised domain adaptive detection through stability analysis. In specific, we treat discrepancies between images and regions from different domains as disturbances, and introduce a novel simple but effective Network Stability Analysis (NSA) framework that considers various disturbances for domain adaptation. Particularly, we explore three types of perturbations including heavy and light image-level disturbances and instancelevel disturbance. For each type, NSA performs external consistency analysis on the outputs from raw and perturbed images and/or internal consistency analysis on their features, using teacher-student models. By integrating NSA into Faster R-CNN, we immediately achieve state-of-the-art results. In particular, we set a new record of 52.7% mAP on Cityscapes-to-FoggyCityscapes, showing the potential of NSA for domain adaptive detection. It is worth noticing, our NSA is designed for general purpose, and thus applicable to one-stage detection model (e.g., FCOS) besides the adopted one, as shown by experiments. https://github.com/tiankongzhang/NSA.", "venue": "arXiv.org", "keywords": ["augmentation stability", "domain adaptation", "promising", "unsupervised da"]}
{"id": "zhuangDivergenceawareFederatedSelfSupervised2022", "title": "Divergence-Aware Federated Self-Supervised Learning", "abstract": "Self-supervised learning (SSL) is capable of learning remarkable representations from centrally available data. Recent works further implement federated learning with SSL to learn from rapidly growing decentralized unlabeled images (e.g., from cameras and phones), often resulted from privacy constraints. Extensive attention has been paid to SSL approaches based on Siamese networks. However, such an effort has not yet revealed deep insights into various fundamental building blocks for the federated self-supervised learning (FedSSL) architecture. We aim to fill in this gap via in-depth empirical study and propose a new method to tackle the non-independently and identically distributed (non-IID) data problem of decentralized data. Firstly, we introduce a generalized FedSSL framework that embraces existing SSL methods based on Siamese networks and presents flexibility catering to future methods. In this framework, a server coordinates multiple clients to conduct SSL training and periodically updates local models of clients with the aggregated global model. Using the framework, our study uncovers unique insights of FedSSL: 1) stop-gradient operation, previously reported to be essential, is not always necessary in FedSSL; 2) retaining local knowledge of clients in FedSSL is particularly beneficial for non-IID data. Inspired by the insights, we then propose a new approach for model update, Federated Divergence-aware Exponential Moving Average update (FedEMA). FedEMA updates local models of clients adaptively using EMA of the global model, where the decay rate is dynamically measured by model divergence. Extensive experiments demonstrate that FedEMA outperforms existing methods by 3-4% on linear evaluation. We hope that this work will provide useful insights for future research.", "venue": "arXiv", "keywords": ["federated learning"]}
{"id": "zhuExploringTextureEnsembles2000", "title": "Exploring Texture Ensembles by Efficient Markov Chain Monte Carlo-Toward a 'Trichromacy' Theory of Texture", "abstract": "This article presents a mathematical definition of texture the Julesz ensemble\\ , which is the set of all images (defined on \\ ) that share identical statistics \\ . Then texture modeling is posed as an inverse problem: Given a set of images sampled from an unknown Julesz ensemble \\ , we search for the statistics \\ which define the ensemble. A Julesz ensemble \\ has an associated probability distribution \\ , which is uniform over the images in the ensemble and has zero probability outside. In a companion paper , \\ is shown to be the limit distribution of the FRAME (Filter, Random Field, And Minimax Entropy) model , as the image lattice \\ . This conclusion establishes the intrinsic link between the scientific definition of texture on \\ and the mathematical models of texture on finite lattices. It brings two advantages to computer vision: 1) The engineering practice of synthesizing texture images by matching statistics has been put on a mathematical foundation. 2) We are released from the burden of learning the expensive FRAME model in feature pursuit, model selection and texture synthesis. In this paper, an efficient Markov chain Monte Carlo algorithm is proposed for sampling Julesz ensembles. The algorithm generates random texture images by moving along the directions of filter coefficients and, thus, extends the traditional single site Gibbs sampler. We also compare four popular statistical measures in the literature, namely, moments, rectified functions, marginal histograms, and joint histograms of linear filter responses in terms of their descriptive abilities. Our experiments suggest that a small number of bins in marginal histograms are sufficient for capturing a variety of texture patterns. We illustrate our theory and algorithm by successfully synthesizing a number of natural textures.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "keywords": ["texture transfer"]}
{"id": "zhuJuleszsQuestTexture", "title": "Julesz's Quest for Texture Perception", "abstract": "", "venue": "", "keywords": []}
{"id": "zhuOODProbeNeuralInterpretation2022", "title": "OOD-Probe: A Neural Interpretation of Out-of-Domain Generalization", "abstract": "The ability to generalize out-of-domain (OOD) is an important goal for deep neural network development, and researchers have proposed many high-performing OOD generalization methods from various foundations. While many OOD algorithms perform well in various scenarios, these systems are evaluated as ``black-boxes''. Instead, we propose a flexible framework that evaluates OOD systems with finer granularity using a probing module that predicts the originating domain from intermediate representations. We find that representations always encode some information about the domain. While the layerwise encoding patterns remain largely stable across different OOD algorithms, they vary across the datasets. For example, the information about rotation (on RotatedMNIST) is the most visible on the lower layers, while the information about style (on VLCS and PACS) is the most visible on the middle layers. In addition, the high probing results correlate to the domain generalization performances, leading to further directions in developing OOD generalization systems.", "venue": "arXiv", "keywords": ["distribution estimation", "domain generalization", "generalization quantification", "neural probing"]}
{"id": "zhuPrimalSketchIntegrating2023", "title": "Primal Sketch: Integrating Textures and Textons", "abstract": "In his monumental book (Marr, Vision: a computational investigation into the human representation and processing of visual information. WH San Francisco: Freeman and Company, 1(2), 1982), Marr inherited Julesz's texton (Julesz et al., Nature 290(5802):91--97, 1981) notion and proposed the concept of image primitives as basic perceptual tokens, such as edges, bars, junctions, and terminators. Inspired by the Nyquist sampling theorem in signal processing, Marr went a step further and asked for a token representation which he named ``primal sketch'' as a perceptually lossless conversion from the raw image. He tried to reconstruct the image with zero-crossings unsuccessfully and his effort was mostly limited by the lack of proper models of texture.", "venue": "Computer Vision: Statistical Models for Marr's Paradigm", "keywords": []}
{"id": "zhuUnpairedImagetoImageTranslation2020", "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks", "abstract": "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \\ to a target domain \\ in the absence of paired examples. Our goal is to learn a mapping \\ such that the distribution of images from \\ is indistinguishable from the distribution \\ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \\ and introduce a cycle consistency loss to push \\ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.", "venue": "arXiv", "keywords": ["adversarial learning", "foundational", "gans", "image synthesis"]}
{"id": "zhuWhatAreTextons2002", "title": "What Are Textons?", "abstract": "Textons refer to fundamental micro-structures in generic natural images and thus constitute the basic elements in early (pre-attentive) visual perception. However, the word ``texton'' remains a vague concept in the literature of computer vision and visual perception, and a precise mathematical definition has yet to be found. In this article, we argue that the definition of texton should be governed by a sound mathematical model of images, and the set of textons must be learned from, or best tuned to, an image ensemble. We adopt a generative image model that an image is a superposition of bases from an over-complete dictionary, then a texton is defined as a mini-template that consists of a varying number of image bases with some geometric and photometric configurations. By analogy to physics, if image bases are like protons, neutrons and electrons, then textons are like atoms. Then a small number of textons can be learned from training images as repeating micro-structures. We report four experiments for comparison. The first experiment computes clusters in feature space of filter responses. The second use transformed component analysis in both feature space and image patches. The third adopts a two-layer generative model where an image is generated by image bases and image bases are generated by textons. The fourth experiment shows textons from motion image sequences, which we call movetons.", "venue": "Proc Eur Conf Comput Vis (ECCV)", "keywords": []}
{"id": "zouDomainAdaptationSemantic", "title": "Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training", "abstract": "Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world ``wild tasks'' where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as ``domain gap'', and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of self-training, we also propose a novel class-balanced self-training framework to avoid the gradual dominance of large classes in pseudo-label generation, and introduce spatial priors to refine the generated pseudo-labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.", "venue": "", "keywords": ["class balancing", "curriculum learning", "domain adaptation", "self-training", "semantic segmentation", "unsupervised da"]}
{"id": "zuiderveldContrastLimitedAdaptive1994", "title": "Contrast Limited Adaptive Histogram Equalization", "abstract": "Semantic Scholar extracted view of \"Contrast Limited Adaptive Histogram Equalization\" by K. Zuiderveld", "venue": "Graphics Gems", "keywords": []}
